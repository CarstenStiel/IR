{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d470079",
   "metadata": {},
   "source": [
    "# <ins> Milestone 1 </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997cda7",
   "metadata": {},
   "source": [
    "The following presents the code for converting the file \"ir-anthology-07-11-2021-ss23\" into a new file with the name \"ir-anthology-final\" in the correct format into the directory \"Created Data\". In addition, this newly created file is registered together with the requested XML file called \"topics\" in \"Tira\". <br/><br/>Afterwards, a reflection will be presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c9955",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2b548",
   "metadata": {},
   "source": [
    "### Formatting \"ir-anthology-07-11-2021-ss23\" with the fields \"doc_id\" and \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfcbfac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:21.381943Z",
     "start_time": "2023-04-28T17:05:21.375467Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json #Module to work on JSON data\n",
    "import os #Module for interacting with the operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161d78f",
   "metadata": {},
   "source": [
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff1288a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:21.395638Z",
     "start_time": "2023-04-28T17:05:21.380942Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDirectory():\n",
    "    return os.path.join(os.getcwd(), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f1b88",
   "metadata": {},
   "source": [
    "Function to get the current directory in the right format.<br/><br/>Return: <br/> &emsp; Current directory as String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3aba6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:21.395638Z",
     "start_time": "2023-04-28T17:05:21.394638Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmlFileName = \"dnc-limited-topics.xml\"\n",
    "inputFileName = \"ir-anthology-07-11-2021-ss23.jsonl\"\n",
    "outputFileName = \"dnc-limited--documents.jsonl\"\n",
    "inputFilePath = getDirectory() + \"data/\" + inputFileName\n",
    "outputFilePath = getDirectory() + \"data/\" + outputFileName\n",
    "xmlFilePath = getDirectory() + \"data/\" + xmlFileName\n",
    "irdatasetName = \"dnc-limited-dataset-tira/\"\n",
    "irdatasetPath = getDirectory() + irdatasetName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9535a3",
   "metadata": {},
   "source": [
    "File names and thier paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc372c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:21.430882Z",
     "start_time": "2023-04-28T17:05:21.395638Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkForFile(file_path, file_name):\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"The File {file_name} already exists.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"The File {file_name} doesn´t exist.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205acd02",
   "metadata": {},
   "source": [
    "Function to check if file already exists at the given path. <br/><br/>Return: <br/> &emsp; True: If file exists <br/> &emsp; False: If file doesn´t exist <br/><br/>Output: <br/> &emsp; Small information text, whether the file exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980d4ecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:21.430882Z",
     "start_time": "2023-04-28T17:05:21.406769Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName):\n",
    "    ##If the File doesn´t exist\n",
    "    if checkForFile(outputFilePath,outputFileName) == False:\n",
    "        with open(outputFilePath, 'w') as f:\n",
    "            #Try to create the \"outputFile\" as an empty file and give the following output\n",
    "            try:\n",
    "                f.write(json.dumps(\"\"))\n",
    "                print(f\"The File {outputFileName} was created in the following path: {outputFilePath}\")\n",
    "            #If the try failed, return the following output\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred creating the File {outputFileName}: {e}\")\n",
    "\n",
    "    # Open the input-JSONL-file and the output-JSONL-file\n",
    "    with open(inputFilePath, \"r\") as input_file, open(outputFilePath, \"w\") as output_file:\n",
    "        # Iterate over each line (object) of the input-JSONL-file\n",
    "        for line in input_file:\n",
    "            lineJSON = json.loads(line) # Load the current line as JSON\n",
    "            array = [] #Array for the values of the current line\n",
    "            for key in lineJSON:\n",
    "                array.append(lineJSON[key]) #Append the value for each key to the array\n",
    "            stringJSON = \" \".join(str(item) for item in array) #Create a string with the values of the object for the \"text\" field\n",
    "\n",
    "            # Create an object with the \"doc_id\" (id of the object) and \"text\" fields \n",
    "            finalObject = {\"doc_id\": lineJSON[\"id\"], \"text\": stringJSON}\n",
    "\n",
    "            # Write the finalObject as JSON to the output JSONL file\n",
    "            output_file.write(json.dumps(finalObject) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf1c73",
   "metadata": {},
   "source": [
    "Function to convert the \"inputFile\" to an \"outputFile\" as JSONL-File by the following steps: <br/> &emsp; 1. First check if \"outputFile\" already exists and if necessary create it. <br/> &emsp; 2. Convert the \"inputFile\" with the \"doc_id\" and \"text\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44131eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-28T17:05:22.868137Z",
     "start_time": "2023-04-28T17:05:21.410884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The File ir-anthology-final.jsonl already exists.\n"
     ]
    }
   ],
   "source": [
    "getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456578a",
   "metadata": {},
   "source": [
    "Execute the function to convert the \"inputFile\" <br/><br/> Output: <br/> &emsp; If file exists or has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d944e",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9269bbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T15:22:20.172500Z",
     "start_time": "2023-04-29T15:22:17.950686Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load .dockerignore\n",
      "#1 transferring context:\n",
      "#1 transferring context: 2B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load build definition from Dockerfile.iranthology\n",
      "#2 transferring dockerfile: 290B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/webis/tira-ir-datasets-starter:0.0.54\n",
      "#3 ...\n",
      "\n",
      "#4 [auth] webis/tira-ir-datasets-starter:pull token for registry-1.docker.io\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/webis/tira-ir-datasets-starter:0.0.54\n",
      "#3 DONE 1.8s\n",
      "\n",
      "#5 [1/2] FROM docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad\n",
      "#5 resolve docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad 0.0s done\n",
      "#5 sha256:6487f50e88d3191df0f699682a9750e1c5e7f9be20990516b94c1846b371b5d4 17.53kB / 17.53kB done\n",
      "#5 sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad 5.78kB / 5.78kB done\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 0B / 29.52MB 0.3s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 0B / 25.93MB 0.3s\n",
      "#5 sha256:aa245df9eeeb0ec487805118b10fbe3e68b402bce09bfc42bfe094a6b3e96839 0B / 1.28MB 0.3s\n",
      "#5 ...\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 transferring context: 59.78MB 0.4s done\n",
      "#6 DONE 0.4s\n",
      "\n",
      "#5 [1/2] FROM docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad\n",
      "#5 sha256:aa245df9eeeb0ec487805118b10fbe3e68b402bce09bfc42bfe094a6b3e96839 1.28MB / 1.28MB 0.6s done\n",
      "#5 extracting sha256:aa245df9eeeb0ec487805118b10fbe3e68b402bce09bfc42bfe094a6b3e96839 0.0s done\n",
      "#5 sha256:5f561b5de99d45db93d486620fb508ad6d221c0d8110124db03acd73c0df807e 0B / 162.47kB 0.6s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 6.29MB / 25.93MB 0.9s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 8.39MB / 25.93MB 1.0s\n",
      "#5 sha256:5f561b5de99d45db93d486620fb508ad6d221c0d8110124db03acd73c0df807e 162.47kB / 162.47kB 1.0s done\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 14.68MB / 25.93MB 1.2s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 2.10MB / 29.52MB 1.3s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 19.92MB / 25.93MB 1.4s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 25.17MB / 25.93MB 1.6s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 4.19MB / 29.52MB 1.7s\n",
      "#5 sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 25.93MB / 25.93MB 1.7s done\n",
      "#5 extracting sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 6.29MB / 29.52MB 1.9s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 8.39MB / 29.52MB 2.1s\n",
      "#5 extracting sha256:9407a8ead9476de915e7358c171ee44e0344514d6cedf225c77f65a02457ba01 0.3s done\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 12.58MB / 29.52MB 2.4s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 14.68MB / 29.52MB 2.5s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 17.83MB / 29.52MB 2.6s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 20.97MB / 29.52MB 2.8s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 28.31MB / 29.52MB 3.1s\n",
      "#5 sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 29.52MB / 29.52MB 3.2s done\n",
      "#5 extracting sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 0.1s\n",
      "#5 extracting sha256:549ef1972eebe48d49d771bc1c106df51f88a888cdb96e488c00e86c857749a6 0.3s done\n",
      "#5 extracting sha256:5f561b5de99d45db93d486620fb508ad6d221c0d8110124db03acd73c0df807e done\n",
      "#5 DONE 3.8s\n",
      "\n",
      "#7 [2/2] COPY iranthology-dnc-limited.py data/dnc-limited-topics.xml  data/dnc-limited-documents.jsonl /usr/lib/python3.8/site-packages/ir_datasets/datasets_in_progress/\n",
      "#7 DONE 0.3s\n",
      "\n",
      "#8 exporting to image\n",
      "#8 exporting layers\n",
      "#8 exporting layers 0.2s done\n",
      "#8 writing image sha256:39103ad202b39a6e2477e204ca750c77f4054e713fdd01cb838dd70ab1f19be7 done\n",
      "#8 naming to docker.io/library/dnc-limited-ir-dataset done\n",
      "#8 DONE 0.2s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t dnc-limited-ir-dataset -f Dockerfile.iranthology ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1123",
   "metadata": {},
   "source": [
    "Building a docker image for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c5efcbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T15:25:33.187606Z",
     "start_time": "2023-04-29T15:25:32.446265Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(irdatasetPath):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe folder already exists! Please delete the folder to recreate the data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(irdatasetPath):\n",
    "    print('The folder already exists! Please delete the folder to recreate the data.')\n",
    "else:\n",
    "    !tira-run --output-directory %cd%\\dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command \"/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir\"\n",
    "    #!tira-run --output-directory ${PWD}/dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command '/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir'\n",
    "    print('The folder has been created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57c4daa",
   "metadata": {},
   "source": [
    "Check, if the tira data already have been created and if needs, create them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccef287",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd58736",
   "metadata": {},
   "source": [
    "The file \"topics.xml\", which contains the topics, can be found in the \"CreatedData\" directory. <br/>Below you will find the section of the topic with the person who created it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e064ce6",
   "metadata": {},
   "source": [
    "Topic 1 by Constantin Urbainsky:\n",
    "```xml\n",
    "<topic number=\"1\">\n",
    "  <title>machine learnign for more relevant results</title>\n",
    "  <description>Which papers describe methods to find more relevant results using machine learning?</description>\n",
    "  <narrative>\n",
    "      Relevant papers describe one or more methods to find more relevant results using machine learning. \n",
    "      Papers about just machine learning in IR in general or papers just about finding more relevant results are not \n",
    "      relevant. \n",
    "  </narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27547c6",
   "metadata": {},
   "source": [
    "Topic 2 by Nils Harbach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e9086",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"2\">\n",
    "  <title>Crawling websites using machine learning</title>\n",
    "  <description>Papers that describe how to use AI to crawl the context of websites more efficient.</description>\n",
    "  <narrative>Papers in this topic describe methods and algorithms to use machine learning for crawling. They also contain information on the latest research findings on the topic. Papers about crawling methods without AI are not relevant for this topic.</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2b5a8",
   "metadata": {},
   "source": [
    "Topic 3 by Willi Bittorf:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8234aac",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"3\">\n",
    "    <title>Recommenders influence on users</title>\n",
    "    <description>Papers that describe the change in user behaviour because of recommenders?</description>\n",
    "    <narrative>Relevant papers describe how users are affected by recommenders, papers about the recommenders from a technological point of view are not relevant</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348114b",
   "metadata": {},
   "source": [
    "Topic 4 by Tom Paul Gresens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83e3c4",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"4\">\n",
    "    <title>Search engine caching effects</title>\n",
    "    <description>Papers that describe the effects and/or efficient use of search engine caching in terms of result freshness, query latency and other potential advantages or disadvantages </description>\n",
    "    <narrative>Papers in this topic will describe the design trade-off between low latency querying and returning the most recently available results as well as different architectures to create efficient caching systems. Results should not contain any other caching related topics (e.g. hardware or web browsers)</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841ead0",
   "metadata": {},
   "source": [
    "Topic 5 by Dorjan Domi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f8d2c",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"5\">\n",
    "    <title>Consumer Product reviews</title>\n",
    "    <description>Papers that describe the effects of product reviews on consumer decisions</description>\n",
    "    <narrative>Relevant papers would describe the influence that reviews have on on individual decisions of the consumer on whether to buy a product or not. Not relevant papers, would contain other studies about reviews, that are not pertaining to human psychology</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bf3c1",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999e24a",
   "metadata": {},
   "source": [
    "By Constantin:\n",
    "While working on the first milestone, I was constantly unsure how to proceed. While many tutorials and all of the data were available to us, it was nonetheless confusing. All information was spread out; some of it was in the notes for the lab, some were on the assignment sheet. Moreover, although I followed the tutorial for installing and using tira, the command that worked for my teammates did not for myself. And although we tried our hardest to find out why it wouldn't work for me, we ultimately failed. I was still able to contribute by helping fix problems as they arose, but I wasn't able to actually run tira locally.\n",
    "\n",
    "While on the topic of teamwork, I would say that I have been very fortunate with my group. Everyone was very fun to be around and determined to get this project done. If I had to critique our team, it would have to be in terms of organization. We had quite a bit of trouble finding times for meetings and group-work sessions because of conflicting schedules. On top of that, since all of us got into the project late and as such missed the first week of lab and lectures, we had to play catch-up, which wasn't great.\n",
    "\n",
    "In terms of prior experience, I would say that I didn't have much in the way of experience with the technologies used. Python is a language I never used before, although it wasn't as big a shift from Java as say Haskell was. Docker was used in our project for softwaretech-lab; however, it is still quite foreign to me, and finding my way around it was challenging. The Terminal is also something that I used a bit in our softwaretech-lab and as such somewhat familiarized myself with, however much like Docker, I would say I still have much to learn.\n",
    "\n",
    "By Willi:\n",
    "My primary source of concern during work on our first submission was the task itself, as there was no obvious path on how to complete. All the subtasks were doable and we managed to complete most of them quickly, but we had a hard time understanding what the final result should actually look like\n",
    "The tutorials were helpful but scattered, leading to more confusion while getting to know all the different technologies we're going to use in this course.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
