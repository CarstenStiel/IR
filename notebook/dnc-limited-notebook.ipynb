{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d470079",
   "metadata": {},
   "source": [
    "# <ins> Milestone 1 </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997cda7",
   "metadata": {},
   "source": [
    "The following presents the code for converting the file \"ir-anthology-07-11-2021-ss23\" into a new file with the name \"dnc-limited-documents.jsonl\" in the correct format into the directory \"data\". In addition, this newly created file is registered together with the requested XML file called \"dnc-limited-topics.xml\". <br/><br/>Afterwards, a reflection will be presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c9955",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2b548",
   "metadata": {},
   "source": [
    "### Formatting \"ir-anthology-07-11-2021-ss23\" with the fields \"doc_id\" and \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cfcbfac",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:43.415937700Z",
     "start_time": "2023-05-01T21:18:43.342336300Z"
    }
   },
   "outputs": [],
   "source": [
    "import json #Module to work on JSON data\n",
    "import os #Module for interacting with the operating system\n",
    "import platform #Module to check the current platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161d78f",
   "metadata": {},
   "source": [
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff1288a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:45.626434700Z",
     "start_time": "2023-05-01T21:18:43.348231600Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDirectory():\n",
    "    return os.path.join(os.getcwd(), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f1b88",
   "metadata": {},
   "source": [
    "Function to get the current directory in the right format.<br/><br/>Return: <br/> &emsp; Current directory as String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3aba6e",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:47.557124300Z",
     "start_time": "2023-05-01T21:18:45.627436400Z"
    }
   },
   "outputs": [],
   "source": [
    "xmlFileName = \"dnc-limited-topics.xml\"\n",
    "inputFileName = \"ir-anthology-07-11-2021-ss23.jsonl\"\n",
    "outputFileName = \"dnc-limited-documents.jsonl\"\n",
    "\n",
    "\n",
    "#Changing slashes to backslashes, depending on the current platform.\n",
    "if platform.system() == 'Linux':\n",
    "    irdatasetName = \"dnc-limited-dataset-tira/\"\n",
    "    directoryData = \"data/\"\n",
    "else:\n",
    "    irdatasetName = \"dnc-limited-dataset-tira\\\\\"\n",
    "    directoryData = \"data\\\\\"\n",
    "\n",
    "inputFilePath = getDirectory() + directoryData + inputFileName\n",
    "outputFilePath = getDirectory() + directoryData + outputFileName\n",
    "xmlFilePath = getDirectory() + directoryData + xmlFileName\n",
    "irdatasetPath = getDirectory() + irdatasetName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9535a3",
   "metadata": {},
   "source": [
    "File names/folders and their paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc372c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:49.727079Z",
     "start_time": "2023-05-01T21:18:47.558123500Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkForFile(file_path, file_name):\n",
    "    #If the file exists at the given path return true with an output\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"The File {file_name} already exists.\")\n",
    "        return True\n",
    "    #If the file doesn´t exists at the given path return flase with an output\n",
    "    else:\n",
    "        print(f\"The File {file_name} doesn´t exist.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205acd02",
   "metadata": {},
   "source": [
    "Function to check if file already exists at the given path. <br/><br/>Return: <br/> &emsp; True: If file exists <br/> &emsp; False: If file doesn´t exist <br/><br/>Output: <br/> &emsp; Small information text, whether the file exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980d4ecf",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:49.738242Z",
     "start_time": "2023-05-01T21:18:49.733964500Z"
    }
   },
   "outputs": [],
   "source": [
    "def getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName):\n",
    "    #If the File doesn´t exist\n",
    "    if checkForFile(outputFilePath,outputFileName) == False:\n",
    "        with open(outputFilePath, 'w') as f:\n",
    "            #Try to create the \"outputFile\" as an empty file and give the following output\n",
    "            try:\n",
    "                f.write(json.dumps(\"\"))\n",
    "                print(f\"The File {outputFileName} was created in the following path: {outputFilePath}\")\n",
    "            #If the try failed, return the following output\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred creating the File {outputFileName}: {e}\")\n",
    "\n",
    "    # Open the input-JSONL-file and the output-JSONL-file\n",
    "    with open(inputFilePath, \"r\") as input_file, open(outputFilePath, \"w\") as output_file:\n",
    "        # Iterate over each line (object) of the input-JSONL-file\n",
    "        for line in input_file:\n",
    "            lineJSON = json.loads(line) # Load the current line as JSON\n",
    "            array = [] #Array for the values of the current line\n",
    "            for key in lineJSON:\n",
    "                array.append(lineJSON[key]) #Append the value for each key to the array\n",
    "            stringJSON = \" \".join(str(item) for item in array) #Create a string with the values of the object for the \"text\" field\n",
    "\n",
    "            # Create an object with the \"doc_id\" (id of the object) and \"text\" fields \n",
    "            finalObject = {\"doc_id\": lineJSON[\"id\"], \"text\": stringJSON}\n",
    "\n",
    "            # Write the finalObject as JSON to the output JSONL file\n",
    "            output_file.write(json.dumps(finalObject) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf1c73",
   "metadata": {},
   "source": [
    "Function to convert the \"inputFile\" to an \"outputFile\" as JSONL-File at the given path by the following steps: <br/> &emsp; 1. First check if \"outputFile\" already exists and if necessary create it. <br/> &emsp; 2. Convert the \"inputFile\" with the \"doc_id\" and \"text\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44131eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:52.336378200Z",
     "start_time": "2023-05-01T21:18:49.739750400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The File dnc-limited-documents.jsonl already exists.\n"
     ]
    }
   ],
   "source": [
    "getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456578a",
   "metadata": {},
   "source": [
    "Execute the function to convert the \"inputFile\" <br/><br/> Output: <br/> &emsp; If file exists or has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d944e",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e92fc7ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:18:54.817586900Z",
     "start_time": "2023-05-01T21:18:52.338036600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile.iranthology\n",
      "#1 transferring dockerfile: 290B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/webis/tira-ir-datasets-starter:0.0.54\n",
      "#3 DONE 0.6s\n",
      "\n",
      "#4 [1/2] FROM docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 59.78MB 0.7s done\n",
      "#5 DONE 0.7s\n",
      "\n",
      "#6 [2/2] COPY iranthology-dnc-limited.py data/dnc-limited-topics.xml  data/dnc-limited-documents.jsonl /usr/lib/python3.8/site-packages/ir_datasets/datasets_in_progress/\n",
      "#6 CACHED\n",
      "\n",
      "#7 exporting to image\n",
      "#7 exporting layers done\n",
      "#7 writing image sha256:fb7900afcc037be917df46496e06d06cb83992a0660a6526982d221a885b5058 done\n",
      "#7 naming to docker.io/library/dnc-limited-ir-dataset done\n",
      "#7 DONE 0.0s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t dnc-limited-ir-dataset -f Dockerfile.iranthology ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1123",
   "metadata": {},
   "source": [
    "Building a docker image for the dataset called \"dnc-limited-ir-dataset\" of the \"Dockerfile.iranthology\" Docker file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1c489d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T21:19:14.118107100Z",
     "start_time": "2023-05-01T21:18:54.820297600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Full-Rank -> create files: \n",
      " documents.jsonl \n",
      " queries.jsonl \n",
      " qrels.txt \n",
      " at /tira-data/output/\n",
      "\n",
      "\n",
      "Load Documents: 0it [00:00, ?it/s]\n",
      "\n",
      "Load Documents: 243it [00:00, 2429.13it/s]\n",
      "\n",
      "Load Documents: 1340it [00:00, 7452.21it/s]\n",
      "\n",
      "Load Documents: 2583it [00:00, 9720.83it/s]\n",
      "\n",
      "Load Documents: 3759it [00:00, 10525.38it/s]\n",
      "\n",
      "Load Documents: 4986it [00:00, 11151.88it/s]\n",
      "\n",
      "Load Documents: 6278it [00:00, 11751.58it/s]\n",
      "\n",
      "Load Documents: 7454it [00:00, 10947.99it/s]\n",
      "\n",
      "Load Documents: 8559it [00:00, 10697.04it/s]\n",
      "\n",
      "Load Documents: 9636it [00:00, 9745.68it/s] \n",
      "\n",
      "Load Documents: 10674it [00:01, 9919.48it/s]\n",
      "\n",
      "Load Documents: 11680it [00:01, 8769.47it/s]\n",
      "\n",
      "Load Documents: 13022it [00:01, 9994.07it/s]\n",
      "\n",
      "Load Documents: 14068it [00:01, 10117.44it/s]\n",
      "\n",
      "Load Documents: 15109it [00:01, 9574.21it/s] \n",
      "\n",
      "Load Documents: 16215it [00:01, 9980.48it/s]\n",
      "\n",
      "Load Documents: 17707it [00:01, 11364.84it/s]\n",
      "\n",
      "Load Documents: 19059it [00:01, 11980.90it/s]\n",
      "\n",
      "Load Documents: 20334it [00:01, 12196.44it/s]\n",
      "\n",
      "Load Documents: 21569it [00:02, 11356.27it/s]\n",
      "\n",
      "Load Documents: 22804it [00:02, 11631.63it/s]\n",
      "\n",
      "Load Documents: 23984it [00:02, 9829.02it/s] \n",
      "\n",
      "Load Documents: 25105it [00:02, 10179.73it/s]\n",
      "\n",
      "Load Documents: 26698it [00:02, 11716.50it/s]\n",
      "\n",
      "Load Documents: 28139it [00:02, 12459.56it/s]\n",
      "\n",
      "Load Documents: 29427it [00:02, 11003.17it/s]\n",
      "\n",
      "Load Documents: 30657it [00:02, 11339.50it/s]\n",
      "\n",
      "Load Documents: 31837it [00:02, 10907.38it/s]\n",
      "\n",
      "Load Documents: 32961it [00:03, 10009.33it/s]\n",
      "\n",
      "Load Documents: 33995it [00:03, 9715.46it/s] \n",
      "\n",
      "Load Documents: 35147it [00:03, 10189.24it/s]\n",
      "\n",
      "Load Documents: 36448it [00:03, 10958.83it/s]\n",
      "\n",
      "Load Documents: 37714it [00:03, 11429.71it/s]\n",
      "\n",
      "Load Documents: 38877it [00:03, 11380.75it/s]\n",
      "\n",
      "Load Documents: 40102it [00:03, 11630.66it/s]\n",
      "\n",
      "Load Documents: 41276it [00:03, 11387.04it/s]\n",
      "\n",
      "Load Documents: 43230it [00:03, 13742.34it/s]\n",
      "\n",
      "Load Documents: 44994it [00:04, 14881.29it/s]\n",
      "\n",
      "Load Documents: 47022it [00:04, 16470.23it/s]\n",
      "\n",
      "Load Documents: 48859it [00:04, 17031.73it/s]\n",
      "\n",
      "Load Documents: 50572it [00:04, 16546.47it/s]\n",
      "\n",
      "Load Documents: 52236it [00:04, 14759.77it/s]\n",
      "\n",
      "Load Documents: 53673it [00:04, 11538.67it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If the directory for the \"tira-run\" output already exists, an output is given\n",
    "if os.path.isdir(irdatasetPath):\n",
    "    print('The folder already exists! Please delete the folder to recreate the data.')\n",
    "#If the directory doesn´t exists, the platform is checked and the required \"tira-run\" command is used\n",
    "else:\n",
    "    if platform.system() == 'Windows':\n",
    "        !tira-run --output-directory %cd%\\dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command \"/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir\"\n",
    "    else:\n",
    "        !tira-run --output-directory ${PWD}/dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command '/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71857d7d",
   "metadata": {},
   "source": [
    "Check, if the tira data already have been created and if it needs, create them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccef287",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd58736",
   "metadata": {},
   "source": [
    "The file \"dnc-limited-topics.xml\", which contains the topics, can be found in the \"data\" directory. <br/>Below you will find the section of the topic with the person who created it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e064ce6",
   "metadata": {},
   "source": [
    "Topic 1 by Constantin Urbainsky:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201486f",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"1\">\n",
    "  <title>machine learnign for more relevant results</title>\n",
    "  <description>Which papers describe methods to find more relevant results using machine learning?</description>\n",
    "  <narrative>\n",
    "      Relevant papers describe one or more methods to find more relevant results using machine learning.\n",
    "      Papers about just machine learning in IR in general or papers just about finding more relevant results are not\n",
    "      relevant.\n",
    "  </narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27547c6",
   "metadata": {},
   "source": [
    "Topic 2 by Nils Harbach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e9086",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"2\">\n",
    "  <title>Crawling websites using machine learning</title>\n",
    "  <description>Papers that describe how to use AI to crawl the context of websites more efficient.</description>\n",
    "  <narrative>Papers in this topic describe methods and algorithms to use machine learning for crawling. They also contain information on the latest research findings on the topic. Papers about crawling methods without AI are not relevant for this topic.</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2b5a8",
   "metadata": {},
   "source": [
    "Topic 3 by Willi Bittorf:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8234aac",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"3\">\n",
    "    <title>Recommenders influence on users</title>\n",
    "    <description>Papers that describe the change in user behaviour because of recommenders?</description>\n",
    "    <narrative>Relevant papers describe how users are affected by recommenders, papers about the recommenders from a technological point of view are not relevant</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348114b",
   "metadata": {},
   "source": [
    "Topic 4 by Tom Paul Gresens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83e3c4",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"4\">\n",
    "    <title>Search engine caching effects</title>\n",
    "    <description>Papers that describe the effects and/or efficient use of search engine caching in terms of result freshness, query latency and other potential advantages or disadvantages </description>\n",
    "    <narrative>Papers in this topic will describe the design trade-off between low latency querying and returning the most recently available results as well as different architectures to create efficient caching systems. Results should not contain any other caching related topics (e.g. hardware or web browsers)</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841ead0",
   "metadata": {},
   "source": [
    "Topic 5 by Dorjan Domi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f8d2c",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"5\">\n",
    "    <title>Consumer Product reviews</title>\n",
    "    <description>Papers that describe the effects of product reviews on consumer decisions</description>\n",
    "    <narrative>Relevant papers would describe the influence that reviews have on on individual decisions of the consumer on whether to buy a product or not. Not relevant papers, would contain other studies about reviews, that are not pertaining to human psychology</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e3470",
   "metadata": {},
   "source": [
    "Topic 6 by Timothy Kriewald:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175c4de",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"6\">\n",
    "    <title>Limitations machine learning</title>\n",
    "    <description>Which papers describe the limitations of machine learning?</description>\n",
    "    <narrative>Relevant papers describe the limitations of machine learning ( e.g. dependence on data quality and quantity, limited ability to handle complex tasks, vulnerability to disturbances and attacks, need for resources and energy). Papers that contains machine learning but not its limitations are not relevant.</narrative>\n",
    "  </topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bf3c1",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999e24a",
   "metadata": {},
   "source": [
    "By Constantin:\n",
    "While working on the first milestone, I was constantly unsure how to proceed. While many tutorials and all of the data were available to us, it was nonetheless confusing. All information was spread out; some of it was in the notes for the lab, some were on the assignment sheet. Moreover, although I followed the tutorial for installing and using tira, the command that worked for my teammates did not for myself. And although we tried our hardest to find out why it wouldn't work for me, we ultimately failed. I was still able to contribute by helping fix problems as they arose, but I wasn't able to actually run tira locally.\n",
    "\n",
    "While on the topic of teamwork, I would say that I have been very fortunate with my group. Everyone was very fun to be around and determined to get this project done. If I had to critique our team, it would have to be in terms of organization. We had quite a bit of trouble finding times for meetings and group-work sessions because of conflicting schedules. On top of that, since all of us got into the project late and as such missed the first week of lab and lectures, we had to play catch-up, which wasn't great.\n",
    "\n",
    "In terms of prior experience, I would say that I didn't have much in the way of experience with the technologies used. Python is a language I never used before, although it wasn't as big a shift from Java as say Haskell was. Docker was used in our project for softwaretech-lab; however, it is still quite foreign to me, and finding my way around it was challenging. The Terminal is also something that I used a bit in our softwaretech-lab and as such somewhat familiarized myself with, however much like Docker, I would say I still have much to learn.\n",
    "\n",
    "By Willi:\n",
    "My primary source of concern during work on our first submission was the task itself, as there was no obvious path on how to complete. All the subtasks were doable and we managed to complete most of them quickly, but we had a hard time understanding what the final result should actually look like\n",
    "The tutorials were helpful but scattered, leading to more confusion while getting to know all the different technologies we're going to use in this course.\n",
    "\n",
    "By Nils:\n",
    "Starting the project was very difficult for me personally, as I didn't have much experience in Python. In addition, the information and tutorials on the project were very widely distributed on all the different platforms, which made it very difficult to read in. Also, in my eyes, the assignment on the sheet was vaguely worded and you didn't know exactly what you had to do now and especially what had to be handed in. This is very frustrating at the beginning when you don't know exactly what to do and you can't develop a clear plan. So we spent a long time developing things that were not required for the first milestone.\n",
    "\n",
    "By Timothy Kriewald: <br/>While working on the first milestone, it became clear that the biggest challenge was understanding the task at hand. As a group, we were often unsure about what was expected of us and how to approach it. Each member had a slightly different understanding, leading to shifts in direction and disagreement throughout the project. Therefor, much of the work that was completed had to be removed in the end.\n",
    "\n",
    "Despite this setback, I can look back positively on our group work. Almost every team member was determined and motivated to complete the task to the best of their abilities. Although there were some difficulties in finding suitable meeting times, we worked continuously. Unfortunately, most of us, including myself, were added to the module a week after the lecture started, which made the start a bit challenging. However, with some time and effort, we were able to overcome it.\n",
    "\n",
    "Potential problems, such as setting up the Jupyter notebook, creating and processing a Docker image, could be prevented through the tutorials. While they were somewhat scattered, they were helpful, and all installations were executed flawlessly by me. Other issues were due to inexperience with Python, but these were comparatively easy to solve through teamwork, StackOverflow, and ChatGPT. Technical problems were much easier and more effective to solve than those resulting from disagreement and misunderstandings. We had to make many changes, causing problems within the program and costing us a lot of time and effort.\n",
    "\n",
    "Overall, the work was interesting, and the group collaboration was better than expected. However, as a group, we need to develop a solid plan and establish a truly unified understanding of the task at the outset to prevent many problems in the long run.\n",
    "\n",
    "By Dorjan:\n",
    "The start of the project was quite rough. We started off with the assignment itself which was quite obscure. We needed to transform this large dataset into a specific new structure such that the result could be used for milestone 2. However, not understanding what milestone 2 really entails makes this part of the assignment very challenging. As a team, we decided to take a particular approach based on our intuition and the given structure in the example of milestone 1. After having completed the main Assignment, however, we started struggling to put this together in a docker image.\n",
    "\n",
    "Unfortunately, since i didn't have any experience with Docker at all, i couldn't be of great help to my teammates in this regard. After several days of pondering on the issue, some of our teammates, that were a bit more well versed in using docker, managed to fix the issue.\n",
    "\n",
    "What i am looking forward to, is a more organized teamwork, that plays to everyones strengths. The hope for the future of this project, is that we can overcome the technical difficulties of making the project work and that we can focus more on the contents of the assignment itself.\n",
    "\n",
    "By Paul:\n",
    "I joined the team later than intended due to a longer period of illness. Despite this setback, my team welcomed me warmly and gave me helpful tips on how to catch up quickly. Although I had never worked with Python or Jupiter before, I understood it quite quickly with the given tutorials, even though some information, including details about the actual assignment, was quite spread out. I'm looking forward to working with the team and contributing my own strengths to the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
