{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d470079",
   "metadata": {},
   "source": [
    "# <ins> Milestone 1 </ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997cda7",
   "metadata": {},
   "source": [
    "The following presents the code for converting the file \"ir-anthology-07-11-2021-ss23\" into a new file with the name \"ir-anthology-final\" in the correct format into the directory \"Created Data\". In addition, this newly created file is registered together with the requested XML file called \"topics\" in \"Tira\". <br/><br/>Afterwards, a reflection will be presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c9955",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2b548",
   "metadata": {},
   "source": [
    "### Formatting \"ir-anthology-07-11-2021-ss23\" with the fields \"doc_id\" and \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfcbfac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.474970Z",
     "end_time": "2023-05-01T12:44:13.478811Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json #Module to work on JSON data\n",
    "import os #Module for interacting with the operating system\n",
    "import platform #Module to check the current platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161d78f",
   "metadata": {},
   "source": [
    "Importing necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff1288a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.478811Z",
     "end_time": "2023-05-01T12:44:13.490073Z"
    }
   },
   "outputs": [],
   "source": [
    "def getDirectory():\n",
    "    return os.path.join(os.getcwd(), \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f1b88",
   "metadata": {},
   "source": [
    "Function to get the current directory in the right format.<br/><br/>Return: <br/> &emsp; Current directory as String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3aba6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.487073Z",
     "end_time": "2023-05-01T12:44:13.494424Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmlFileName = \"dnc-limited-topics.xml\"\n",
    "inputFileName = \"ir-anthology-07-11-2021-ss23.jsonl\"\n",
    "outputFileName = \"dnc-limited-documents.jsonl\"\n",
    "\n",
    "\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    irdatasetName = \"dnc-limited-dataset-tira/\"\n",
    "    directoryData = \"data/\"\n",
    "else:\n",
    "    irdatasetName = \"dnc-limited-dataset-tira\\\\\"\n",
    "    directoryData = \"data\\\\\"\n",
    "\n",
    "inputFilePath = getDirectory() + directoryData + inputFileName\n",
    "outputFilePath = getDirectory() + directoryData + outputFileName\n",
    "xmlFilePath = getDirectory() + directoryData + xmlFileName\n",
    "irdatasetPath = getDirectory() + irdatasetName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9535a3",
   "metadata": {},
   "source": [
    "File names and thier paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc372c51",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.495424Z",
     "end_time": "2023-05-01T12:44:13.503568Z"
    }
   },
   "outputs": [],
   "source": [
    "def checkForFile(file_path, file_name):\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"The File {file_name} already exists.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"The File {file_name} doesn´t exist.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205acd02",
   "metadata": {},
   "source": [
    "Function to check if file already exists at the given path. <br/><br/>Return: <br/> &emsp; True: If file exists <br/> &emsp; False: If file doesn´t exist <br/><br/>Output: <br/> &emsp; Small information text, whether the file exists or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980d4ecf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.503568Z",
     "end_time": "2023-05-01T12:44:13.508709Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName):\n",
    "    ##If the File doesn´t exist\n",
    "    if checkForFile(outputFilePath,outputFileName) == False:\n",
    "        with open(outputFilePath, 'w') as f:\n",
    "            #Try to create the \"outputFile\" as an empty file and give the following output\n",
    "            try:\n",
    "                f.write(json.dumps(\"\"))\n",
    "                print(f\"The File {outputFileName} was created in the following path: {outputFilePath}\")\n",
    "            #If the try failed, return the following output\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred creating the File {outputFileName}: {e}\")\n",
    "\n",
    "    # Open the input-JSONL-file and the output-JSONL-file\n",
    "    with open(inputFilePath, \"r\") as input_file, open(outputFilePath, \"w\") as output_file:\n",
    "        # Iterate over each line (object) of the input-JSONL-file\n",
    "        for line in input_file:\n",
    "            lineJSON = json.loads(line) # Load the current line as JSON\n",
    "            array = [] #Array for the values of the current line\n",
    "            for key in lineJSON:\n",
    "                array.append(lineJSON[key]) #Append the value for each key to the array\n",
    "            stringJSON = \" \".join(str(item) for item in array) #Create a string with the values of the object for the \"text\" field\n",
    "\n",
    "            # Create an object with the \"doc_id\" (id of the object) and \"text\" fields \n",
    "            finalObject = {\"doc_id\": lineJSON[\"id\"], \"text\": stringJSON}\n",
    "\n",
    "            # Write the finalObject as JSON to the output JSONL file\n",
    "            output_file.write(json.dumps(finalObject) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf1c73",
   "metadata": {},
   "source": [
    "Function to convert the \"inputFile\" to an \"outputFile\" as JSONL-File by the following steps: <br/> &emsp; 1. First check if \"outputFile\" already exists and if necessary create it. <br/> &emsp; 2. Convert the \"inputFile\" with the \"doc_id\" and \"text\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44131eb1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:13.508709Z",
     "end_time": "2023-05-01T12:44:15.102315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The File dnc-limited-documents.jsonl already exists.\n"
     ]
    }
   ],
   "source": [
    "getEntriesToJSONL(inputFilePath, outputFilePath, outputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456578a",
   "metadata": {},
   "source": [
    "Execute the function to convert the \"inputFile\" <br/><br/> Output: <br/> &emsp; If file exists or has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d944e",
   "metadata": {},
   "source": [
    "## Register Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9269bbc2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:15.102315Z",
     "end_time": "2023-05-01T12:44:20.708902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load .dockerignore\n",
      "#1 transferring context: 2B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load build definition from Dockerfile.iranthology\n",
      "#2 transferring dockerfile: 290B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/webis/tira-ir-datasets-starter:0.0.54\n",
      "#3 ...\n",
      "\n",
      "#4 [auth] webis/tira-ir-datasets-starter:pull token for registry-1.docker.io\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/webis/tira-ir-datasets-starter:0.0.54\n",
      "#3 DONE 1.2s\n",
      "\n",
      "#5 [1/2] FROM docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 transferring context: 59.78MB 0.4s done\n",
      "#6 DONE 0.4s\n",
      "\n",
      "#5 [1/2] FROM docker.io/webis/tira-ir-datasets-starter:0.0.54@sha256:2d59e9cd38cfdde34662f8fba5b426dd1f2a7b29e54e50ce8be676d0ad3af2ad\n",
      "#5 CACHED\n",
      "\n",
      "#7 [2/2] COPY iranthology-dnc-limited.py data/dnc-limited-topics.xml  data/dnc-limited-documents.jsonl /usr/lib/python3.8/site-packages/ir_datasets/datasets_in_progress/\n",
      "#7 DONE 0.2s\n",
      "\n",
      "#8 exporting to image\n",
      "#8 exporting layers\n",
      "#8 exporting layers 0.2s done\n",
      "#8 writing image sha256:e44b70cb1f24579b602f36842dc26889a9b7f4f44408d676c2da8a83587e9e7d done\n",
      "#8 naming to docker.io/library/dnc-limited-ir-dataset done\n",
      "#8 DONE 0.2s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t dnc-limited-ir-dataset -f Dockerfile.iranthology ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1123",
   "metadata": {},
   "source": [
    "Building a docker image for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5efcbb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T12:44:20.713125Z",
     "end_time": "2023-05-01T12:44:28.191571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running on Windows.\n",
      "Task: Full-Rank -> create files: \n",
      " documents.jsonl \n",
      " queries.jsonl \n",
      " qrels.txt \n",
      " at /tira-data/output/\n",
      "\n",
      "\n",
      "Load Documents: 0it [00:00, ?it/s]\n",
      "\n",
      "Load Documents: 628it [00:00, 6278.27it/s]\n",
      "\n",
      "Load Documents: 1785it [00:00, 9390.09it/s]\n",
      "\n",
      "Load Documents: 2969it [00:00, 10503.71it/s]\n",
      "\n",
      "Load Documents: 4226it [00:00, 11317.41it/s]\n",
      "\n",
      "Load Documents: 5358it [00:00, 10977.39it/s]\n",
      "\n",
      "Load Documents: 6592it [00:00, 11426.85it/s]\n",
      "\n",
      "Load Documents: 7737it [00:00, 11058.05it/s]\n",
      "\n",
      "Load Documents: 8847it [00:00, 10710.67it/s]\n",
      "\n",
      "Load Documents: 9922it [00:00, 10282.12it/s]\n",
      "\n",
      "Load Documents: 10955it [00:01, 10095.82it/s]\n",
      "\n",
      "Load Documents: 12081it [00:01, 10427.82it/s]\n",
      "\n",
      "Load Documents: 13192it [00:01, 10624.12it/s]\n",
      "\n",
      "Load Documents: 14299it [00:01, 10754.28it/s]\n",
      "\n",
      "Load Documents: 15378it [00:01, 9926.26it/s] \n",
      "\n",
      "Load Documents: 16759it [00:01, 11013.69it/s]\n",
      "\n",
      "Load Documents: 18135it [00:01, 11787.23it/s]\n",
      "\n",
      "Load Documents: 19330it [00:01, 11661.19it/s]\n",
      "\n",
      "Load Documents: 20607it [00:01, 11979.18it/s]\n",
      "\n",
      "Load Documents: 21814it [00:01, 11571.46it/s]\n",
      "\n",
      "Load Documents: 22980it [00:02, 11518.85it/s]\n",
      "\n",
      "Load Documents: 24138it [00:02, 11260.70it/s]\n",
      "\n",
      "Load Documents: 25269it [00:02, 11250.33it/s]\n",
      "\n",
      "Load Documents: 27069it [00:02, 13212.54it/s]\n",
      "\n",
      "Load Documents: 28461it [00:02, 13417.84it/s]\n",
      "\n",
      "Load Documents: 29810it [00:02, 12534.11it/s]\n",
      "\n",
      "Load Documents: 31085it [00:02, 12594.65it/s]\n",
      "\n",
      "Load Documents: 32356it [00:02, 11958.51it/s]\n",
      "\n",
      "Load Documents: 33565it [00:02, 11024.58it/s]\n",
      "\n",
      "Load Documents: 34687it [00:03, 10643.09it/s]\n",
      "\n",
      "Load Documents: 35765it [00:03, 10566.01it/s]\n",
      "\n",
      "Load Documents: 36988it [00:03, 11021.12it/s]\n",
      "\n",
      "Load Documents: 38252it [00:03, 11475.61it/s]\n",
      "\n",
      "Load Documents: 39410it [00:03, 10955.43it/s]\n",
      "\n",
      "Load Documents: 40765it [00:03, 11679.59it/s]\n",
      "\n",
      "Load Documents: 42117it [00:03, 12205.79it/s]\n",
      "\n",
      "Load Documents: 44046it [00:03, 14262.14it/s]\n",
      "\n",
      "Load Documents: 45487it [00:03, 13759.78it/s]\n",
      "\n",
      "Load Documents: 47577it [00:04, 15805.56it/s]\n",
      "\n",
      "Load Documents: 49533it [00:04, 16895.63it/s]\n",
      "\n",
      "Load Documents: 51239it [00:04, 15925.73it/s]\n",
      "\n",
      "Load Documents: 52854it [00:04, 15544.95it/s]\n",
      "\n",
      "Load Documents: 53673it [00:04, 12062.69it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(irdatasetPath):\n",
    "    print(irdatasetPath)\n",
    "    print('The folder already exists! Please delete the folder to recreate the data.')\n",
    "else:\n",
    "    if platform.system() == 'Windows':\n",
    "        print(\"You are running on Windows.\")\n",
    "        !tira-run --output-directory %cd%\\dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command \"/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir\"\n",
    "    else:\n",
    "        print(\"You are running on Linux.\")\n",
    "        !tira-run --output-directory ${PWD}/dnc-limited-dataset-tira --image dnc-limited-ir-dataset --allow-network true --command '/irds_cli.sh --ir_datasets_id iranthology-dnc-limited --output_dataset_path $outputDir'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check, if the tira data already have been created and if needs, create them!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "7ccef287",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd58736",
   "metadata": {},
   "source": [
    "The file \"topics.xml\", which contains the topics, can be found in the \"CreatedData\" directory. <br/>Below you will find the section of the topic with the person who created it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e064ce6",
   "metadata": {},
   "source": [
    "Topic 1 by Constantin Urbainsky:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```xml\n",
    "<topic number=\"1\">\n",
    "  <title>machine learnign for more relevant results</title>\n",
    "  <description>Which papers describe methods to find more relevant results using machine learning?</description>\n",
    "  <narrative>\n",
    "      Relevant papers describe one or more methods to find more relevant results using machine learning.\n",
    "      Papers about just machine learning in IR in general or papers just about finding more relevant results are not\n",
    "      relevant.\n",
    "  </narrative>\n",
    "</topic>\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "d27547c6",
   "metadata": {},
   "source": [
    "Topic 2 by Nils Harbach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131e9086",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"2\">\n",
    "  <title>Crawling websites using machine learning</title>\n",
    "  <description>Papers that describe how to use AI to crawl the context of websites more efficient.</description>\n",
    "  <narrative>Papers in this topic describe methods and algorithms to use machine learning for crawling. They also contain information on the latest research findings on the topic. Papers about crawling methods without AI are not relevant for this topic.</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2b5a8",
   "metadata": {},
   "source": [
    "Topic 3 by Willi Bittorf:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8234aac",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"3\">\n",
    "    <title>Recommenders influence on users</title>\n",
    "    <description>Papers that describe the change in user behaviour because of recommenders?</description>\n",
    "    <narrative>Relevant papers describe how users are affected by recommenders, papers about the recommenders from a technological point of view are not relevant</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348114b",
   "metadata": {},
   "source": [
    "Topic 4 by Tom Paul Gresens:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83e3c4",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"4\">\n",
    "    <title>Search engine caching effects</title>\n",
    "    <description>Papers that describe the effects and/or efficient use of search engine caching in terms of result freshness, query latency and other potential advantages or disadvantages </description>\n",
    "    <narrative>Papers in this topic will describe the design trade-off between low latency querying and returning the most recently available results as well as different architectures to create efficient caching systems. Results should not contain any other caching related topics (e.g. hardware or web browsers)</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841ead0",
   "metadata": {},
   "source": [
    "Topic 5 by Dorjan Domi:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f8d2c",
   "metadata": {},
   "source": [
    "```xml\n",
    "<topic number=\"5\">\n",
    "    <title>Consumer Product reviews</title>\n",
    "    <description>Papers that describe the effects of product reviews on consumer decisions</description>\n",
    "    <narrative>Relevant papers would describe the influence that reviews have on on individual decisions of the consumer on whether to buy a product or not. Not relevant papers, would contain other studies about reviews, that are not pertaining to human psychology</narrative>\n",
    "</topic>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Topic 6 by Timothy Kriewald:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```xml\n",
    "<topic number=\"6\">\n",
    "    <title>Limitations machine learning</title>\n",
    "    <description>Which papers describe the limitations of machine learning?</description>\n",
    "    <narrative>Relevant papers describe the limitations of machine learning ( e.g. dependence on data quality and quantity, limited ability to handle complex tasks, vulnerability to disturbances and attacks, need for resources and energy). Papers that contains machine learning but not its limitations are not relevant.</narrative>\n",
    "  </topic>\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "387bf3c1",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999e24a",
   "metadata": {},
   "source": [
    "By Constantin:\n",
    "While working on the first milestone, I was constantly unsure how to proceed. While many tutorials and all of the data were available to us, it was nonetheless confusing. All information was spread out; some of it was in the notes for the lab, some were on the assignment sheet. Moreover, although I followed the tutorial for installing and using tira, the command that worked for my teammates did not for myself. And although we tried our hardest to find out why it wouldn't work for me, we ultimately failed. I was still able to contribute by helping fix problems as they arose, but I wasn't able to actually run tira locally.\n",
    "\n",
    "While on the topic of teamwork, I would say that I have been very fortunate with my group. Everyone was very fun to be around and determined to get this project done. If I had to critique our team, it would have to be in terms of organization. We had quite a bit of trouble finding times for meetings and group-work sessions because of conflicting schedules. On top of that, since all of us got into the project late and as such missed the first week of lab and lectures, we had to play catch-up, which wasn't great.\n",
    "\n",
    "In terms of prior experience, I would say that I didn't have much in the way of experience with the technologies used. Python is a language I never used before, although it wasn't as big a shift from Java as say Haskell was. Docker was used in our project for softwaretech-lab; however, it is still quite foreign to me, and finding my way around it was challenging. The Terminal is also something that I used a bit in our softwaretech-lab and as such somewhat familiarized myself with, however much like Docker, I would say I still have much to learn.\n",
    "\n",
    "By Willi:\n",
    "My primary source of concern during work on our first submission was the task itself, as there was no obvious path on how to complete. All the subtasks were doable and we managed to complete most of them quickly, but we had a hard time understanding what the final result should actually look like\n",
    "The tutorials were helpful but scattered, leading to more confusion while getting to know all the different technologies we're going to use in this course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
