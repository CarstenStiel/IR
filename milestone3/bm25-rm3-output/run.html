<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="icon"
    href="https://raw.githubusercontent.com/capreolus-ir/diffir/master/docs/images/icon.png">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <title>diffir: IR model comparision</title>
  <style>
    .card {
      margin: 5px !important;
    }

    .highlight {
      background-color: #ffffd3;
    }

    #DocumentOverlay {
      position: fixed;
      top: 0;
      bottom: 0;
      left: 0;
      right: 0;
      background-color: rgba(0, 0, 0, .25);
    }

    #DocumentDetails {
      position: fixed;
      top: 60px;
      left: 10%;
      right: 10%;
      bottom: 60px;
      background-color: white;
      padding: 20px;
      border: 1px solid rgba(0, 0, 0, .125);
      border-radius: 0.25rem;
      box-shadow: 0 0 16px black;
      overflow: auto;
    }

    .close-overlay {
      position: absolute;
      top: 4px;
      right: 4px;
      border-radius: 100%;
      background-color: #111;
      font-size: 17px;
      padding: 9px;
      color: white;
      width: 30px;
      height: 30px;
      text-align: center;
      font-weight: normal;
      line-height: 11px;
      cursor: pointer;
    }

    .docid {
      background-color: rgb(224, 135, 55);
      position: absolute;
      top: 0;
      bottom: 0;
      width: 20px;
      overflow: hidden;
      margin-bottom: 0;
      border-radius: 0;
      font-weight: normal;
      white-space: nowrap;
    }

    .docid-value {
      transform: rotate(90deg);
      font-size: 0.7em;
      padding-left: 8px;
    }

    .fields th {
      vertical-align: top;
      text-align: right;
      padding-right: 12px;
      color: #999;
      font-weight: normal;
    }

    #query-container {
      max-width: 600px;
      border: 1px solid #999;
      border-radius: 0.25rem;
      margin: 20px auto;
      padding: 10px;
    }

    .other-rank {
      font-size: 1.2em;
      display: inline-block;
      width: 20px;
      margin-top: 46px;
      margin-left: 3px;
      margin-right: 3px;
      cursor: help;
    }

    mark {
      padding: 0;
      font-weight: bold;
    }

    .snippet {
      font-size: 0.9em;
      line-height: 1.2;
    }

    .elip {
      text-align: center;
      margin: 16px;
      color: gray;
    }

    .doc-info {
      white-space: nowrap;
    }

    .card-header {
      min-height: 128px;
      cursor: pointer;
    }

    .swatch {
      display: inline-block;
      width: 16px;
      height: 16px;
      vertical-align: middle;
    }

    .form-group {
      width: 150px;
      height: 20px;
      padding-left: 10px;
      padding-top: 10px;
    }

    .nobackground {
      background: transparent !important;
      font-weight: normal;
    }
    .styled-table {
      margin-left: 0px;
      margin-top: 10px;
      border-collapse: collapse;    
      font-size: 0.9em;
      /* font-family: sans-serif;       */
      min-width: 350px;      
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    }
    .styled-table thead tr {
      background-color: #17a2b8;
      color: #ffffff;
      text-align: left;
    }    
    /* .styled-table th, */
    .styled-table td {
      /* padding: 12px 15px; */
      text-align: center;
    }    
    .styled-table tbody tr {
      color: #ffffff;
    }
    /*
    .styled-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3;
    } */

    .styled-table tbody tr:last-of-type {
      border-bottom: 2px solid #009879;
    }

    #ranking-summary ul li span {
      margin-right: 5px;
    }    
  </style>
</head>

<body>
  <header>
    <div class="collapse bg-dark" id="navbarHeader">
      <div class="container">
        <div class="row">
          <div class="col-sm-6 col-md-6 py-4">
            <h6 class="text-white">Summary</h6>
            <p class="text-white" id="ranking-summary"></p>
          </div>
          <div class="col-sm-5 col-md-5 py-4"> 
            <h6 class="text-white">Ranking statistics</h6>                                   
            <ul>
              <li class="text-white"><span id="contrast-measure"></span></li>
              <li class="text-white"> <span>Relevance metrics</span> <div id="metrics"></div></li>
            </ul>                        
          </div>
        </div>
      </div>
    </div>
    <div class="navbar navbar-dark bg-dark box-shadow navbar-fixed-top">
      <div class="container d-flex justify-content-between">
        <a href="#" class="navbar-brand d-flex align-items-center">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-search"
            viewBox="0 0 16 16">
            <path
              d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z" />
          </svg>
          <strong style="padding-left:  5px;">DiffIR</strong>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarHeader"
          aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </header>
  <div class="container">
    <div class="input-group" style="padding-top: 50px; padding-bottom: 10px; background-color: white;">
      <select id="Queries" data-width="100%" data-style="border" data-container="body"></select>
    </div>
    <div id="query-container" class="sticky-top" style="background-color: white;">
      <div style="position:relative">
        <button id="query-collapse-btn" style="position: absolute; top: 12px; right: 8px;" type="button"
          class="btn btn-outline-info btn-sm" data-toggle="collapse" data-target=".query_collapse" aria-expanded="false"
          aria-controls="query_collapse">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
            class="bi bi-arrows-angle-expand" viewBox="0 0 16 16">
            <path fill-rule="evenodd"
              d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z" />
          </svg>
        </button>
      </div>
      <div id="Query" style="padding-right: 45px;">
      </div>
    </div>
    <div class="row justify-content-center" id="runName">
      <div class="col">
        <h6 id="Run1Name" style="text-align: center;"></h6>
      </div>
      <div class="col">
        <h6 id="Run2Name" style="text-align: center;"></h6>
      </div>
    </div>
    <div class="row" id="docList">
      <div id="Run1Docs" class="col">
      </div>
      <div id="Run2Docs" class="col">
      </div>
    </div>    
  </div>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>
  <script type="text/javascript">
    var data = {"meta": {"run1_name": "/tira-data/output/run.txt", "run2_name": null, "dataset": "iranthology-dnc-limited", "measure": "topk", "qrelDefs": {"0": "Not Relevant", "1": "Relevant"}, "queryFields": ["query_id", "title", "description", "narrative"], "docFields": ["doc_id", "text"], "relevanceColors": {"null": "#888888", "0": "#d54541", "1": "#52b262"}}, "queries": [{"fields": {"query_id": "1", "title": "machine learning for more relevant results", "description": "Which papers describe methods to find more relevant results using machine learning?", "narrative": "Relevant papers describe one or more methods to find more relevant results using machine learning. Papers about just machine learning in IR in general or papers just about finding more relevant results are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [0.6], "P@10": [0.5], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [0.7227265726449519], "nDCG@10": [0.6086182388744239]}, "run_1": [{"doc_id": "2007.sigirconf_conference-2007.39", "score": 20.016769833011615, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[675, 683, 1.0], [807, 815, 1.0], [833, 840, 1.0], [841, 849, 1.0], [1003, 1010, 1.0], [1092, 1099, 1.0], [1440, 1448, 1.0], [1500, 1507, 1.0], [1586, 1593, 1.0], [1703, 1711, 1.0]]}, "snippet": {"field": "default_text", "start": 670, "stop": 870, "weights": [[5, 13, 1.0], [137, 145, 1.0], [163, 170, 1.0], [171, 179, 1.0]]}}, {"doc_id": "2009.cikm_conference-2009.190", "score": 17.390856083132412, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[562, 569, 1.0], [639, 646, 1.0], [709, 717, 1.0], [925, 932, 1.0], [979, 986, 1.0], [1170, 1177, 1.0]]}, "snippet": {"field": "default_text", "start": 557, "stop": 757, "weights": [[5, 12, 1.0], [82, 89, 1.0], [152, 160, 1.0]]}}, {"doc_id": "2012.sigirconf_conference-2012.5", "score": 17.35222734100875, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[663, 670, 1.0], [671, 679, 1.0], [769, 777, 1.0], [831, 838, 1.0], [839, 847, 1.0], [1058, 1065, 1.0], [1066, 1074, 1.0], [1134, 1142, 1.0], [1535, 1543, 1.0], [1558, 1565, 1.0], [1566, 1574, 1.0]]}, "snippet": {"field": "default_text", "start": 658, "stop": 858, "weights": [[5, 12, 1.0], [13, 21, 1.0], [111, 119, 1.0], [173, 180, 1.0], [181, 189, 1.0]]}}, {"doc_id": "2016.airs_conference-2016.27", "score": 16.210996491918525, "relevance": null, "rank": 4, "weights": {"doc_id": [], "default_text": [[880, 887, 1.0], [888, 896, 1.0], [1484, 1491, 1.0], [1565, 1573, 1.0]]}, "snippet": {"field": "default_text", "start": 875, "stop": 1075, "weights": [[5, 12, 1.0], [13, 21, 1.0]]}}, {"doc_id": "2009.cikm_conference-2009.296", "score": 15.793770455201358, "relevance": null, "rank": 5, "weights": {"doc_id": [], "default_text": [[1237, 1244, 1.0], [1389, 1396, 1.0], [1397, 1405, 1.0]]}, "snippet": {"field": "default_text", "start": 1232, "stop": 1432, "weights": [[5, 12, 1.0], [157, 164, 1.0], [165, 173, 1.0]]}}, {"doc_id": "2007.ntcir_workshop-2007.5", "score": 15.534645647209645, "relevance": null, "rank": 6, "weights": {"doc_id": [], "default_text": [[945, 952, 1.0], [953, 961, 1.0], [1120, 1127, 1.0], [1352, 1359, 1.0], [1360, 1368, 1.0]]}, "snippet": {"field": "default_text", "start": 940, "stop": 1140, "weights": [[5, 12, 1.0], [13, 21, 1.0], [180, 187, 1.0]]}}, {"doc_id": "2012.wsdm_conference-2012.45", "score": 15.26478033166901, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[613, 620, 1.0], [707, 714, 1.0], [715, 723, 1.0]]}, "snippet": {"field": "default_text", "start": 608, "stop": 808, "weights": [[5, 12, 1.0], [99, 106, 1.0], [107, 115, 1.0]]}}, {"doc_id": "2010.cikm_conference-2010.189", "score": 14.953271645915722, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[534, 541, 1.0], [695, 703, 1.0], [1170, 1178, 1.0], [1740, 1748, 1.0]]}, "snippet": {"field": "default_text", "start": 529, "stop": 729, "weights": [[5, 12, 1.0], [166, 174, 1.0]]}}, {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A4.3", "score": 14.859428308821117, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[732, 740, 1.0], [925, 933, 1.0], [1310, 1318, 1.0], [1465, 1473, 1.0], [1487, 1495, 1.0], [1642, 1650, 1.0]]}, "snippet": {"field": "default_text", "start": 1305, "stop": 1505, "weights": [[5, 13, 1.0], [160, 168, 1.0], [182, 190, 1.0]]}}, {"doc_id": "2012.sigirconf_conference-2012.31", "score": 14.590057899104487, "relevance": null, "rank": 10, "weights": {"doc_id": [], "default_text": [[583, 590, 1.0], [1113, 1120, 1.0], [1367, 1374, 1.0], [1514, 1522, 1.0]]}, "snippet": {"field": "default_text", "start": 1362, "stop": 1562, "weights": [[5, 12, 1.0], [152, 160, 1.0]]}}], "run_2": [], "summary": [["5 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "2", "title": "Crawling websites using machine learning", "description": "Papers that describe how to use AI to crawl the context of websites more efficient.", "narrative": "Papers in this topic describe methods and algorithms to use machine learning for crawling. They also contain information on the latest research findings on the topic. Papers about crawling methods without AI are not relevant for this topic.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.3333333333333333], "P@5": [0.4], "P@10": [0.2], "nDCG@1": [0.0], "nDCG@3": [0.23463936301137822], "nDCG@5": [0.30078518014914984], "nDCG@10": [0.2084508053262215]}, "run_1": [{"doc_id": "2010.wwwconf_conference-2010.62", "score": 25.509627480395437, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[596, 604, 1.0], [704, 709, 1.0], [876, 884, 1.0], [983, 991, 1.0], [1755, 1763, 1.0], [1802, 1810, 1.0], [2096, 2104, 1.0], [2280, 2288, 1.0]]}, "snippet": {"field": "default_text", "start": 591, "stop": 791, "weights": [[5, 13, 1.0], [113, 118, 1.0]]}}, {"doc_id": "2006.wwwconf_conference-2006.195", "score": 24.421544961877082, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[570, 578, 1.0], [591, 599, 1.0], [734, 742, 1.0], [1206, 1214, 1.0], [1256, 1264, 1.0], [1335, 1343, 1.0]]}, "snippet": {"field": "default_text", "start": 565, "stop": 765, "weights": [[5, 13, 1.0], [26, 34, 1.0], [169, 177, 1.0]]}}, {"doc_id": "2014.wwwconf_conference-2014c.198", "score": 20.68438584175763, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[506, 514, 1.0], [619, 627, 1.0], [804, 812, 1.0], [1839, 1844, 1.0], [1889, 1894, 1.0], [1897, 1904, 1.0], [1905, 1913, 1.0], [2309, 2317, 1.0], [2563, 2571, 1.0]]}, "snippet": {"field": "default_text", "start": 1834, "stop": 2034, "weights": [[5, 10, 1.0], [55, 60, 1.0], [63, 70, 1.0], [71, 79, 1.0]]}}, {"doc_id": "2010.cikm_conference-2010.192", "score": 20.536516515201672, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[516, 524, 1.0], [700, 708, 1.0], [1077, 1085, 1.0], [1172, 1180, 1.0], [1536, 1541, 1.0], [1648, 1653, 1.0], [1856, 1864, 1.0], [1865, 1870, 1.0]]}, "snippet": {"field": "default_text", "start": 511, "stop": 711, "weights": [[5, 13, 1.0], [189, 197, 1.0]]}}, {"doc_id": "2013.wwwconf_conference-2013.41", "score": 20.301979609329365, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[667, 675, 1.0], [895, 903, 1.0], [1075, 1083, 1.0], [1267, 1275, 1.0], [1544, 1552, 1.0], [1600, 1608, 1.0], [1643, 1648, 1.0], [3536, 3544, 1.0], [3919, 3927, 1.0], [4181, 4189, 1.0], [4271, 4279, 1.0], [4450, 4458, 1.0], [4472, 4480, 1.0], [4701, 4709, 1.0], [4812, 4820, 1.0], [5255, 5263, 1.0], [5343, 5350, 1.0], [5351, 5359, 1.0], [5849, 5854, 1.0], [5902, 5910, 1.0], [6133, 6141, 1.0], [6174, 6179, 1.0], [6422, 6427, 1.0], [6468, 6476, 1.0], [7235, 7240, 1.0], [7710, 7718, 1.0], [8274, 8282, 1.0], [8675, 8683, 1.0], [8920, 8928, 1.0], [10245, 10253, 1.0], [10270, 10275, 1.0], [10380, 10385, 1.0], [10454, 10462, 1.0], [10463, 10468, 1.0], [10493, 10501, 1.0], [10590, 10598, 1.0], [10795, 10803, 1.0], [10954, 10962, 1.0], [11001, 11006, 1.0], [11342, 11350, 1.0], [11791, 11799, 1.0], [11975, 11983, 1.0], [12446, 12451, 1.0], [12571, 12576, 1.0], [12890, 12895, 1.0], [13770, 13775, 1.0], [16304, 16309, 1.0], [17184, 17191, 1.0], [17700, 17705, 1.0], [17957, 17965, 1.0], [18512, 18517, 1.0], [18974, 18979, 1.0], [20545, 20553, 1.0], [20767, 20772, 1.0], [22422, 22427, 1.0], [23283, 23291, 1.0], [25643, 25651, 1.0], [26848, 26856, 1.0], [26991, 26996, 1.0], [27063, 27070, 1.0], [27071, 27079, 1.0], [27907, 27915, 1.0], [27931, 27939, 1.0], [29520, 29527, 1.0], [29528, 29536, 1.0], [29913, 29918, 1.0], [30016, 30021, 1.0], [30104, 30109, 1.0], [32470, 32475, 1.0]]}, "snippet": {"field": "default_text", "start": 10265, "stop": 10465, "weights": [[5, 10, 1.0], [115, 120, 1.0], [189, 197, 1.0], [198, 203, 1.0]]}}, {"doc_id": "2013.cikm_conference-2013.177", "score": 20.19734170987231, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[1534, 1542, 1.0], [1612, 1620, 1.0], [1830, 1838, 1.0]]}, "snippet": {"field": "default_text", "start": 1529, "stop": 1729, "weights": [[5, 13, 1.0], [83, 91, 1.0]]}}, {"doc_id": "2006.sigirconf_conference-2006.11", "score": 19.804495521889073, "relevance": 0, "rank": 7, "weights": {"doc_id": [], "default_text": [[695, 703, 1.0], [772, 780, 1.0], [811, 819, 1.0], [879, 887, 1.0], [941, 949, 1.0], [988, 996, 1.0], [1131, 1139, 1.0], [1733, 1741, 1.0], [1802, 1810, 1.0], [2194, 2202, 1.0]]}, "snippet": {"field": "default_text", "start": 690, "stop": 890, "weights": [[5, 13, 1.0], [82, 90, 1.0], [121, 129, 1.0], [189, 197, 1.0]]}}, {"doc_id": "2014.sigirconf_conference-2014.179", "score": 19.802536431072582, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[768, 776, 1.0], [799, 807, 1.0], [865, 873, 1.0], [932, 940, 1.0]]}, "snippet": {"field": "default_text", "start": 763, "stop": 963, "weights": [[5, 13, 1.0], [36, 44, 1.0], [102, 110, 1.0], [169, 177, 1.0]]}}, {"doc_id": "2010.wsdm_conference-2010.39", "score": 19.188183314154582, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[634, 642, 1.0], [831, 836, 1.0], [1194, 1201, 1.0], [1202, 1210, 1.0], [1482, 1487, 1.0], [1806, 1811, 1.0], [1837, 1845, 1.0]]}, "snippet": {"field": "default_text", "start": 629, "stop": 829, "weights": [[5, 13, 1.0]]}}, {"doc_id": "2006.wwwconf_conference-2006.32", "score": 18.526294208982023, "relevance": null, "rank": 10, "weights": {"doc_id": [], "default_text": [[497, 505, 1.0], [527, 535, 1.0], [653, 658, 1.0], [673, 681, 1.0], [733, 741, 1.0], [784, 792, 1.0], [1057, 1065, 1.0], [1115, 1123, 1.0], [1138, 1146, 1.0], [1190, 1198, 1.0], [1279, 1287, 1.0], [1302, 1310, 1.0], [1551, 1559, 1.0], [1598, 1606, 1.0]]}, "snippet": {"field": "default_text", "start": 1110, "stop": 1310, "weights": [[5, 13, 1.0], [28, 36, 1.0], [80, 88, 1.0], [169, 177, 1.0], [192, 200, 1.0]]}}], "run_2": [], "summary": [["1 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "3", "title": "Recommenders influence on users", "description": "Papers that describe the change in user behaviour because of recommenders?", "narrative": "Relevant papers describe how users are affected by recommenders, papers about the recommenders from a technological point of view are not relevant", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.0], "P@5": [0.0], "P@10": [0.0], "nDCG@1": [0.0], "nDCG@3": [0.0], "nDCG@5": [0.0], "nDCG@10": [0.0]}, "run_1": [{"doc_id": "2011.sigirconf_conference-2011.35", "score": 27.041238701044875, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[739, 748, 1.0], [766, 775, 1.0], [906, 915, 1.0], [972, 981, 1.0], [1102, 1111, 1.0], [1301, 1310, 1.0], [1449, 1458, 1.0], [1476, 1485, 1.0], [1827, 1836, 1.0]]}, "snippet": {"field": "default_text", "start": 734, "stop": 934, "weights": [[5, 14, 1.0], [32, 41, 1.0], [172, 181, 1.0]]}}, {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11", "score": 26.578290398355126, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[503, 508, 1.0], [601, 606, 1.0], [665, 670, 1.0], [729, 734, 1.0], [833, 842, 1.0], [906, 915, 1.0], [1162, 1171, 1.0], [1199, 1208, 1.0], [1317, 1326, 1.0], [1497, 1506, 1.0], [1533, 1542, 1.0], [1962, 1967, 1.0]]}, "snippet": {"field": "default_text", "start": 498, "stop": 698, "weights": [[5, 10, 1.0], [103, 108, 1.0], [167, 172, 1.0]]}}, {"doc_id": "2014.cikm_conference-2014.67", "score": 25.176509598281285, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[756, 761, 1.0], [818, 823, 1.0], [862, 867, 1.0], [951, 956, 1.0], [1014, 1019, 1.0], [1439, 1448, 1.0], [1462, 1471, 1.0]]}, "snippet": {"field": "default_text", "start": 751, "stop": 951, "weights": [[5, 10, 1.0], [67, 72, 1.0], [111, 116, 1.0]]}}, {"doc_id": "2018.sigirconf_conference-2018.21", "score": 24.097847577777113, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[683, 692, 1.0], [699, 704, 1.0], [929, 934, 1.0], [970, 979, 1.0], [1028, 1033, 1.0], [1196, 1205, 1.0], [1293, 1302, 1.0], [1539, 1544, 1.0], [1620, 1629, 1.0], [1636, 1641, 1.0], [1798, 1803, 1.0], [1917, 1926, 1.0], [2113, 2122, 1.0], [2129, 2134, 1.0]]}, "snippet": {"field": "default_text", "start": 924, "stop": 1124, "weights": [[5, 10, 1.0], [46, 55, 1.0], [104, 109, 1.0]]}}, {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.10", "score": 23.78689642082826, "relevance": null, "rank": 5, "weights": {"doc_id": [], "default_text": [[799, 804, 1.0], [1278, 1287, 1.0]]}, "snippet": {"field": "default_text", "start": 794, "stop": 994, "weights": [[5, 10, 1.0]]}}, {"doc_id": "2020.ecir_conference-20201.14", "score": 21.885973124396546, "relevance": null, "rank": 6, "weights": {"doc_id": [], "default_text": [[721, 726, 1.0], [1023, 1032, 1.0], [1081, 1086, 1.0], [1252, 1257, 1.0], [1413, 1418, 1.0], [2077, 2082, 1.0]]}, "snippet": {"field": "default_text", "start": 1018, "stop": 1218, "weights": [[5, 14, 1.0], [63, 68, 1.0]]}}, {"doc_id": "2016.tist_journal-ir0anthology0volumeA8A1.9", "score": 21.788298186694362, "relevance": null, "rank": 7, "weights": {"doc_id": [], "default_text": [[521, 526, 1.0], [729, 734, 1.0], [897, 902, 1.0], [1080, 1085, 1.0], [1272, 1277, 1.0], [1316, 1325, 1.0], [1430, 1435, 1.0], [1467, 1472, 1.0], [1527, 1532, 1.0], [1729, 1734, 1.0], [2136, 2141, 1.0], [2169, 2178, 1.0]]}, "snippet": {"field": "default_text", "start": 1267, "stop": 1467, "weights": [[5, 10, 1.0], [49, 58, 1.0], [163, 168, 1.0]]}}, {"doc_id": "2018.cikm_conference-2018.98", "score": 21.374443710978206, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[899, 908, 1.0], [912, 917, 1.0], [1033, 1038, 1.0], [1141, 1150, 1.0], [1154, 1159, 1.0], [1254, 1259, 1.0], [1455, 1464, 1.0], [1468, 1473, 1.0], [1507, 1516, 1.0], [1540, 1549, 1.0], [2042, 2047, 1.0], [2080, 2089, 1.0], [2106, 2115, 1.0]]}, "snippet": {"field": "default_text", "start": 1450, "stop": 1650, "weights": [[5, 14, 1.0], [18, 23, 1.0], [57, 66, 1.0], [90, 99, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.45", "score": 21.303052805223405, "relevance": null, "rank": 9, "weights": {"doc_id": [], "default_text": [[1178, 1183, 1.0], [1520, 1529, 1.0], [1543, 1552, 1.0]]}, "snippet": {"field": "default_text", "start": 1515, "stop": 1715, "weights": [[5, 14, 1.0], [28, 37, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020.114", "score": 20.988237170057825, "relevance": null, "rank": 10, "weights": {"doc_id": [], "default_text": [[779, 784, 1.0], [1025, 1034, 1.0], [1293, 1302, 1.0]]}, "snippet": {"field": "default_text", "start": 774, "stop": 974, "weights": [[5, 10, 1.0]]}}], "run_2": [], "summary": [["5 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "4", "title": "Search engine caching effects", "description": "Papers that describe the effects and/or efficient use of search engine caching in terms of result freshness, query latency and other potential advantages or disadvantages ", "narrative": "Papers in this topic will describe the design trade-off between low latency querying and returning the most recently available results as well as different architectures to create efficient caching systems. Results should not contain any other caching related topics (e.g. hardware or web browsers)", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [1.0], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [1.0]}, "run_1": [{"doc_id": "2008.tweb_journal-ir0anthology0volumeA2A4.2", "score": 24.87296305814977, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[534, 541, 1.0], [558, 564, 1.0], [648, 655, 1.0], [661, 668, 1.0], [687, 694, 1.0], [781, 788, 1.0], [813, 820, 1.0], [869, 876, 1.0], [930, 937, 1.0], [1197, 1204, 1.0], [1478, 1484, 1.0], [1485, 1491, 1.0], [1492, 1499, 1.0]]}, "snippet": {"field": "default_text", "start": 529, "stop": 729, "weights": [[5, 12, 1.0], [29, 35, 1.0], [119, 126, 1.0], [132, 139, 1.0], [158, 165, 1.0]]}}, {"doc_id": "2003.wwwconf_conference-2003.3", "score": 24.263796376854792, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[447, 454, 1.0], [484, 490, 1.0], [508, 514, 1.0], [587, 594, 1.0], [765, 772, 1.0], [773, 779, 1.0], [831, 837, 1.0], [838, 844, 1.0], [919, 925, 1.0], [926, 932, 1.0], [1002, 1009, 1.0], [1129, 1135, 1.0], [1365, 1372, 1.0], [1409, 1415, 1.0]]}, "snippet": {"field": "default_text", "start": 760, "stop": 960, "weights": [[5, 12, 1.0], [13, 19, 1.0], [71, 77, 1.0], [78, 84, 1.0], [159, 165, 1.0], [166, 172, 1.0]]}}, {"doc_id": "2007.sigirconf_conference-2007.26", "score": 23.609345959230943, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[692, 699, 1.0], [716, 722, 1.0], [806, 813, 1.0], [819, 826, 1.0], [845, 852, 1.0], [938, 945, 1.0], [970, 977, 1.0], [1026, 1033, 1.0], [1087, 1094, 1.0], [1351, 1358, 1.0], [1624, 1631, 1.0], [1635, 1641, 1.0]]}, "snippet": {"field": "default_text", "start": 687, "stop": 887, "weights": [[5, 12, 1.0], [29, 35, 1.0], [119, 126, 1.0], [132, 139, 1.0], [158, 165, 1.0]]}}, {"doc_id": "2006.tois_journal-ir0anthology0volumeA24A1.1", "score": 22.512908288006237, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[474, 481, 1.0], [524, 530, 1.0], [531, 537, 1.0], [590, 597, 1.0], [1571, 1578, 1.0], [1753, 1759, 1.0], [1769, 1776, 1.0]]}, "snippet": {"field": "default_text", "start": 469, "stop": 669, "weights": [[5, 12, 1.0], [55, 61, 1.0], [62, 68, 1.0], [121, 128, 1.0]]}}, {"doc_id": "2013.tweb_journal-ir0anthology0volumeA8A1.2", "score": 22.17127318457944, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[401, 407, 1.0], [587, 593, 1.0], [974, 981, 1.0], [1422, 1429, 1.0], [1572, 1579, 1.0], [1599, 1605, 1.0]]}, "snippet": {"field": "default_text", "start": 1417, "stop": 1617, "weights": [[5, 12, 1.0], [155, 162, 1.0], [182, 188, 1.0]]}}, {"doc_id": "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1", "score": 22.088779504311155, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[379, 385, 1.0], [695, 701, 1.0], [799, 806, 1.0], [913, 920, 1.0], [1122, 1129, 1.0], [1172, 1179, 1.0], [1297, 1304, 1.0], [1457, 1464, 1.0], [1592, 1598, 1.0], [1599, 1605, 1.0], [1726, 1733, 1.0], [1877, 1883, 1.0], [1886, 1892, 1.0], [1893, 1899, 1.0], [1915, 1921, 1.0], [1922, 1928, 1.0], [1965, 1972, 1.0], [2018, 2025, 1.0], [2070, 2076, 1.0]]}, "snippet": {"field": "default_text", "start": 1872, "stop": 2072, "weights": [[5, 11, 1.0], [14, 20, 1.0], [21, 27, 1.0], [43, 49, 1.0], [50, 56, 1.0], [93, 100, 1.0], [146, 153, 1.0], [198, 204, 1.0]]}}, {"doc_id": "2011.tweb_journal-ir0anthology0volumeA5A2.3", "score": 21.587858315109106, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[383, 389, 1.0], [513, 520, 1.0], [684, 691, 1.0], [1096, 1103, 1.0], [1261, 1268, 1.0], [1509, 1516, 1.0], [1524, 1530, 1.0]]}, "snippet": {"field": "default_text", "start": 378, "stop": 578, "weights": [[5, 11, 1.0], [135, 142, 1.0]]}}, {"doc_id": "2005.wwwconf_conference-2005.28", "score": 21.383204809942924, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[442, 448, 1.0], [758, 764, 1.0], [862, 869, 1.0], [976, 983, 1.0], [1184, 1191, 1.0], [1234, 1241, 1.0], [1359, 1366, 1.0], [1519, 1526, 1.0], [1654, 1660, 1.0], [1661, 1667, 1.0], [1788, 1795, 1.0], [1939, 1946, 1.0], [1991, 1997, 1.0]]}, "snippet": {"field": "default_text", "start": 1179, "stop": 1379, "weights": [[5, 12, 1.0], [55, 62, 1.0], [180, 187, 1.0]]}}, {"doc_id": "2017.ipm_journal-ir0anthology0volumeA53A4.5", "score": 21.33505032443405, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[492, 498, 1.0], [499, 505, 1.0], [528, 535, 1.0], [547, 554, 1.0], [1169, 1175, 1.0], [1176, 1182, 1.0], [1287, 1294, 1.0], [1530, 1537, 1.0], [1545, 1551, 1.0]]}, "snippet": {"field": "default_text", "start": 487, "stop": 687, "weights": [[5, 11, 1.0], [12, 18, 1.0], [41, 48, 1.0], [60, 67, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.149", "score": 21.14077470591127, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[589, 595, 1.0], [878, 885, 1.0], [1019, 1025, 1.0], [1026, 1032, 1.0], [1203, 1210, 1.0], [1223, 1230, 1.0], [1448, 1455, 1.0], [1800, 1807, 1.0], [1825, 1832, 1.0], [1849, 1855, 1.0]]}, "snippet": {"field": "default_text", "start": 1014, "stop": 1214, "weights": [[5, 11, 1.0], [12, 18, 1.0], [189, 196, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "5", "title": "Consumer Product reviews", "description": "Papers that describe the effects of product reviews on consumer decisions", "narrative": "Relevant papers would describe the influence that reviews have on individual decisions of the consumer on whether to buy a product or not. Not relevant papers, would contain other studies about reviews, that are not pertaining to human psychology", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.0], "P@5": [0.2], "P@10": [0.1], "nDCG@1": [0.0], "nDCG@3": [0.0], "nDCG@5": [0.13120507751234178], "nDCG@10": [0.08514311764162098]}, "run_1": [{"doc_id": "2011.wwwconf_conference-2011c.86", "score": 29.949115889812628, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[560, 567, 1.0], [662, 670, 1.0], [671, 678, 1.0], [759, 767, 1.0], [768, 775, 1.0], [808, 815, 1.0], [831, 839, 1.0], [942, 950, 1.0], [951, 958, 1.0], [1008, 1016, 1.0], [1017, 1024, 1.0], [1127, 1134, 1.0], [1190, 1197, 1.0], [1301, 1308, 1.0], [1450, 1458, 1.0], [1459, 1466, 1.0]]}, "snippet": {"field": "default_text", "start": 657, "stop": 857, "weights": [[5, 13, 1.0], [14, 21, 1.0], [102, 110, 1.0], [111, 118, 1.0], [151, 158, 1.0], [174, 182, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.41", "score": 29.903223314858963, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[583, 590, 1.0], [591, 598, 1.0], [661, 668, 1.0], [758, 765, 1.0], [822, 829, 1.0], [871, 878, 1.0], [899, 906, 1.0], [923, 930, 1.0], [1009, 1016, 1.0], [1078, 1085, 1.0], [1124, 1131, 1.0], [1157, 1164, 1.0], [1194, 1201, 1.0], [1294, 1302, 1.0], [1388, 1395, 1.0], [1423, 1430, 1.0], [1703, 1710, 1.0]]}, "snippet": {"field": "default_text", "start": 753, "stop": 953, "weights": [[5, 12, 1.0], [69, 76, 1.0], [118, 125, 1.0], [146, 153, 1.0], [170, 177, 1.0]]}}, {"doc_id": "2009.ecir_conference-2009.41", "score": 28.043735503305435, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[568, 575, 1.0], [576, 583, 1.0], [824, 831, 1.0], [961, 969, 1.0], [970, 977, 1.0], [1097, 1104, 1.0], [1186, 1193, 1.0], [1555, 1562, 1.0], [1640, 1647, 1.0], [1648, 1655, 1.0]]}, "snippet": {"field": "default_text", "start": 819, "stop": 1019, "weights": [[5, 12, 1.0], [142, 150, 1.0], [151, 158, 1.0]]}}, {"doc_id": "2005.wwwconf_conference-2005.38", "score": 26.212172120655374, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[497, 505, 1.0], [590, 597, 1.0], [687, 694, 1.0], [801, 809, 1.0], [1042, 1049, 1.0], [1096, 1103, 1.0], [1172, 1179, 1.0], [1295, 1303, 1.0], [1368, 1375, 1.0], [1390, 1397, 1.0], [1482, 1489, 1.0], [1596, 1603, 1.0], [1656, 1663, 1.0]]}, "snippet": {"field": "default_text", "start": 1290, "stop": 1490, "weights": [[5, 13, 1.0], [78, 85, 1.0], [100, 107, 1.0], [192, 199, 1.0]]}}, {"doc_id": "2016.wwwconf_conference-2016.11", "score": 25.16470293656804, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[478, 485, 1.0], [501, 509, 1.0], [510, 517, 1.0], [867, 874, 1.0], [890, 898, 1.0], [1616, 1623, 1.0], [1795, 1802, 1.0], [1870, 1877, 1.0], [2285, 2292, 1.0], [2308, 2316, 1.0], [2317, 2324, 1.0]]}, "snippet": {"field": "default_text", "start": 473, "stop": 673, "weights": [[5, 12, 1.0], [28, 36, 1.0], [37, 44, 1.0]]}}, {"doc_id": "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2", "score": 25.043848563511673, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[608, 615, 1.0], [691, 698, 1.0], [705, 712, 1.0], [873, 880, 1.0], [1040, 1047, 1.0], [1120, 1127, 1.0], [1272, 1279, 1.0], [1750, 1757, 1.0]]}, "snippet": {"field": "default_text", "start": 603, "stop": 803, "weights": [[5, 12, 1.0], [88, 95, 1.0], [102, 109, 1.0]]}}, {"doc_id": "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3", "score": 24.893937391522176, "relevance": 0, "rank": 7, "weights": {"doc_id": [], "default_text": [[482, 489, 1.0], [843, 850, 1.0], [1148, 1155, 1.0], [1503, 1510, 1.0], [1599, 1606, 1.0]]}, "snippet": {"field": "default_text", "start": 1498, "stop": 1698, "weights": [[5, 12, 1.0], [101, 108, 1.0]]}}, {"doc_id": "2009.cikm_conference-2009.114", "score": 24.14963101198166, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[554, 561, 1.0], [698, 705, 1.0], [1006, 1013, 1.0], [1015, 1022, 1.0], [1096, 1103, 1.0], [1377, 1384, 1.0], [1419, 1426, 1.0], [1628, 1635, 1.0], [1700, 1707, 1.0], [1791, 1798, 1.0], [1982, 1989, 1.0]]}, "snippet": {"field": "default_text", "start": 1001, "stop": 1201, "weights": [[5, 12, 1.0], [14, 21, 1.0], [95, 102, 1.0]]}}, {"doc_id": "2008.wwwconf_conference-2008.153", "score": 23.83185487514402, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[584, 591, 1.0], [727, 734, 1.0], [847, 854, 1.0], [906, 913, 1.0], [1046, 1053, 1.0], [1086, 1093, 1.0], [1375, 1382, 1.0], [1525, 1532, 1.0], [1640, 1647, 1.0]]}, "snippet": {"field": "default_text", "start": 722, "stop": 922, "weights": [[5, 12, 1.0], [125, 132, 1.0], [184, 191, 1.0]]}}, {"doc_id": "2015.ipm_journal-ir0anthology0volumeA51A1.3", "score": 23.81611435834398, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[526, 534, 1.0], [552, 559, 1.0], [701, 709, 1.0], [710, 717, 1.0], [1085, 1092, 1.0], [1248, 1255, 1.0]]}, "snippet": {"field": "default_text", "start": 521, "stop": 721, "weights": [[5, 13, 1.0], [31, 38, 1.0], [180, 188, 1.0], [189, 196, 1.0]]}}], "run_2": [], "summary": [["1 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}, {"fields": {"query_id": "6", "title": "Limitations machine learning", "description": "Which papers describe the limitations of machine learning?", "narrative": "Relevant papers describe the limitations of machine learning ( e.g. dependence on data quality and quantity, limited ability to handle complex tasks, vulnerability to disturbances and attacks, need for resources and energy). Papers that contains machine learning but not its limitations are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [0.6666666666666666], "P@5": [0.6], "P@10": [0.4], "nDCG@1": [1.0], "nDCG@3": [0.7039180890341347], "nDCG@5": [0.6548086577531307], "nDCG@10": [0.685094857324862]}, "run_1": [{"doc_id": "2021.wsdm_conference-2021.78", "score": 25.03503799718144, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[549, 556, 1.0], [557, 565, 1.0], [679, 686, 1.0], [687, 695, 1.0], [1424, 1432, 1.0], [1970, 1978, 1.0]]}, "snippet": {"field": "default_text", "start": 544, "stop": 744, "weights": [[5, 12, 1.0], [13, 21, 1.0], [135, 142, 1.0], [143, 151, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.156", "score": 22.783004128310548, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[588, 595, 1.0], [596, 604, 1.0], [625, 633, 1.0], [802, 810, 1.0], [903, 911, 1.0], [1313, 1320, 1.0], [1592, 1599, 1.0], [1736, 1743, 1.0], [1769, 1776, 1.0]]}, "snippet": {"field": "default_text", "start": 583, "stop": 783, "weights": [[5, 12, 1.0], [13, 21, 1.0], [42, 50, 1.0]]}}, {"doc_id": "2017.wsdm_conference-2017.57", "score": 20.900779120317644, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[514, 521, 1.0], [522, 530, 1.0], [585, 592, 1.0], [593, 601, 1.0], [744, 751, 1.0], [752, 760, 1.0], [975, 983, 1.0], [1358, 1366, 1.0], [1441, 1449, 1.0], [1649, 1657, 1.0], [1764, 1771, 1.0], [1772, 1780, 1.0], [1824, 1831, 1.0], [1992, 1999, 1.0], [2000, 2008, 1.0], [2035, 2042, 1.0], [2043, 2051, 1.0]]}, "snippet": {"field": "default_text", "start": 509, "stop": 709, "weights": [[5, 12, 1.0], [13, 21, 1.0], [76, 83, 1.0], [84, 92, 1.0]]}}, {"doc_id": "2018.wsdm_conference-2018.3", "score": 19.898038251132583, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[452, 459, 1.0], [460, 468, 1.0], [776, 784, 1.0], [1005, 1012, 1.0], [1013, 1021, 1.0], [1127, 1134, 1.0], [1135, 1143, 1.0]]}, "snippet": {"field": "default_text", "start": 1000, "stop": 1200, "weights": [[5, 12, 1.0], [13, 21, 1.0], [127, 134, 1.0], [135, 143, 1.0]]}}, {"doc_id": "2020.wsdm_workshop-2020privatenlp.2", "score": 16.90374977865575, "relevance": null, "rank": 5, "weights": {"doc_id": [], "default_text": [[1418, 1426, 1.0], [1653, 1660, 1.0], [1661, 1669, 1.0], [1954, 1961, 1.0], [1962, 1970, 1.0], [2088, 2095, 1.0], [2096, 2104, 1.0]]}, "snippet": {"field": "default_text", "start": 1949, "stop": 2149, "weights": [[5, 12, 1.0], [13, 21, 1.0], [139, 146, 1.0], [147, 155, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.140", "score": 16.609729336896113, "relevance": null, "rank": 6, "weights": {"doc_id": [], "default_text": [[437, 444, 1.0], [445, 453, 1.0], [824, 831, 1.0], [832, 840, 1.0], [1218, 1225, 1.0], [1226, 1234, 1.0], [1335, 1342, 1.0], [1343, 1351, 1.0], [1375, 1382, 1.0], [1383, 1391, 1.0]]}, "snippet": {"field": "default_text", "start": 1213, "stop": 1413, "weights": [[5, 12, 1.0], [13, 21, 1.0], [122, 129, 1.0], [130, 138, 1.0], [162, 169, 1.0], [170, 178, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020c.108", "score": 16.492219800864167, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[484, 491, 1.0], [492, 500, 1.0], [988, 999, 1.0], [1053, 1060, 1.0], [1061, 1069, 1.0], [1142, 1153, 1.0], [1400, 1408, 1.0], [1483, 1494, 1.0]]}, "snippet": {"field": "default_text", "start": 983, "stop": 1183, "weights": [[5, 16, 1.0], [70, 77, 1.0], [78, 86, 1.0], [159, 170, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.28", "score": 16.393260043269173, "relevance": null, "rank": 8, "weights": {"doc_id": [], "default_text": [[779, 786, 1.0], [787, 795, 1.0], [1117, 1125, 1.0], [1493, 1501, 1.0], [2448, 2456, 1.0]]}, "snippet": {"field": "default_text", "start": 774, "stop": 974, "weights": [[5, 12, 1.0], [13, 21, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.132", "score": 16.166315374652587, "relevance": null, "rank": 9, "weights": {"doc_id": [], "default_text": [[497, 504, 1.0], [505, 513, 1.0], [1112, 1123, 1.0]]}, "snippet": {"field": "default_text", "start": 492, "stop": 692, "weights": [[5, 12, 1.0], [13, 21, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.108", "score": 15.837150521103608, "relevance": null, "rank": 10, "weights": {"doc_id": [], "default_text": [[502, 510, 1.0], [548, 555, 1.0], [556, 564, 1.0], [745, 752, 1.0], [753, 761, 1.0], [797, 804, 1.0], [805, 813, 1.0], [1022, 1030, 1.0], [1175, 1183, 1.0], [1575, 1582, 1.0], [1583, 1591, 1.0], [1615, 1623, 1.0]]}, "snippet": {"field": "default_text", "start": 543, "stop": 743, "weights": [[5, 12, 1.0], [13, 21, 1.0]]}}], "run_2": [], "summary": [["5 unjudged doc(s) move to a top spot in the ranking"]], "mergedWeights": {}}], "docs": {"2007.ntcir_workshop-2007.5": {"doc_id": "2007.ntcir_workshop-2007.5", "default_text": "DBLP:conf/ntcir/2007 Proceedings of the 6th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-6, National Center of Sciences, Tokyo, Japan, May 15-18, 2007 National Institute of Informatics (NII) 2007 http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/66.pdf https://dblp.org/rec/conf/ntcir/Evans07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ntcir/Evans07 inproceedings http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/66.pdf ['David Kirk Evans'] [] NTCIR 2007.ntcir_workshop-2007.5 1584009138.0 In this paper I present a system for automatic opinion analysis built in a short time-frame using freely available open-source processing tools and lexical resources available from prior research. I use a simple feature-set that is largely language independent and a freely available machine-learning framework to model the subtasks as classification problems and report on my system's performance. Additionally, I show that blind relevance feedback improves results sentencelevel relevance judgment. My system shows that it is possible to quickly build an opinion analysis system in a short period of time that can perform at an average level. A low-resources approach to Opinion Analysis: Machine Learning and Simple Approaches", "text": "DBLP:conf/ntcir/2007 Proceedings of the 6th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access, NTCIR-6, National Center of Sciences, Tokyo, Japan, May 15-18, 2007 National Institute of Informatics (NII) 2007 http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/66.pdf https://dblp.org/rec/conf/ntcir/Evans07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ntcir/Evans07 inproceedings http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/66.pdf ['David Kirk Evans'] [] NTCIR 2007.ntcir_workshop-2007.5 1584009138.0 In this paper I present a system for automatic opinion analysis built in a short time-frame using freely available open-source processing tools and lexical resources available from prior research. I use a simple feature-set that is largely language independent and a freely available machine-learning framework to model the subtasks as classification problems and report on my system's performance. Additionally, I show that blind relevance feedback improves results sentencelevel relevance judgment. My system shows that it is possible to quickly build an opinion analysis system in a short period of time that can perform at an average level. A low-resources approach to Opinion Analysis: Machine Learning and Simple Approaches"}, "2020.wsdm_workshop-2020privatenlp.2": {"doc_id": "2020.wsdm_workshop-2020privatenlp.2", "default_text": "DBLP:conf/wsdm/2020privatenlp Proceedings of the PrivateNLP 2020: Workshop on Privacy in Natural Language Processing - Colocated with WSDM 2020, Houston, USA, Feb 7, 2020 CEUR Workshop Proceedings 2573 4\u20137 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2573/PrivateNLP_InvitedTalk1.pdf https://dblp.org/rec/conf/wsdm/ThaineP20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ThaineP20 inproceedings http://ceur-ws.org/Vol-2573/PrivateNLP_InvitedTalk1.pdf ['Patricia Thaine', 'Gerald Penn'] [] WSDM 2020.wsdm_workshop-2020privatenlp.2 1584378247.0 Many AI applications need to process huge amounts of sensitive information for model training, evaluation, and real-world integration. These tasks include facial recognition, speaker recognition, text processing, and genomic data analysis. Unfortunately, one of the following two scenarios occur when training models to perform the aforementioned tasks: either models end up being trained on sensitive user information, making them vulnerable to malicious actors, or their evaluations are not representative of their abilities since the scope of the test set is limited. In some cases, the models never get created in the first place. There are a number of approaches that can be integrated into AI algorithms in order to maintain various levels of privacy. Namely, differential privacy, secure multi-party computation, homomorphic encryption, federated learning, secure enclaves, and automatic data de-identification. We will briefly explain each of these methods and describe the scenarios in which they would be most appropriate. Recently, several of these methods have been applied to machine learning models. We will cover some of the most interesting examples of privacy-preserving ML, including the integration of differential privacy with neural networks to avoid unwanted inferences from being made of a network's training data. Finally, we will discuss how the privacy-preserving machine learning approaches that have been proposed so far would need to be combined in order to achieve perfectly privacy-preserving machine learning. Perfectly Privacy-Preserving AI What Is It and How Do We Achieve It?", "text": "DBLP:conf/wsdm/2020privatenlp Proceedings of the PrivateNLP 2020: Workshop on Privacy in Natural Language Processing - Colocated with WSDM 2020, Houston, USA, Feb 7, 2020 CEUR Workshop Proceedings 2573 4\u20137 CEUR-WS.org 2020 http://ceur-ws.org/Vol-2573/PrivateNLP_InvitedTalk1.pdf https://dblp.org/rec/conf/wsdm/ThaineP20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ThaineP20 inproceedings http://ceur-ws.org/Vol-2573/PrivateNLP_InvitedTalk1.pdf ['Patricia Thaine', 'Gerald Penn'] [] WSDM 2020.wsdm_workshop-2020privatenlp.2 1584378247.0 Many AI applications need to process huge amounts of sensitive information for model training, evaluation, and real-world integration. These tasks include facial recognition, speaker recognition, text processing, and genomic data analysis. Unfortunately, one of the following two scenarios occur when training models to perform the aforementioned tasks: either models end up being trained on sensitive user information, making them vulnerable to malicious actors, or their evaluations are not representative of their abilities since the scope of the test set is limited. In some cases, the models never get created in the first place. There are a number of approaches that can be integrated into AI algorithms in order to maintain various levels of privacy. Namely, differential privacy, secure multi-party computation, homomorphic encryption, federated learning, secure enclaves, and automatic data de-identification. We will briefly explain each of these methods and describe the scenarios in which they would be most appropriate. Recently, several of these methods have been applied to machine learning models. We will cover some of the most interesting examples of privacy-preserving ML, including the integration of differential privacy with neural networks to avoid unwanted inferences from being made of a network's training data. Finally, we will discuss how the privacy-preserving machine learning approaches that have been proposed so far would need to be combined in order to achieve perfectly privacy-preserving machine learning. Perfectly Privacy-Preserving AI What Is It and How Do We Achieve It?"}, "2011.sigirconf_conference-2011.35": {"doc_id": "2011.sigirconf_conference-2011.35", "default_text": "DBLP:conf/sigir/2011 Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 325\u2013334 ACM 2011 https://doi.org/10.1145/2009916.2009962 10.1145/2009916.2009962 https://dblp.org/rec/conf/sigir/YeYLL11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeYLL11 inproceedings ['Mao Ye', 'Peifeng Yin', 'Wang-Chien Lee', 'Dik Lun Lee'] [] SIGIR 2011.sigirconf_conference-2011.35 1569168938.0 ABSTRACTIn this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches. Exploiting geographical influence for collaborative point-of-interest recommendation", "text": "DBLP:conf/sigir/2011 Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 325\u2013334 ACM 2011 https://doi.org/10.1145/2009916.2009962 10.1145/2009916.2009962 https://dblp.org/rec/conf/sigir/YeYLL11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeYLL11 inproceedings ['Mao Ye', 'Peifeng Yin', 'Wang-Chien Lee', 'Dik Lun Lee'] [] SIGIR 2011.sigirconf_conference-2011.35 1569168938.0 ABSTRACTIn this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches. Exploiting geographical influence for collaborative point-of-interest recommendation"}, "2015.sigirconf_conference-2015.41": {"doc_id": "2015.sigirconf_conference-2015.41", "default_text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 393\u2013402 ACM 2015 https://doi.org/10.1145/2766462.2767748 10.1145/2766462.2767748 https://dblp.org/rec/conf/sigir/ParkKZG15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ParkKZG15 inproceedings ['Dae Hoon Park', 'Hyun Duk Kim', 'ChengXiang Zhai', 'Lifan Guo'] [] SIGIR 2015.sigirconf_conference-2015.41 1569168938.0 ABSTRACTWith the rapid development of Internet and E-commerce, abundant product reviews have been written by consumers who bought the products. These reviews are very useful for consumers to optimize their purchasing decisions. However, since the reviews are all written by consumers who have bought and used a product, there are generally very few or even no reviews available for a new product or an unpopular product. We study the novel problem of retrieving relevant opinion sentences from the reviews of other products using specifications of a new or unpopular product as query. Our key idea is to leverage product specifications to assess product similarity between the query product and other products and extract relevant opinion sentences from the similar products where a consumer may find useful discussions. Then, we provide ranked opinion sentences for the query product that has no user-generated reviews. We first propose a popular summarization method and its modified version to solve the problem. Then, we propose our novel probabilistic methods. Experiment results show that the proposed methods can effectively retrieve useful opinion sentences for products that have no reviews. Retrieval of Relevant Opinion Sentences for New Products", "text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 393\u2013402 ACM 2015 https://doi.org/10.1145/2766462.2767748 10.1145/2766462.2767748 https://dblp.org/rec/conf/sigir/ParkKZG15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ParkKZG15 inproceedings ['Dae Hoon Park', 'Hyun Duk Kim', 'ChengXiang Zhai', 'Lifan Guo'] [] SIGIR 2015.sigirconf_conference-2015.41 1569168938.0 ABSTRACTWith the rapid development of Internet and E-commerce, abundant product reviews have been written by consumers who bought the products. These reviews are very useful for consumers to optimize their purchasing decisions. However, since the reviews are all written by consumers who have bought and used a product, there are generally very few or even no reviews available for a new product or an unpopular product. We study the novel problem of retrieving relevant opinion sentences from the reviews of other products using specifications of a new or unpopular product as query. Our key idea is to leverage product specifications to assess product similarity between the query product and other products and extract relevant opinion sentences from the similar products where a consumer may find useful discussions. Then, we provide ranked opinion sentences for the query product that has no user-generated reviews. We first propose a popular summarization method and its modified version to solve the problem. Then, we propose our novel probabilistic methods. Experiment results show that the proposed methods can effectively retrieve useful opinion sentences for products that have no reviews. Retrieval of Relevant Opinion Sentences for New Products"}, "2015.sigirconf_conference-2015.45": {"doc_id": "2015.sigirconf_conference-2015.45", "default_text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 433\u2013442 ACM 2015 https://doi.org/10.1145/2766462.2767722 10.1145/2766462.2767722 https://dblp.org/rec/conf/sigir/LiCLPK15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/LiCLPK15 inproceedings ['Xutao Li', 'Gao Cong', 'Xiaoli Li', 'Tuan-Anh Nguyen Pham', 'Shonali Krishnaswamy'] [] SIGIR 2015.sigirconf_conference-2015.45 1609263446.0 ABSTRACTWith the rapid growth of location-based social networks, Point of Interest (POI) recommendation has become an important research problem. However, the scarcity of the check-in data, a type of implicit feedback data, poses a severe challenge for existing POI recommendation methods. Moreover, different types of context information about POIs are available and how to leverage them becomes another challenge. In this paper, we propose a ranking based geographical factorization method, called Rank-GeoFM, for POI recommendation, which addresses the two challenges. In the proposed model, we consider that the check-in frequency characterizes users' visiting preference and learn the factorization by ranking the POIs correctly. In our model, POIs both with and without check-ins will contribute to learning the ranking and thus the data sparsity problem can be alleviated. In addition, our model can easily incorporate different types of context information, such as the geographical influence and temporal influence. We propose a stochastic gradient descent based algorithm to learn the factorization. Experiments on publicly available datasets under both user-POI setting and user-time-POI setting have been conducted to test the effectiveness of the proposed method. Experimental results under both settings show that the proposed method outperforms the state-of-the-art methods significantly in terms of recommendation accuracy. Rank-GeoFM: A Ranking based Geographical Factorization Method for Point of Interest Recommendation", "text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 433\u2013442 ACM 2015 https://doi.org/10.1145/2766462.2767722 10.1145/2766462.2767722 https://dblp.org/rec/conf/sigir/LiCLPK15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/LiCLPK15 inproceedings ['Xutao Li', 'Gao Cong', 'Xiaoli Li', 'Tuan-Anh Nguyen Pham', 'Shonali Krishnaswamy'] [] SIGIR 2015.sigirconf_conference-2015.45 1609263446.0 ABSTRACTWith the rapid growth of location-based social networks, Point of Interest (POI) recommendation has become an important research problem. However, the scarcity of the check-in data, a type of implicit feedback data, poses a severe challenge for existing POI recommendation methods. Moreover, different types of context information about POIs are available and how to leverage them becomes another challenge. In this paper, we propose a ranking based geographical factorization method, called Rank-GeoFM, for POI recommendation, which addresses the two challenges. In the proposed model, we consider that the check-in frequency characterizes users' visiting preference and learn the factorization by ranking the POIs correctly. In our model, POIs both with and without check-ins will contribute to learning the ranking and thus the data sparsity problem can be alleviated. In addition, our model can easily incorporate different types of context information, such as the geographical influence and temporal influence. We propose a stochastic gradient descent based algorithm to learn the factorization. Experiments on publicly available datasets under both user-POI setting and user-time-POI setting have been conducted to test the effectiveness of the proposed method. Experimental results under both settings show that the proposed method outperforms the state-of-the-art methods significantly in terms of recommendation accuracy. Rank-GeoFM: A Ranking based Geographical Factorization Method for Point of Interest Recommendation"}, "2015.sigirconf_conference-2015.149": {"doc_id": "2015.sigirconf_conference-2015.149", "default_text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 1015\u20131018 ACM 2015 https://doi.org/10.1145/2766462.2767764 10.1145/2766462.2767764 https://dblp.org/rec/conf/sigir/ZhangSTSWL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhangSTSWL15 inproceedings ['Rui Zhang', 'Pengyu Sun', 'Jiancong Tong', 'Rebecca Jane Stones', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2015.sigirconf_conference-2015.149 1614258237.0 ABSTRACTIn response to a user query, search engines return the topk relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding document. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to reduce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memory the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve comparable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching. Compact Snippet Caching for Flash-based Search Engines", "text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 1015\u20131018 ACM 2015 https://doi.org/10.1145/2766462.2767764 10.1145/2766462.2767764 https://dblp.org/rec/conf/sigir/ZhangSTSWL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhangSTSWL15 inproceedings ['Rui Zhang', 'Pengyu Sun', 'Jiancong Tong', 'Rebecca Jane Stones', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2015.sigirconf_conference-2015.149 1614258237.0 ABSTRACTIn response to a user query, search engines return the topk relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding document. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to reduce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memory the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve comparable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching. Compact Snippet Caching for Flash-based Search Engines"}, "2012.sigirconf_conference-2012.5": {"doc_id": "2012.sigirconf_conference-2012.5", "default_text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 25\u201334 ACM 2012 https://doi.org/10.1145/2348283.2348290 10.1145/2348283.2348290 https://dblp.org/rec/conf/sigir/OzertemCDV12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/OzertemCDV12 inproceedings ['Umut Ozertem', 'Olivier Chapelle', 'Pinar Donmez', 'Emre Velipasaoglu'] [] SIGIR 2012.sigirconf_conference-2012.5 1543248348.0 ABSTRACTWe consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines. Learning to suggest: a machine learning framework for ranking query suggestions", "text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 25\u201334 ACM 2012 https://doi.org/10.1145/2348283.2348290 10.1145/2348283.2348290 https://dblp.org/rec/conf/sigir/OzertemCDV12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/OzertemCDV12 inproceedings ['Umut Ozertem', 'Olivier Chapelle', 'Pinar Donmez', 'Emre Velipasaoglu'] [] SIGIR 2012.sigirconf_conference-2012.5 1543248348.0 ABSTRACTWe consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines. Learning to suggest: a machine learning framework for ranking query suggestions"}, "2012.sigirconf_conference-2012.31": {"doc_id": "2012.sigirconf_conference-2012.31", "default_text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 285\u2013294 ACM 2012 https://doi.org/10.1145/2348283.2348324 10.1145/2348283.2348324 https://dblp.org/rec/conf/sigir/PantelGAH12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/PantelGAH12 inproceedings ['Patrick Pantel', 'Michael Gamon', 'Omar Alonso', 'Kevin Haas'] [] SIGIR 2012.sigirconf_conference-2012.31 1542189490.0 ABSTRACTSocial features are increasingly integrated within the search results page of the main commercial search engines. There is, however, little understanding of the utility of social features in traditional search. In this paper, we study utility in the context of social annotations, which are markings indicating that a person in the social network of the user has liked or shared a result document. We introduce a taxonomy of social relevance aspects that influence the utility of social annotations in search, spanning query classes, the social network, and content relevance. We present the results of a user study quantifying the utility of social annotations and the interplay between social relevance aspects. Through the user study we gain insights on conditions under which social annotations are most useful to a user. Finally, we present machine learned models for predicting the utility of a social annotation using the user study judgments as an optimization criterion. We model the learning task with features drawn from web usage logs, and show empirical evidence over real-world head and tail queries that the problem is learnable and that in many cases we can predict the utility of a social annotation. Social annotations: utility and prediction modeling", "text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 285\u2013294 ACM 2012 https://doi.org/10.1145/2348283.2348324 10.1145/2348283.2348324 https://dblp.org/rec/conf/sigir/PantelGAH12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/PantelGAH12 inproceedings ['Patrick Pantel', 'Michael Gamon', 'Omar Alonso', 'Kevin Haas'] [] SIGIR 2012.sigirconf_conference-2012.31 1542189490.0 ABSTRACTSocial features are increasingly integrated within the search results page of the main commercial search engines. There is, however, little understanding of the utility of social features in traditional search. In this paper, we study utility in the context of social annotations, which are markings indicating that a person in the social network of the user has liked or shared a result document. We introduce a taxonomy of social relevance aspects that influence the utility of social annotations in search, spanning query classes, the social network, and content relevance. We present the results of a user study quantifying the utility of social annotations and the interplay between social relevance aspects. Through the user study we gain insights on conditions under which social annotations are most useful to a user. Finally, we present machine learned models for predicting the utility of a social annotation using the user study judgments as an optimization criterion. We model the learning task with features drawn from web usage logs, and show empirical evidence over real-world head and tail queries that the problem is learnable and that in many cases we can predict the utility of a social annotation. Social annotations: utility and prediction modeling"}, "2014.sigirconf_conference-2014.179": {"doc_id": "2014.sigirconf_conference-2014.179", "default_text": "DBLP:conf/sigir/2014 The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014 1199\u20131202 ACM 2014 https://doi.org/10.1145/2600428.2609544 10.1145/2600428.2609544 https://dblp.org/rec/conf/sigir/SamarHBKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SamarHBKV14 inproceedings ['Thaer Samar', 'Hugo C. Huurdeman', 'Anat Ben-David', 'Jaap Kamps', 'Arjen P. de Vries'] [] SIGIR 2014.sigirconf_conference-2014.179 1541498845.0 ABSTRACTMany national and international heritage institutes realize the importance of archiving the web for future culture heritage. Web archiving is currently performed either by harvesting a national domain, or by crawling a pre-defined list of websites selected by the archiving institution. In either method, crawling results in more information being harvested than just the websites intended for preservation; which could be used to reconstruct impressions of pages that existed on the live web of the crawl date, but would have been lost forever. We present a method to create representations of what we will refer to as a web collection's aura: the web documents that were not included in the archived collection, but are known to have existed -due to their mentions on pages that were included in the archived web collection. To create representations of these unarchived pages, we exploit the information about the unarchived URLs that can be derived from the crawls by combining crawl date distribution, anchor text and link structure. We illustrate empirically that the size of the aura can be substantial: in 2012, the Dutch Web archive contained 12.3M unique pages, while we uncover references to 11.9M additional (unarchived) pages. Uncovering the unarchived web", "text": "DBLP:conf/sigir/2014 The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014 1199\u20131202 ACM 2014 https://doi.org/10.1145/2600428.2609544 10.1145/2600428.2609544 https://dblp.org/rec/conf/sigir/SamarHBKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SamarHBKV14 inproceedings ['Thaer Samar', 'Hugo C. Huurdeman', 'Anat Ben-David', 'Jaap Kamps', 'Arjen P. de Vries'] [] SIGIR 2014.sigirconf_conference-2014.179 1541498845.0 ABSTRACTMany national and international heritage institutes realize the importance of archiving the web for future culture heritage. Web archiving is currently performed either by harvesting a national domain, or by crawling a pre-defined list of websites selected by the archiving institution. In either method, crawling results in more information being harvested than just the websites intended for preservation; which could be used to reconstruct impressions of pages that existed on the live web of the crawl date, but would have been lost forever. We present a method to create representations of what we will refer to as a web collection's aura: the web documents that were not included in the archived collection, but are known to have existed -due to their mentions on pages that were included in the archived web collection. To create representations of these unarchived pages, we exploit the information about the unarchived URLs that can be derived from the crawls by combining crawl date distribution, anchor text and link structure. We illustrate empirically that the size of the aura can be substantial: in 2012, the Dutch Web archive contained 12.3M unique pages, while we uncover references to 11.9M additional (unarchived) pages. Uncovering the unarchived web"}, "2018.sigirconf_conference-2018.21": {"doc_id": "2018.sigirconf_conference-2018.21", "default_text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 185\u2013194 ACM 2018 https://doi.org/10.1145/3209978.3210023 10.1145/3209978.3210023 https://dblp.org/rec/conf/sigir/SunW018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SunW018 inproceedings ['Peijie Sun', 'Le Wu', 'Meng Wang'] [] SIGIR 2018.sigirconf_conference-2018.21 1600256062.0 ABSTRACTCollaborative filtering (CF) is one of the most popular techniques for building recommender systems. To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance. In these systems, uses' preferences over time are determined by their temporal dynamic interests as well as the general static interests. In the meantime, the complex interplay between users' internal interests and the social influence from the social network drives the evolution of users' preferences over time. Nevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation. Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open. To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation. In the proposed approach, we model users' complex dynamic and general static preferences over time by fusing social influence among users with two attention networks. Specifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture users' complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time. In the general static preference modeling process, we characterize each user's static interest by introducing a static social attention network to model the stationary social influence among users. The output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task. Finally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baselines. Attentive Recurrent Social Recommendation", "text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 185\u2013194 ACM 2018 https://doi.org/10.1145/3209978.3210023 10.1145/3209978.3210023 https://dblp.org/rec/conf/sigir/SunW018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SunW018 inproceedings ['Peijie Sun', 'Le Wu', 'Meng Wang'] [] SIGIR 2018.sigirconf_conference-2018.21 1600256062.0 ABSTRACTCollaborative filtering (CF) is one of the most popular techniques for building recommender systems. To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance. In these systems, uses' preferences over time are determined by their temporal dynamic interests as well as the general static interests. In the meantime, the complex interplay between users' internal interests and the social influence from the social network drives the evolution of users' preferences over time. Nevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation. Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open. To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation. In the proposed approach, we model users' complex dynamic and general static preferences over time by fusing social influence among users with two attention networks. Specifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture users' complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time. In the general static preference modeling process, we characterize each user's static interest by introducing a static social attention network to model the stationary social influence among users. The output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task. Finally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baselines. Attentive Recurrent Social Recommendation"}, "2006.sigirconf_conference-2006.11": {"doc_id": "2006.sigirconf_conference-2006.11", "default_text": "DBLP:conf/sigir/2006 SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006 75\u201382 ACM 2006 https://doi.org/10.1145/1148170.1148187 10.1145/1148170.1148187 https://dblp.org/rec/conf/sigir/FengLWBMZM06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/FengLWBMZM06 inproceedings ['Guang Feng', 'Tie-Yan Liu', 'Ying Wang', 'Ying Bao', 'Zhiming Ma', 'Xu-Dong Zhang', 'Wei-Ying Ma'] [] SIGIR 2006.sigirconf_conference-2006.11 1614003129.0 ABSTRACTSince the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applications, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.In this work, we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than that of websites, it is not feasible to base the calculation of the ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods. AggregateRank: bringing order to web sites", "text": "DBLP:conf/sigir/2006 SIGIR 2006: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, Washington, USA, August 6-11, 2006 75\u201382 ACM 2006 https://doi.org/10.1145/1148170.1148187 10.1145/1148170.1148187 https://dblp.org/rec/conf/sigir/FengLWBMZM06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/FengLWBMZM06 inproceedings ['Guang Feng', 'Tie-Yan Liu', 'Ying Wang', 'Ying Bao', 'Zhiming Ma', 'Xu-Dong Zhang', 'Wei-Ying Ma'] [] SIGIR 2006.sigirconf_conference-2006.11 1614003129.0 ABSTRACTSince the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applications, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.In this work, we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than that of websites, it is not feasible to base the calculation of the ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods. AggregateRank: bringing order to web sites"}, "2007.sigirconf_conference-2007.26": {"doc_id": "2007.sigirconf_conference-2007.26", "default_text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 183\u2013190 ACM 2007 https://doi.org/10.1145/1277741.1277775 10.1145/1277741.1277775 https://dblp.org/rec/conf/sigir/Baeza-YatesGJMPS07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Baeza-YatesGJMPS07 inproceedings ['Ricardo A. Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] SIGIR 2007.sigirconf_conference-2007.26 1619422021.0 ABSTRACTIn this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. The impact of caching on search engines", "text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 183\u2013190 ACM 2007 https://doi.org/10.1145/1277741.1277775 10.1145/1277741.1277775 https://dblp.org/rec/conf/sigir/Baeza-YatesGJMPS07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Baeza-YatesGJMPS07 inproceedings ['Ricardo A. Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] SIGIR 2007.sigirconf_conference-2007.26 1619422021.0 ABSTRACTIn this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. The impact of caching on search engines"}, "2007.sigirconf_conference-2007.39": {"doc_id": "2007.sigirconf_conference-2007.39", "default_text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 287\u2013294 ACM 2007 https://doi.org/10.1145/1277741.1277792 10.1145/1277741.1277792 https://dblp.org/rec/conf/sigir/ZhengCSZ07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhengCSZ07 inproceedings ['Zhaohui Zheng', 'Keke Chen', 'Gordon Sun', 'Hongyuan Zha'] [] SIGIR 2007.sigirconf_conference-2007.39 1541498845.0 ABSTRACTEffective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods. A regression framework for learning ranking functions using relative relevance judgments", "text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 287\u2013294 ACM 2007 https://doi.org/10.1145/1277741.1277792 10.1145/1277741.1277792 https://dblp.org/rec/conf/sigir/ZhengCSZ07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhengCSZ07 inproceedings ['Zhaohui Zheng', 'Keke Chen', 'Gordon Sun', 'Hongyuan Zha'] [] SIGIR 2007.sigirconf_conference-2007.39 1541498845.0 ABSTRACTEffective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods. A regression framework for learning ranking functions using relative relevance judgments"}, "2012.wsdm_conference-2012.45": {"doc_id": "2012.wsdm_conference-2012.45", "default_text": "DBLP:conf/wsdm/2012 Proceedings of the Fifth International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, WA, USA, February 8-12, 2012 433\u2013442 ACM 2012 https://doi.org/10.1145/2124295.2124348 10.1145/2124295.2124348 https://dblp.org/rec/conf/wsdm/SontagCBWDB12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/SontagCBWDB12 inproceedings ['David A. Sontag', 'Kevyn Collins-Thompson', 'Paul N. Bennett', 'Ryen W. White', 'Susan T. Dumais', 'Bodo Billerbeck'] [] WSDM 2012.wsdm_conference-2012.45 1573834565.0 ABSTRACTWe present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries. Probabilistic models for personalizing web search", "text": "DBLP:conf/wsdm/2012 Proceedings of the Fifth International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, WA, USA, February 8-12, 2012 433\u2013442 ACM 2012 https://doi.org/10.1145/2124295.2124348 10.1145/2124295.2124348 https://dblp.org/rec/conf/wsdm/SontagCBWDB12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/SontagCBWDB12 inproceedings ['David A. Sontag', 'Kevyn Collins-Thompson', 'Paul N. Bennett', 'Ryen W. White', 'Susan T. Dumais', 'Bodo Billerbeck'] [] WSDM 2012.wsdm_conference-2012.45 1573834565.0 ABSTRACTWe present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries. Probabilistic models for personalizing web search"}, "2018.wsdm_conference-2018.3": {"doc_id": "2018.wsdm_conference-2018.3", "default_text": "DBLP:conf/wsdm/2018 Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 3 ACM 2018 https://doi.org/10.1145/3159652.3176182 10.1145/3159652.3176182 https://dblp.org/rec/conf/wsdm/Pearl18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Pearl18 inproceedings ['Judea Pearl'] [] WSDM 2018.wsdm_conference-2018.3 1597335218.0 ABSTRACTCurrent machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference. Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution", "text": "DBLP:conf/wsdm/2018 Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 3 ACM 2018 https://doi.org/10.1145/3159652.3176182 10.1145/3159652.3176182 https://dblp.org/rec/conf/wsdm/Pearl18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Pearl18 inproceedings ['Judea Pearl'] [] WSDM 2018.wsdm_conference-2018.3 1597335218.0 ABSTRACTCurrent machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference. Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution"}, "2010.wsdm_conference-2010.39": {"doc_id": "2010.wsdm_conference-2010.39", "default_text": "DBLP:conf/wsdm/2010 Proceedings of the Third International Conference on Web Search and Web Data Mining, WSDM 2010, New York, NY, USA, February 4-6, 2010 381\u2013390 ACM 2010 https://doi.org/10.1145/1718487.1718535 10.1145/1718487.1718535 https://dblp.org/rec/conf/wsdm/KoppulaLACGS10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/KoppulaLACGS10 inproceedings ['Hema Swetha Koppula', 'Krishna P. Leela', 'Amit Agarwal', 'Krishna Prasad Chitrapura', 'Sachin Garg', 'Amit Sasturkar'] [] WSDM 2010.wsdm_conference-2010.39 1558431513.0 ABSTRACTPresence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserving each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We compare the precision and scalability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework. Learning URL patterns for webpage de-duplication", "text": "DBLP:conf/wsdm/2010 Proceedings of the Third International Conference on Web Search and Web Data Mining, WSDM 2010, New York, NY, USA, February 4-6, 2010 381\u2013390 ACM 2010 https://doi.org/10.1145/1718487.1718535 10.1145/1718487.1718535 https://dblp.org/rec/conf/wsdm/KoppulaLACGS10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/KoppulaLACGS10 inproceedings ['Hema Swetha Koppula', 'Krishna P. Leela', 'Amit Agarwal', 'Krishna Prasad Chitrapura', 'Sachin Garg', 'Amit Sasturkar'] [] WSDM 2010.wsdm_conference-2010.39 1558431513.0 ABSTRACTPresence of duplicate documents in the World Wide Web adversely affects crawling, indexing and relevance, which are the core building blocks of web search. In this paper, we present a set of techniques to mine rules from URLs and utilize these rules for de-duplication using just URL strings without fetching the content explicitly. Our technique is composed of mining the crawl logs and utilizing clusters of similar pages to extract transformation rules, which are used to normalize URLs belonging to each cluster. Preserving each mined rule for de-duplication is not efficient due to the large number of such rules. We present a machine learning technique to generalize the set of rules, which reduces the resource footprint to be usable at web-scale. The rule extraction techniques are robust against web-site specific URL conventions. We compare the precision and scalability of our approach with recent efforts in using URLs for de-duplication. Experimental results demonstrate that our approach achieves 2 times more reduction in duplicates with only half the rules compared to the most recent previous approach. Scalability of the framework is demonstrated by performing a large scale evaluation on a set of 3 Billion URLs, implemented using the MapReduce framework. Learning URL patterns for webpage de-duplication"}, "2021.wsdm_conference-2021.28": {"doc_id": "2021.wsdm_conference-2021.28", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 220\u2013228 ACM 2021 https://doi.org/10.1145/3437963.3441750 10.1145/3437963.3441750 https://dblp.org/rec/conf/wsdm/ShiYWLLWZ21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ShiYWLLWZ21 inproceedings ['Jiatu Shi', 'Huaxiu Yao', 'Xian Wu', 'Tong Li', 'Zedong Lin', 'Tengfei Wang', 'Binqiang Zhao'] [] WSDM 2021.wsdm_conference-2021.28 1617805064.0 E-commerce business is revolutionizing our shopping experiences by providing convenient and straightforward services. One of the most fundamental problems is how to balance the demand and supply in market segments to build an efficient platform. While conventional machine learning models have achieved great success on data-sufficient segments, it may fail in a large-portion of segments in E-commerce platforms, where there are not sufficient records to learn well-trained models. In this paper, we tackle this problem in the context of market segment demand prediction. The goal is to facilitate the learning process in the target segments by leveraging the learned knowledge from data-sufficient source segments. Specifically, we propose a novel algorithm, RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a metalearning paradigm. The multi-pattern fusion network considers both local and seasonal temporal patterns for segment demand prediction. In the meta-learning paradigm, transferable knowledge is regarded as the model parameter initialization of MPFN, which are learned from diverse source segments. Furthermore, we capture the segment relations by combining data-driven segment representation and segment knowledge graph representation and tailor the segment-specific relations to customize transferable model parameter initialization. Thus, even with limited data, the target segment can quickly find the most relevant transferred knowledge and adapt to the optimal parameters. We conduct extensive experiments on two large-scale industrial datasets. The results justify that our RMLDP outperforms a set of state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a real-world E-commerce platform. The online A/B testing results further demonstrate the practicality of RMLDP. CCS CONCEPTS \u2022 Information systems \u2192 Data mining; \u2022 Applied computing \u2192 Electronic commerce. Relation-aware Meta-learning for E-commerce Market Segment Demand Prediction with Limited Records", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 220\u2013228 ACM 2021 https://doi.org/10.1145/3437963.3441750 10.1145/3437963.3441750 https://dblp.org/rec/conf/wsdm/ShiYWLLWZ21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ShiYWLLWZ21 inproceedings ['Jiatu Shi', 'Huaxiu Yao', 'Xian Wu', 'Tong Li', 'Zedong Lin', 'Tengfei Wang', 'Binqiang Zhao'] [] WSDM 2021.wsdm_conference-2021.28 1617805064.0 E-commerce business is revolutionizing our shopping experiences by providing convenient and straightforward services. One of the most fundamental problems is how to balance the demand and supply in market segments to build an efficient platform. While conventional machine learning models have achieved great success on data-sufficient segments, it may fail in a large-portion of segments in E-commerce platforms, where there are not sufficient records to learn well-trained models. In this paper, we tackle this problem in the context of market segment demand prediction. The goal is to facilitate the learning process in the target segments by leveraging the learned knowledge from data-sufficient source segments. Specifically, we propose a novel algorithm, RMLDP, to incorporate a multi-pattern fusion network (MPFN) with a metalearning paradigm. The multi-pattern fusion network considers both local and seasonal temporal patterns for segment demand prediction. In the meta-learning paradigm, transferable knowledge is regarded as the model parameter initialization of MPFN, which are learned from diverse source segments. Furthermore, we capture the segment relations by combining data-driven segment representation and segment knowledge graph representation and tailor the segment-specific relations to customize transferable model parameter initialization. Thus, even with limited data, the target segment can quickly find the most relevant transferred knowledge and adapt to the optimal parameters. We conduct extensive experiments on two large-scale industrial datasets. The results justify that our RMLDP outperforms a set of state-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a real-world E-commerce platform. The online A/B testing results further demonstrate the practicality of RMLDP. CCS CONCEPTS \u2022 Information systems \u2192 Data mining; \u2022 Applied computing \u2192 Electronic commerce. Relation-aware Meta-learning for E-commerce Market Segment Demand Prediction with Limited Records"}, "2021.wsdm_conference-2021.78": {"doc_id": "2021.wsdm_conference-2021.78", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 680\u2013688 ACM 2021 https://doi.org/10.1145/3437963.3441752 10.1145/3437963.3441752 https://dblp.org/rec/conf/wsdm/DaiW21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/DaiW21 inproceedings ['Enyan Dai', 'Suhang Wang'] [] WSDM 2021.wsdm_conference-2021.78 1617805064.0 Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy. Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 680\u2013688 ACM 2021 https://doi.org/10.1145/3437963.3441752 10.1145/3437963.3441752 https://dblp.org/rec/conf/wsdm/DaiW21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/DaiW21 inproceedings ['Enyan Dai', 'Suhang Wang'] [] WSDM 2021.wsdm_conference-2021.78 1617805064.0 Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy. Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information"}, "2021.wsdm_conference-2021.108": {"doc_id": "2021.wsdm_conference-2021.108", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 910\u2013913 ACM 2021 https://doi.org/10.1145/3437963.3441702 10.1145/3437963.3441702 https://dblp.org/rec/conf/wsdm/SchneebeliKK21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/SchneebeliKK21 inproceedings ['Christian Schneebeli', 'Saikishore Kalloori', 'Severin Klingler'] [] WSDM 2021.wsdm_conference-2021.108 1617805064.0 Federated Learning (FL) allows to collaboratively build machine learning models between different entities without the need for sharing or gathering the data. In FL, typically there is a global server and a set of clients (stakeholders) to build shared machine learning models. In contrast to distributed machine learning, the controller of the training process (here the global server) never sees the data of the stakeholders participating in FL. Every stakeholder owns his own data and doesn't share it. During the training and learning process, only the model updates (e.g. gradients) are shared. To our best of knowledge, we did not find a publicly available practical federated learning framework for stakeholders. We have built a framework that enables FL for a small number of stakeholders. In the paper, we describe the framework architecture, communication protocol, and algorithms. Our framework is open-sourced and it is easy to set up for stakeholders and ensures that no private information is leaked during the training process CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning. A Practical Federated Learning Framework for Small Number of Stakeholders", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 910\u2013913 ACM 2021 https://doi.org/10.1145/3437963.3441702 10.1145/3437963.3441702 https://dblp.org/rec/conf/wsdm/SchneebeliKK21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/SchneebeliKK21 inproceedings ['Christian Schneebeli', 'Saikishore Kalloori', 'Severin Klingler'] [] WSDM 2021.wsdm_conference-2021.108 1617805064.0 Federated Learning (FL) allows to collaboratively build machine learning models between different entities without the need for sharing or gathering the data. In FL, typically there is a global server and a set of clients (stakeholders) to build shared machine learning models. In contrast to distributed machine learning, the controller of the training process (here the global server) never sees the data of the stakeholders participating in FL. Every stakeholder owns his own data and doesn't share it. During the training and learning process, only the model updates (e.g. gradients) are shared. To our best of knowledge, we did not find a publicly available practical federated learning framework for stakeholders. We have built a framework that enables FL for a small number of stakeholders. In the paper, we describe the framework architecture, communication protocol, and algorithms. Our framework is open-sourced and it is easy to set up for stakeholders and ensures that no private information is leaked during the training process CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning. A Practical Federated Learning Framework for Small Number of Stakeholders"}, "2021.wsdm_conference-2021.132": {"doc_id": "2021.wsdm_conference-2021.132", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1089\u20131092 ACM 2021 https://doi.org/10.1145/3437963.3441705 10.1145/3437963.3441705 https://dblp.org/rec/conf/wsdm/NemirovskyTXG21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/NemirovskyTXG21 inproceedings ['Daniel Nemirovsky', 'Nicolas Thiebaut', 'Ye Xu', 'Abhishek Gupta'] [] WSDM 2021.wsdm_conference-2021.132 1617805064.0 Machine learning predictors have been increasingly applied in production settings, including in one of the world's largest hiring platforms, Hired, to provide a better candidate and recruiter experience. The ability to provide actionable feedback is desirable for candidates to improve their chances of achieving success in the marketplace. Until recently, however, methods aimed at providing actionable feedback have been limited in terms of realism and latency. In this work, we demonstrate how, by applying a newly introduced method based on Generative Adversarial Networks (GANs), we are able to overcome these limitations and provide actionable feedback in real-time to candidates in production settings. Our experimental results highlight the significant benefits of utilizing a GAN-based approach on our dataset relative to two other state-of-the-art approaches (including over 1000x latency gains). We also illustrate the potential impact of this approach in detail on two real candidate profile examples. Providing Actionable Feedback in Hiring Marketplaces using Generative Adversarial Networks", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1089\u20131092 ACM 2021 https://doi.org/10.1145/3437963.3441705 10.1145/3437963.3441705 https://dblp.org/rec/conf/wsdm/NemirovskyTXG21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/NemirovskyTXG21 inproceedings ['Daniel Nemirovsky', 'Nicolas Thiebaut', 'Ye Xu', 'Abhishek Gupta'] [] WSDM 2021.wsdm_conference-2021.132 1617805064.0 Machine learning predictors have been increasingly applied in production settings, including in one of the world's largest hiring platforms, Hired, to provide a better candidate and recruiter experience. The ability to provide actionable feedback is desirable for candidates to improve their chances of achieving success in the marketplace. Until recently, however, methods aimed at providing actionable feedback have been limited in terms of realism and latency. In this work, we demonstrate how, by applying a newly introduced method based on Generative Adversarial Networks (GANs), we are able to overcome these limitations and provide actionable feedback in real-time to candidates in production settings. Our experimental results highlight the significant benefits of utilizing a GAN-based approach on our dataset relative to two other state-of-the-art approaches (including over 1000x latency gains). We also illustrate the potential impact of this approach in detail on two real candidate profile examples. Providing Actionable Feedback in Hiring Marketplaces using Generative Adversarial Networks"}, "2021.wsdm_conference-2021.140": {"doc_id": "2021.wsdm_conference-2021.140", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1115\u20131116 ACM 2021 https://doi.org/10.1145/3437963.3441671 10.1145/3437963.3441671 https://dblp.org/rec/conf/wsdm/Shtar21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Shtar21 inproceedings ['Guy Shtar'] [] WSDM 2021.wsdm_conference-2021.140 1617805064.0 Multimodal machine learning deals with building models that can process information from multiple modalities (i.e., ways of doing or experiencing something). Experiments involving humans are used to guarantee drug safety in the complex task of drug development. Drug-related data is readily available and comes in various modalities. The proposed study aims to develop novel methods for multimodal machine learning that can be used to process the diverse multimodal data used in drug development and other challenging tasks that could benefit from the use of multimodal data. We present a series of drug-related tasks which are used to both evaluate the models proposed in this ongoing study and discover new drug knowledge. This research will make far-reaching contributions to the field of machine learning, as well as practical contributions in the medical domain. CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning approaches. Multimodal Machine Learning for Drug Knowledge Discovery", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1115\u20131116 ACM 2021 https://doi.org/10.1145/3437963.3441671 10.1145/3437963.3441671 https://dblp.org/rec/conf/wsdm/Shtar21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Shtar21 inproceedings ['Guy Shtar'] [] WSDM 2021.wsdm_conference-2021.140 1617805064.0 Multimodal machine learning deals with building models that can process information from multiple modalities (i.e., ways of doing or experiencing something). Experiments involving humans are used to guarantee drug safety in the complex task of drug development. Drug-related data is readily available and comes in various modalities. The proposed study aims to develop novel methods for multimodal machine learning that can be used to process the diverse multimodal data used in drug development and other challenging tasks that could benefit from the use of multimodal data. We present a series of drug-related tasks which are used to both evaluate the models proposed in this ongoing study and discover new drug knowledge. This research will make far-reaching contributions to the field of machine learning, as well as practical contributions in the medical domain. CCS CONCEPTS \u2022 Computing methodologies \u2192 Machine learning approaches. Multimodal Machine Learning for Drug Knowledge Discovery"}, "2021.wsdm_conference-2021.156": {"doc_id": "2021.wsdm_conference-2021.156", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1161\u20131162 ACM 2021 https://doi.org/10.1145/3437963.3441838 10.1145/3437963.3441838 https://dblp.org/rec/conf/wsdm/ZhangZC0CGSD21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhangZC0CGSD21 inproceedings ['Yongfeng Zhang', 'Min Zhang', 'Hanxiong Chen', 'Xu Chen', 'Xianjie Chen', 'Chuang Gan', 'Tong Sun', 'Xin Luna Dong'] [] WSDM 2021.wsdm_conference-2021.156 1617805064.0 Recent years have witnessed the success of machine learning and especially deep learning in many research areas such as Vision and Language Processing, Information Retrieval and Recommender Systems, Social Networks and Conversational Agents. Though various learning approaches have demonstrated satisfying performance in perceptual tasks such as associative learning and matching by extracting useful similarity patterns from data, the area still sees a large amount of research needed to advance the ability of reasoning towards cognitive intelligence in the coming years. This includes but is not limited to neural logical reasoning, neural-symbolic reasoning, causal reasoning, knowledge reasoning and commonsense reasoning. The workshop focuses on the research of machine reasoning techniques and their application in various intelligent tasks. It will gather researchers as well as practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent progress in machine intelligence to a broader community, including but not limited to CV, IR, NLP, ML, DM, AI and beyond. The 1st International Workshop on Machine Reasoning: International Machine Reasoning Conference (MRC 2021)", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1161\u20131162 ACM 2021 https://doi.org/10.1145/3437963.3441838 10.1145/3437963.3441838 https://dblp.org/rec/conf/wsdm/ZhangZC0CGSD21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhangZC0CGSD21 inproceedings ['Yongfeng Zhang', 'Min Zhang', 'Hanxiong Chen', 'Xu Chen', 'Xianjie Chen', 'Chuang Gan', 'Tong Sun', 'Xin Luna Dong'] [] WSDM 2021.wsdm_conference-2021.156 1617805064.0 Recent years have witnessed the success of machine learning and especially deep learning in many research areas such as Vision and Language Processing, Information Retrieval and Recommender Systems, Social Networks and Conversational Agents. Though various learning approaches have demonstrated satisfying performance in perceptual tasks such as associative learning and matching by extracting useful similarity patterns from data, the area still sees a large amount of research needed to advance the ability of reasoning towards cognitive intelligence in the coming years. This includes but is not limited to neural logical reasoning, neural-symbolic reasoning, causal reasoning, knowledge reasoning and commonsense reasoning. The workshop focuses on the research of machine reasoning techniques and their application in various intelligent tasks. It will gather researchers as well as practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent progress in machine intelligence to a broader community, including but not limited to CV, IR, NLP, ML, DM, AI and beyond. The 1st International Workshop on Machine Reasoning: International Machine Reasoning Conference (MRC 2021)"}, "2017.wsdm_conference-2017.57": {"doc_id": "2017.wsdm_conference-2017.57", "default_text": "DBLP:conf/wsdm/2017 Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 535 ACM 2017 https://doi.org/10.1145/3018661.3022764 10.1145/3018661.3022764 https://dblp.org/rec/conf/wsdm/Herbrich17.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Herbrich17 inproceedings ['Ralf Herbrich'] [] WSDM 2017.wsdm_conference-2017.57 1558364082.0 ABSTRACTIn this talk I will give an introduction into the field of machine learning and discuss why it is a crucial technology for Amazon.Machine learning is the science of automatically extracting patterns from data in order to make automated predictions of future data. One way to differentiate machine learning tasks is by the following two factors: (1) How much noise is contained in the data? and (2) How far into the future is the prediction task? The former presents a limit to the learnability of task -regardless which learning algorithm is used -whereas the latter has a crucial implication on the representation of the predictions: while most tasks in search and advertising typically only forecast minutes into the future, tasks in e-commerce can require predictions up to a year into the future. The further the forecast horizon, the more important it is to take account of uncertainty in both the learning algorithm and the representation of the predictions. I will discuss which learning frameworks are best suited for the various scenarios, that is, short-term predictions with little noise vs. long-term predictions with lots of noise, and present some ideas to combine representation learning with probabilistic methods.In the second half of the talk, I will give an overview of the applications of machine learning at Amazon ranging from demand forecasting, machine translation to automation of computer vision tasks and robotics. I will also discuss the importance of tools for data scientist and share learnings on bringing machine learning algorithms into products. Machine Learning at Amazon", "text": "DBLP:conf/wsdm/2017 Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 535 ACM 2017 https://doi.org/10.1145/3018661.3022764 10.1145/3018661.3022764 https://dblp.org/rec/conf/wsdm/Herbrich17.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Herbrich17 inproceedings ['Ralf Herbrich'] [] WSDM 2017.wsdm_conference-2017.57 1558364082.0 ABSTRACTIn this talk I will give an introduction into the field of machine learning and discuss why it is a crucial technology for Amazon.Machine learning is the science of automatically extracting patterns from data in order to make automated predictions of future data. One way to differentiate machine learning tasks is by the following two factors: (1) How much noise is contained in the data? and (2) How far into the future is the prediction task? The former presents a limit to the learnability of task -regardless which learning algorithm is used -whereas the latter has a crucial implication on the representation of the predictions: while most tasks in search and advertising typically only forecast minutes into the future, tasks in e-commerce can require predictions up to a year into the future. The further the forecast horizon, the more important it is to take account of uncertainty in both the learning algorithm and the representation of the predictions. I will discuss which learning frameworks are best suited for the various scenarios, that is, short-term predictions with little noise vs. long-term predictions with lots of noise, and present some ideas to combine representation learning with probabilistic methods.In the second half of the talk, I will give an overview of the applications of machine learning at Amazon ranging from demand forecasting, machine translation to automation of computer vision tasks and robotics. I will also discuss the importance of tools for data scientist and share learnings on bringing machine learning algorithms into products. Machine Learning at Amazon"}, "2016.airs_conference-2016.27": {"doc_id": "2016.airs_conference-2016.27", "default_text": "DBLP:conf/airs/2016 Information Retrieval Technology - 12th Asia Information Retrieval Societies Conference, AIRS 2016, Beijing, China, November 30 - December 2, 2016, Proceedings Lecture Notes in Computer Science 9994 329\u2013334 Springer 2016 https://doi.org/10.1007/978-3-319-48051-0_27 10.1007/978-3-319-48051-0_27 https://dblp.org/rec/conf/airs/LinYXLX16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/airs/LinYXLX16 inproceedings ['Yuan Lin', 'Liang Yang', 'Bo Xu', 'Hongfei Lin', 'Kan Xu'] [] AIRS 2016.airs_conference-2016.27 1600256063.0 Abstract. According to a given query in training set, the documents can be grouped based on their relevance judgments. If the group with higher relevance labels is in front of the one with lower relevance judgments, the ranking performance of ranking model could be perfect. Inspired by this idea, we propose a novel machine learning framework for ranking, which depends on two new samples. The first sample is one-group constructed of one document with higher relevance judgment and a group of documents with lower relevance judgment; the second sample is group-group constructed of a group of documents with higher relevance judgment and a group of documents with lower relevance judgment. We also develop a novel preference-weighted loss function for multiple relevance judgment data sets. Finally, we optimize the group ranking approaches by optimizing initial ranking list for likelihood loss function. Experimental results show that our approaches are effective in improving ranking performance. Learning to Rank with Likelihood Loss Functions", "text": "DBLP:conf/airs/2016 Information Retrieval Technology - 12th Asia Information Retrieval Societies Conference, AIRS 2016, Beijing, China, November 30 - December 2, 2016, Proceedings Lecture Notes in Computer Science 9994 329\u2013334 Springer 2016 https://doi.org/10.1007/978-3-319-48051-0_27 10.1007/978-3-319-48051-0_27 https://dblp.org/rec/conf/airs/LinYXLX16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/airs/LinYXLX16 inproceedings ['Yuan Lin', 'Liang Yang', 'Bo Xu', 'Hongfei Lin', 'Kan Xu'] [] AIRS 2016.airs_conference-2016.27 1600256063.0 Abstract. According to a given query in training set, the documents can be grouped based on their relevance judgments. If the group with higher relevance labels is in front of the one with lower relevance judgments, the ranking performance of ranking model could be perfect. Inspired by this idea, we propose a novel machine learning framework for ranking, which depends on two new samples. The first sample is one-group constructed of one document with higher relevance judgment and a group of documents with lower relevance judgment; the second sample is group-group constructed of a group of documents with higher relevance judgment and a group of documents with lower relevance judgment. We also develop a novel preference-weighted loss function for multiple relevance judgment data sets. Finally, we optimize the group ranking approaches by optimizing initial ranking list for likelihood loss function. Experimental results show that our approaches are effective in improving ranking performance. Learning to Rank with Likelihood Loss Functions"}, "2009.ecir_conference-2009.41": {"doc_id": "2009.ecir_conference-2009.41", "default_text": "DBLP:conf/ecir/2009 Advances in Information Retrieval, 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings Lecture Notes in Computer Science 5478 461\u2013472 Springer 2009 https://doi.org/10.1007/978-3-642-00958-7_41 10.1007/978-3-642-00958-7_41 https://dblp.org/rec/conf/ecir/BaccianellaES09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BaccianellaES09 inproceedings ['Stefano Baccianella', 'Andrea Esuli', 'Fabrizio Sebastiani'] [] ECIR 2009.ecir_conference-2009.41 1557820837.0 Abstract. Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site. Multi-facet Rating of Product Reviews", "text": "DBLP:conf/ecir/2009 Advances in Information Retrieval, 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings Lecture Notes in Computer Science 5478 461\u2013472 Springer 2009 https://doi.org/10.1007/978-3-642-00958-7_41 10.1007/978-3-642-00958-7_41 https://dblp.org/rec/conf/ecir/BaccianellaES09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BaccianellaES09 inproceedings ['Stefano Baccianella', 'Andrea Esuli', 'Fabrizio Sebastiani'] [] ECIR 2009.ecir_conference-2009.41 1557820837.0 Abstract. Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site. Multi-facet Rating of Product Reviews"}, "2020.ecir_conference-20201.14": {"doc_id": "2020.ecir_conference-20201.14", "default_text": "DBLP:conf/ecir/2020-1 Advances in Information Retrieval - 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part I Lecture Notes in Computer Science 12035 205\u2013219 Springer 2020 https://doi.org/10.1007/978-3-030-45439-5_14 10.1007/978-3-030-45439-5_14 https://dblp.org/rec/conf/ecir/RahmaniABC20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/RahmaniABC20 inproceedings ['Hossein A. Rahmani', 'Mohammad Aliannejadi', 'Mitra Baratchi', 'Fabio Crestani'] [] ECIR 2020.ecir_conference-20201.14 1589444236.0 With the popularity of Location-based Social Networks, Point-of-Interest (POI) recommendation has become an important task, which learns the users' preferences and mobility patterns to recommend POIs. Previous studies show that incorporating contextual information such as geographical and temporal influences is necessary to improve POI recommendation by addressing the data sparsity problem. However, existing methods model the geographical influence based on the physical distance between POIs and users, while ignoring the temporal characteristics of such geographical influences. In this paper, we perform a study on the user mobility patterns where we find out that users' check-ins happen around several centers depending on their current temporal state. Next, we propose a spatio-temporal activity-centers algorithm to model users' behavior more accurately. Finally, we demonstrate the effectiveness of our proposed contextual model by incorporating it into the matrix factorization model under two different settings: (i) static and (ii) temporal. To show the effectiveness of our proposed method, which we refer to as STACP, we conduct experiments on two well-known real-world datasets acquired from Gowalla and Foursquare LBSNs. Experimental results show that the STACP model achieves a statistically significant performance improvement, compared to the state-of-the-art techniques. Also, we demonstrate the effectiveness of capturing geographical and temporal information for modeling users' activity centers and the importance of modeling them jointly. Joint Geographical and Temporal Modeling Based on Matrix Factorization for Point-of-Interest Recommendation", "text": "DBLP:conf/ecir/2020-1 Advances in Information Retrieval - 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14-17, 2020, Proceedings, Part I Lecture Notes in Computer Science 12035 205\u2013219 Springer 2020 https://doi.org/10.1007/978-3-030-45439-5_14 10.1007/978-3-030-45439-5_14 https://dblp.org/rec/conf/ecir/RahmaniABC20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/RahmaniABC20 inproceedings ['Hossein A. Rahmani', 'Mohammad Aliannejadi', 'Mitra Baratchi', 'Fabio Crestani'] [] ECIR 2020.ecir_conference-20201.14 1589444236.0 With the popularity of Location-based Social Networks, Point-of-Interest (POI) recommendation has become an important task, which learns the users' preferences and mobility patterns to recommend POIs. Previous studies show that incorporating contextual information such as geographical and temporal influences is necessary to improve POI recommendation by addressing the data sparsity problem. However, existing methods model the geographical influence based on the physical distance between POIs and users, while ignoring the temporal characteristics of such geographical influences. In this paper, we perform a study on the user mobility patterns where we find out that users' check-ins happen around several centers depending on their current temporal state. Next, we propose a spatio-temporal activity-centers algorithm to model users' behavior more accurately. Finally, we demonstrate the effectiveness of our proposed contextual model by incorporating it into the matrix factorization model under two different settings: (i) static and (ii) temporal. To show the effectiveness of our proposed method, which we refer to as STACP, we conduct experiments on two well-known real-world datasets acquired from Gowalla and Foursquare LBSNs. Experimental results show that the STACP model achieves a statistically significant performance improvement, compared to the state-of-the-art techniques. Also, we demonstrate the effectiveness of capturing geographical and temporal information for modeling users' activity centers and the importance of modeling them jointly. Joint Geographical and Temporal Modeling Based on Matrix Factorization for Point-of-Interest Recommendation"}, "2014.cikm_conference-2014.67": {"doc_id": "2014.cikm_conference-2014.67", "default_text": "DBLP:conf/cikm/2014 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014 659\u2013668 ACM 2014 https://doi.org/10.1145/2661829.2661983 10.1145/2661829.2661983 https://dblp.org/rec/conf/cikm/YuanCS14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/YuanCS14 inproceedings ['Quan Yuan', 'Gao Cong', 'Aixin Sun'] [] CIKM 2014.cikm_conference-2014.67 1585296194.0 ABSTRACTThe availability of user check-in data in large volume from the rapid growing location-based social networks (LBSNs) enables a number of important location-aware services. Point-of-interest (POI) recommendation is one of such services, which is to recommend POIs that users have not visited before. It has been observed that: (i) users tend to visit nearby places, and (ii) users tend to visit different places in different time slots, and in the same time slot, users tend to periodically visit the same places. For example, users usually visit a restaurant during lunch hours, and visit a pub at night. In this paper, we focus on the problem of time-aware POI recommendation, which aims at recommending a list of POIs for a user to visit at a given time. To exploit both geographical and temporal influences in time-aware POI recommendation, we propose the Geographical-Temporal influences Aware Graph (GTAG) to model check-in records, geographical influence and temporal influence. For effective and efficient recommendation based on GTAG, we develop a preference propagation algorithm named Breadth-first Preference Propagation (BPP). The algorithm follows a relaxed breathfirst search strategy, and returns recommendation results within at most 6 propagation steps. Our experimental results on two realworld datasets show that the proposed graph-based approach outperforms state-of-the-art POI recommendation methods substantially. Graph-based Point-of-interest Recommendation with Geographical and Temporal Influences", "text": "DBLP:conf/cikm/2014 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014 659\u2013668 ACM 2014 https://doi.org/10.1145/2661829.2661983 10.1145/2661829.2661983 https://dblp.org/rec/conf/cikm/YuanCS14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/YuanCS14 inproceedings ['Quan Yuan', 'Gao Cong', 'Aixin Sun'] [] CIKM 2014.cikm_conference-2014.67 1585296194.0 ABSTRACTThe availability of user check-in data in large volume from the rapid growing location-based social networks (LBSNs) enables a number of important location-aware services. Point-of-interest (POI) recommendation is one of such services, which is to recommend POIs that users have not visited before. It has been observed that: (i) users tend to visit nearby places, and (ii) users tend to visit different places in different time slots, and in the same time slot, users tend to periodically visit the same places. For example, users usually visit a restaurant during lunch hours, and visit a pub at night. In this paper, we focus on the problem of time-aware POI recommendation, which aims at recommending a list of POIs for a user to visit at a given time. To exploit both geographical and temporal influences in time-aware POI recommendation, we propose the Geographical-Temporal influences Aware Graph (GTAG) to model check-in records, geographical influence and temporal influence. For effective and efficient recommendation based on GTAG, we develop a preference propagation algorithm named Breadth-first Preference Propagation (BPP). The algorithm follows a relaxed breathfirst search strategy, and returns recommendation results within at most 6 propagation steps. Our experimental results on two realworld datasets show that the proposed graph-based approach outperforms state-of-the-art POI recommendation methods substantially. Graph-based Point-of-interest Recommendation with Geographical and Temporal Influences"}, "2009.cikm_conference-2009.114": {"doc_id": "2009.cikm_conference-2009.114", "default_text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1087\u20131096 ACM 2009 https://doi.org/10.1145/1645953.1646091 10.1145/1645953.1646091 https://dblp.org/rec/conf/cikm/GuoZGZS09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/GuoZGZS09 inproceedings ['Honglei Guo', 'Huijia Zhu', 'Zhili Guo', 'Xiaoxun Zhang', 'Zhong Su'] [] CIKM 2009.cikm_conference-2009.114 1617981078.0 ABSTRACTIn recent years, the number of freely available online reviews is increasing at a high speed. Aspect-based opinion mining technique has been employed to find out reviewers' opinions toward different product aspects. Such finer-grained opinion mining is valuable for the potential customers to make their purchase decisions. Productfeature extraction and categorization is very important for better mining aspect-oriented opinions. Since people usually use different words to describe the same aspect in the reviews, product-feature extraction and categorization becomes more challenging. Manually product-feature extraction and categorization is tedious and time consuming, and practically infeasible for the massive amount of products. In this paper, we propose an unsupervised productfeature categorization method with multilevel latent semantic association. After extracting product-features from the semi-structured reviews, we construct the first latent semantic association (LaSA) model to group words into a set of concepts according to their virtual context documents. It generates the latent semantic structure for each product-feature. The second LaSA model is constructed to categorize the product-features according to their latent semantic structures and context snippets in the reviews. Experimental results demonstrate that our method achieves better performance compared with the existing approaches. Moreover, the proposed method is language-and domain-independent. Product feature categorization with multilevel latent semantic association", "text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1087\u20131096 ACM 2009 https://doi.org/10.1145/1645953.1646091 10.1145/1645953.1646091 https://dblp.org/rec/conf/cikm/GuoZGZS09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/GuoZGZS09 inproceedings ['Honglei Guo', 'Huijia Zhu', 'Zhili Guo', 'Xiaoxun Zhang', 'Zhong Su'] [] CIKM 2009.cikm_conference-2009.114 1617981078.0 ABSTRACTIn recent years, the number of freely available online reviews is increasing at a high speed. Aspect-based opinion mining technique has been employed to find out reviewers' opinions toward different product aspects. Such finer-grained opinion mining is valuable for the potential customers to make their purchase decisions. Productfeature extraction and categorization is very important for better mining aspect-oriented opinions. Since people usually use different words to describe the same aspect in the reviews, product-feature extraction and categorization becomes more challenging. Manually product-feature extraction and categorization is tedious and time consuming, and practically infeasible for the massive amount of products. In this paper, we propose an unsupervised productfeature categorization method with multilevel latent semantic association. After extracting product-features from the semi-structured reviews, we construct the first latent semantic association (LaSA) model to group words into a set of concepts according to their virtual context documents. It generates the latent semantic structure for each product-feature. The second LaSA model is constructed to categorize the product-features according to their latent semantic structures and context snippets in the reviews. Experimental results demonstrate that our method achieves better performance compared with the existing approaches. Moreover, the proposed method is language-and domain-independent. Product feature categorization with multilevel latent semantic association"}, "2009.cikm_conference-2009.190": {"doc_id": "2009.cikm_conference-2009.190", "default_text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1581\u20131584 ACM 2009 https://doi.org/10.1145/1645953.1646177 10.1145/1645953.1646177 https://dblp.org/rec/conf/cikm/KanungoGKW09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KanungoGKW09 inproceedings ['Tapas Kanungo', 'Nadia Ghamrawi', 'Ki Yuen Kim', 'Lawrence Wai'] [] CIKM 2009.cikm_conference-2009.190 1617981078.0 ABSTRACTEye tracking experiments have shown that titles of Web search results play a crucial role in guiding a user's search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pairwise modeling approach gives better results in terms of three offline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process -search success -that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success. Web search result summarization: title selection algorithms and user satisfaction", "text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1581\u20131584 ACM 2009 https://doi.org/10.1145/1645953.1646177 10.1145/1645953.1646177 https://dblp.org/rec/conf/cikm/KanungoGKW09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KanungoGKW09 inproceedings ['Tapas Kanungo', 'Nadia Ghamrawi', 'Ki Yuen Kim', 'Lawrence Wai'] [] CIKM 2009.cikm_conference-2009.190 1617981078.0 ABSTRACTEye tracking experiments have shown that titles of Web search results play a crucial role in guiding a user's search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pairwise modeling approach gives better results in terms of three offline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process -search success -that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success. Web search result summarization: title selection algorithms and user satisfaction"}, "2009.cikm_conference-2009.296": {"doc_id": "2009.cikm_conference-2009.296", "default_text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 2007\u20132010 ACM 2009 https://doi.org/10.1145/1645953.1646288 10.1145/1645953.1646288 https://dblp.org/rec/conf/cikm/LiLJZCD09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/LiLJZCD09 inproceedings ['Xin Li', 'Fan Li', 'Shihao Ji', 'Zhaohui Zheng', 'Yi Chang', 'Anlei Dong'] [] CIKM 2009.cikm_conference-2009.296 1617987391.0 ABSTRACTIn many Web search engines, a ranking function is selected for deployment mainly by comparing the relevance measurements over candidates. Due to the dynamical nature of the Web, the ranking features and the query and URL distribution on which the ranking functions are built, may change dramatically over time. The actual relevance of the function may degrade, and thus the previous function selection conclusions become invalid. In this work we suggest to select Web ranking functions according to both their relevance and robustness to the changes that may lead to relevance degradation over time. We argue that the ranking robustness can be effectively measured by taking into account the ranking score distribution across search results. We then propose two alternatives to the NDCG metric that both incorporate ranking robustness into ranking function evaluation and selection. A machine learning approach is developed to learn the parameters that control the metric sensitivity to score turbulence, from human-judged preference data. Incorporating robustness into web ranking evaluation", "text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 2007\u20132010 ACM 2009 https://doi.org/10.1145/1645953.1646288 10.1145/1645953.1646288 https://dblp.org/rec/conf/cikm/LiLJZCD09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/LiLJZCD09 inproceedings ['Xin Li', 'Fan Li', 'Shihao Ji', 'Zhaohui Zheng', 'Yi Chang', 'Anlei Dong'] [] CIKM 2009.cikm_conference-2009.296 1617987391.0 ABSTRACTIn many Web search engines, a ranking function is selected for deployment mainly by comparing the relevance measurements over candidates. Due to the dynamical nature of the Web, the ranking features and the query and URL distribution on which the ranking functions are built, may change dramatically over time. The actual relevance of the function may degrade, and thus the previous function selection conclusions become invalid. In this work we suggest to select Web ranking functions according to both their relevance and robustness to the changes that may lead to relevance degradation over time. We argue that the ranking robustness can be effectively measured by taking into account the ranking score distribution across search results. We then propose two alternatives to the NDCG metric that both incorporate ranking robustness into ranking function evaluation and selection. A machine learning approach is developed to learn the parameters that control the metric sensitivity to score turbulence, from human-judged preference data. Incorporating robustness into web ranking evaluation"}, "2018.cikm_conference-2018.98": {"doc_id": "2018.cikm_conference-2018.98", "default_text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 953\u2013962 ACM 2018 https://doi.org/10.1145/3269206.3271742 10.1145/3269206.3271742 https://dblp.org/rec/conf/cikm/ChenFEZC018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ChenFEZC018 inproceedings ['Jiawei Chen', 'Yan Feng', 'Martin Ester', 'Sheng Zhou', 'Chun Chen', 'Can Wang'] [] CIKM 2018.cikm_conference-2018.98 1605210009.0 ABSTRACTUsers' consumption behaviors are affected by both their personal preference and their exposure to items (i.e. whether a user knows the items). Most of the recent works in social recommendation assume that people share similar preference with their socially connected friends. However, this assumption may not hold due to the diversity of social relations, and modeling social influence on users' preference may not be suitable for implicit feedback data (i.e. whether a user has consumed certain items). Since users often share item information with their social relations, it will be less restrictive to model social influence on users' exposure to items. We notice that a user's exposure is affected by the exposure of the other users in his social communities and by the consumption of his connected friends. In this paper, we propose a novel social exposure-based recommendation model SoEXBMF by integrating two kinds of social influence on users' exposure, i.e. social knowledge influence and social consumption influence, into basic EXMF model for better recommendation performance. Furthermore, SoEXBMF uses Bernoulli distribution instead of Gaussian distribution in EXMF to better model the binary implicit feedback data. A variational inference method has been developed for the proposed SoEXBMF model to infer the posterior and make the recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over existing methods in various evaluation metrics. Modeling Users' Exposure with Social Knowledge Influence and Consumption Influence for Recommendation", "text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 953\u2013962 ACM 2018 https://doi.org/10.1145/3269206.3271742 10.1145/3269206.3271742 https://dblp.org/rec/conf/cikm/ChenFEZC018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ChenFEZC018 inproceedings ['Jiawei Chen', 'Yan Feng', 'Martin Ester', 'Sheng Zhou', 'Chun Chen', 'Can Wang'] [] CIKM 2018.cikm_conference-2018.98 1605210009.0 ABSTRACTUsers' consumption behaviors are affected by both their personal preference and their exposure to items (i.e. whether a user knows the items). Most of the recent works in social recommendation assume that people share similar preference with their socially connected friends. However, this assumption may not hold due to the diversity of social relations, and modeling social influence on users' preference may not be suitable for implicit feedback data (i.e. whether a user has consumed certain items). Since users often share item information with their social relations, it will be less restrictive to model social influence on users' exposure to items. We notice that a user's exposure is affected by the exposure of the other users in his social communities and by the consumption of his connected friends. In this paper, we propose a novel social exposure-based recommendation model SoEXBMF by integrating two kinds of social influence on users' exposure, i.e. social knowledge influence and social consumption influence, into basic EXMF model for better recommendation performance. Furthermore, SoEXBMF uses Bernoulli distribution instead of Gaussian distribution in EXMF to better model the binary implicit feedback data. A variational inference method has been developed for the proposed SoEXBMF model to infer the posterior and make the recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over existing methods in various evaluation metrics. Modeling Users' Exposure with Social Knowledge Influence and Consumption Influence for Recommendation"}, "2010.cikm_conference-2010.189": {"doc_id": "2010.cikm_conference-2010.189", "default_text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1501\u20131504 ACM 2010 https://doi.org/10.1145/1871437.1871657 10.1145/1871437.1871657 https://dblp.org/rec/conf/cikm/MoonLCLZC10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/MoonLCLZC10 inproceedings ['Taesup Moon', 'Lihong Li', 'Wei Chu', 'Ciya Liao', 'Zhaohui Zheng', 'Yi Chang'] [] CIKM 2010.cikm_conference-2010.189 1597335221.0 ABSTRACTTraditional machine-learned ranking algorithms for web search are trained in batch mode, which assume static relevance of documents for a given query. Although such a batch-learning framework has been tremendously successful in commercial search engines, in scenarios where relevance of documents to a query changes over time, such as ranking recent documents for a breaking news query, the batch-learned ranking functions do have limitations. Users' real-time click feedback becomes a better and timely proxy for the varying relevance of documents rather than the editorial judgments provided by human editors. In this paper, we propose an online learning algorithm that can quickly learn the best reranking of the top portion of the original ranked list based on real-time users' click feedback. In order to devise our algorithm and evaluate it accurately, we collected exploration bucket data that removes positional biases on clicks on the documents for recency-classified queries. Our initial experimental result shows that our scheme is more capable of quickly adjusting the ranking to track the varying relevance of documents reflected in the click feedback, compared to batch-trained ranking functions. Online learning for recency search ranking using real-time user feedback", "text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1501\u20131504 ACM 2010 https://doi.org/10.1145/1871437.1871657 10.1145/1871437.1871657 https://dblp.org/rec/conf/cikm/MoonLCLZC10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/MoonLCLZC10 inproceedings ['Taesup Moon', 'Lihong Li', 'Wei Chu', 'Ciya Liao', 'Zhaohui Zheng', 'Yi Chang'] [] CIKM 2010.cikm_conference-2010.189 1597335221.0 ABSTRACTTraditional machine-learned ranking algorithms for web search are trained in batch mode, which assume static relevance of documents for a given query. Although such a batch-learning framework has been tremendously successful in commercial search engines, in scenarios where relevance of documents to a query changes over time, such as ranking recent documents for a breaking news query, the batch-learned ranking functions do have limitations. Users' real-time click feedback becomes a better and timely proxy for the varying relevance of documents rather than the editorial judgments provided by human editors. In this paper, we propose an online learning algorithm that can quickly learn the best reranking of the top portion of the original ranked list based on real-time users' click feedback. In order to devise our algorithm and evaluate it accurately, we collected exploration bucket data that removes positional biases on clicks on the documents for recency-classified queries. Our initial experimental result shows that our scheme is more capable of quickly adjusting the ranking to track the varying relevance of documents reflected in the click feedback, compared to batch-trained ranking functions. Online learning for recency search ranking using real-time user feedback"}, "2010.cikm_conference-2010.192": {"doc_id": "2010.cikm_conference-2010.192", "default_text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1513\u20131516 ACM 2010 https://doi.org/10.1145/1871437.1871660 10.1145/1871437.1871660 https://dblp.org/rec/conf/cikm/FengZXY10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/FengZXY10 inproceedings ['Shicong Feng', 'Li Zhang', 'Yuhong Xiong', 'Conglei Yao'] [] CIKM 2010.cikm_conference-2010.192 1546877862.0 ABSTRACTThe goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessible web documents. The major challenge in focused crawling is how to effectively determine each hyperlink's capability of leading to target pages. To compute this capability, we 1 present a novel approach, called Navigational Rank (NR). In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link's capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experimental results show that our approach is domain-independent and significantly outperforms the state-of-arts. Focused crawling using navigational rank", "text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1513\u20131516 ACM 2010 https://doi.org/10.1145/1871437.1871660 10.1145/1871437.1871660 https://dblp.org/rec/conf/cikm/FengZXY10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/FengZXY10 inproceedings ['Shicong Feng', 'Li Zhang', 'Yuhong Xiong', 'Conglei Yao'] [] CIKM 2010.cikm_conference-2010.192 1546877862.0 ABSTRACTThe goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessible web documents. The major challenge in focused crawling is how to effectively determine each hyperlink's capability of leading to target pages. To compute this capability, we 1 present a novel approach, called Navigational Rank (NR). In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link's capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experimental results show that our approach is domain-independent and significantly outperforms the state-of-arts. Focused crawling using navigational rank"}, "2013.cikm_conference-2013.177": {"doc_id": "2013.cikm_conference-2013.177", "default_text": "DBLP:conf/cikm/2013 22nd ACM International Conference on Information and Knowledge Management, CIKM'13, San Francisco, CA, USA, October 27 - November 1, 2013 1441\u20131450 ACM 2013 https://doi.org/10.1145/2505515.2505707 10.1145/2505515.2505707 https://dblp.org/rec/conf/cikm/ZhouZHW13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ZhouZHW13 inproceedings ['Yaqian Zhou', 'Qi Zhang', 'Xuanjing Huang', 'Lide Wu'] [] CIKM 2013.cikm_conference-2013.177 1541519864.0 ABSTRACTTraditional recrawling methods learn navigation patterns in order to crawl related web pages. However, they cannot remove the redundancy found on the web, especially at the object level. To deal with this problem, we propose a new hypertext resource discovery method, called \"selective recrawling\" for object-level vertical search applications. The goal of selective recrawling is to automatically generate URL patterns, then select those pages that have the widest coverage, and least irrelevance and redundancy relative to a pre-defined vertical domain. This method only requires a few seed objects and can select the set of URL patterns that covers the greatest number of objects. The selected set can continue to be used for some time to recrawl web pages and can be renewed periodically. This leads to significant savings in hardware and network resources.In this paper we present a detailed framework of selective recrawling for object-level vertical search. The selective recrawling method automatically extends the set of candidate websites from initial seed objects. Based on the objects extracted from these websites it learns a set of URL patterns which covers the greatest number of target objects with little redundancy. Finally, the navigation patterns generated from the selected URL pattern set are used to guide future crawling. Experiments on local event data show that our method can greatly reduce downloading of web pages while maintaining comparative object coverage. A pattern-based selective recrawling approach for object-level vertical search", "text": "DBLP:conf/cikm/2013 22nd ACM International Conference on Information and Knowledge Management, CIKM'13, San Francisco, CA, USA, October 27 - November 1, 2013 1441\u20131450 ACM 2013 https://doi.org/10.1145/2505515.2505707 10.1145/2505515.2505707 https://dblp.org/rec/conf/cikm/ZhouZHW13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ZhouZHW13 inproceedings ['Yaqian Zhou', 'Qi Zhang', 'Xuanjing Huang', 'Lide Wu'] [] CIKM 2013.cikm_conference-2013.177 1541519864.0 ABSTRACTTraditional recrawling methods learn navigation patterns in order to crawl related web pages. However, they cannot remove the redundancy found on the web, especially at the object level. To deal with this problem, we propose a new hypertext resource discovery method, called \"selective recrawling\" for object-level vertical search applications. The goal of selective recrawling is to automatically generate URL patterns, then select those pages that have the widest coverage, and least irrelevance and redundancy relative to a pre-defined vertical domain. This method only requires a few seed objects and can select the set of URL patterns that covers the greatest number of objects. The selected set can continue to be used for some time to recrawl web pages and can be renewed periodically. This leads to significant savings in hardware and network resources.In this paper we present a detailed framework of selective recrawling for object-level vertical search. The selective recrawling method automatically extends the set of candidate websites from initial seed objects. Based on the objects extracted from these websites it learns a set of URL patterns which covers the greatest number of target objects with little redundancy. Finally, the navigation patterns generated from the selected URL pattern set are used to guide future crawling. Experiments on local event data show that our method can greatly reduce downloading of web pages while maintaining comparative object coverage. A pattern-based selective recrawling approach for object-level vertical search"}, "2011.wwwconf_conference-2011c.86": {"doc_id": "2011.wwwconf_conference-2011c.86", "default_text": "DBLP:conf/www/2011c Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011 (Companion Volume) 171\u2013172 ACM 2011 https://doi.org/10.1145/1963192.1963279 10.1145/1963192.1963279 https://dblp.org/rec/conf/www/YuZWC11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuZWC11 inproceedings ['Jianxing Yu', 'Zheng-Jun Zha', 'Meng Wang', 'Tat-Seng Chua'] [] WWW 2011.wwwconf_conference-2011c.86 1585295462.0 ABSTRACTIn this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach. Hierarchical organization of unstructured consumer reviews", "text": "DBLP:conf/www/2011c Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011 (Companion Volume) 171\u2013172 ACM 2011 https://doi.org/10.1145/1963192.1963279 10.1145/1963192.1963279 https://dblp.org/rec/conf/www/YuZWC11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuZWC11 inproceedings ['Jianxing Yu', 'Zheng-Jun Zha', 'Meng Wang', 'Tat-Seng Chua'] [] WWW 2011.wwwconf_conference-2011c.86 1585295462.0 ABSTRACTIn this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach. Hierarchical organization of unstructured consumer reviews"}, "2014.wwwconf_conference-2014c.198": {"doc_id": "2014.wwwconf_conference-2014c.198", "default_text": "DBLP:conf/www/2014c 23rd International World Wide Web Conference, WWW '14, Seoul, Republic of Korea, April 7-11, 2014, Companion Volume 595\u2013598 ACM 2014 https://doi.org/10.1145/2567948.2578039 10.1145/2567948.2578039 https://dblp.org/rec/conf/www/BorgolteKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BorgolteKV14 inproceedings ['Kevin Borgolte', 'Christopher Kruegel', 'Giovanni Vigna'] [] WWW 2014.wwwconf_conference-2014c.198 1541519827.0 ABSTRACTTracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on \"diffing\" XML-encoded literary documents or XMLencoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software. Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines", "text": "DBLP:conf/www/2014c 23rd International World Wide Web Conference, WWW '14, Seoul, Republic of Korea, April 7-11, 2014, Companion Volume 595\u2013598 ACM 2014 https://doi.org/10.1145/2567948.2578039 10.1145/2567948.2578039 https://dblp.org/rec/conf/www/BorgolteKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BorgolteKV14 inproceedings ['Kevin Borgolte', 'Christopher Kruegel', 'Giovanni Vigna'] [] WWW 2014.wwwconf_conference-2014c.198 1541519827.0 ABSTRACTTracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on \"diffing\" XML-encoded literary documents or XMLencoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software. Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines"}, "2008.wwwconf_conference-2008.153": {"doc_id": "2008.wwwconf_conference-2008.153", "default_text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 1117\u20131118 ACM 2008 https://doi.org/10.1145/1367497.1367684 10.1145/1367497.1367684 https://dblp.org/rec/conf/www/HuLCS08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HuLCS08 inproceedings ['Nan Hu', 'Ling Liu', 'Bin Chen', 'Jialie Shen'] [] WWW 2008.wwwconf_conference-2008.153 1541519828.0 ABSTRACTThis paper investigates the strategic decisions of online vendors for offering different mechanism, such as sampling and online reviews of information products, to increase their online sales. Focusing on measuring the effectiveness of electronic market design (offering reviews, sampling, or both), our study shows that online markets behavior as communication markets, and consumers learn product quality information both passively (reading online reviews) and actively but subjectively (listening to music sampling). Using data from Amazon, first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect. In general, products with sampling option enjoy a higher conversion rate (which leads to better sales) than those without sampling because sampling decreases the uncertainty of consuming experience goods. Second, the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without. Third, when the uncertainty of the societal reviews is higher, sampling plays a more important role because it mitigates such uncertainty introduced by online reviews. How to influence my customers?: the impact of electronic market design", "text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 1117\u20131118 ACM 2008 https://doi.org/10.1145/1367497.1367684 10.1145/1367497.1367684 https://dblp.org/rec/conf/www/HuLCS08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HuLCS08 inproceedings ['Nan Hu', 'Ling Liu', 'Bin Chen', 'Jialie Shen'] [] WWW 2008.wwwconf_conference-2008.153 1541519828.0 ABSTRACTThis paper investigates the strategic decisions of online vendors for offering different mechanism, such as sampling and online reviews of information products, to increase their online sales. Focusing on measuring the effectiveness of electronic market design (offering reviews, sampling, or both), our study shows that online markets behavior as communication markets, and consumers learn product quality information both passively (reading online reviews) and actively but subjectively (listening to music sampling). Using data from Amazon, first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect. In general, products with sampling option enjoy a higher conversion rate (which leads to better sales) than those without sampling because sampling decreases the uncertainty of consuming experience goods. Second, the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without. Third, when the uncertainty of the societal reviews is higher, sampling plays a more important role because it mitigates such uncertainty introduced by online reviews. How to influence my customers?: the impact of electronic market design"}, "2020.wwwconf_conference-2020c.108": {"doc_id": "2020.wwwconf_conference-2020c.108", "default_text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 331\u2013336 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382184 10.1145/3366424.3382184 https://dblp.org/rec/conf/www/HernandezNI20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HernandezNI20 inproceedings ['Anthony Hernandez', 'Kin Wai Ng', 'Adriana Iamnitchi'] [] WWW 2020.wwwconf_conference-2020c.108 1602688588.0 The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub. Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations", "text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 331\u2013336 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382184 10.1145/3366424.3382184 https://dblp.org/rec/conf/www/HernandezNI20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HernandezNI20 inproceedings ['Anthony Hernandez', 'Kin Wai Ng', 'Adriana Iamnitchi'] [] WWW 2020.wwwconf_conference-2020c.108 1602688588.0 The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub. Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations"}, "2020.wwwconf_conference-2020.114": {"doc_id": "2020.wwwconf_conference-2020.114", "default_text": "DBLP:conf/www/2020 WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 1264\u20131274 ACM / IW3C2 2020 https://doi.org/10.1145/3366423.3380202 10.1145/3366423.3380202 https://dblp.org/rec/conf/www/YuCGLLL20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuCGLLL20 inproceedings ['Fuqiang Yu', 'Lizhen Cui', 'Wei Guo', 'Xudong Lu', 'Qingzhong Li', 'Hua Lu'] [] WWW 2020.wwwconf_conference-2020.114 1608855298.0 As considerable amounts of POI check-in data have been accumulated, successive point-of-interest (POI) recommendation is increasingly popular. Existing successive POI recommendation methods only predict where user will go next, ignoring when this behavior will occur. In this work, we focus on predicting POIs that will be visited by users in the next 24 hours. As check-in data is very sparse, it is challenging to accurately capture user preferences in temporal patterns. To this end, we propose a category-aware deep model CatDM that incorporates POI category and geographical influence to reduce search space to overcome data sparsity. We design two deep encoders based on LSTM to model the time series data. The first encoder captures user preferences in POI categories, whereas the second exploits user preferences in POIs. Considering clock influence in the second encoder, we divide each user's check-in history into several different time windows and develop a personalized attention mechanism for each window to facilitate CatDM to exploit temporal patterns. Moreover, to sort the candidate set, we consider four specific dependencies: user-POI, user-category, POI-time and POI-user current preferences. Extensive experiments are conducted on two large real datasets. The experimental results demonstrate that our CatDM outperforms the state-of-the-art models for successive POI recommendation on sparse check-in data. CCS CONCEPTS \u2022 Information systems \u2192 Data mining; \u2022 Computing methodologies \u2192 Machine learning. A Category-Aware Deep Model for Successive POI Recommendation on Sparse Check-in Data", "text": "DBLP:conf/www/2020 WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 1264\u20131274 ACM / IW3C2 2020 https://doi.org/10.1145/3366423.3380202 10.1145/3366423.3380202 https://dblp.org/rec/conf/www/YuCGLLL20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuCGLLL20 inproceedings ['Fuqiang Yu', 'Lizhen Cui', 'Wei Guo', 'Xudong Lu', 'Qingzhong Li', 'Hua Lu'] [] WWW 2020.wwwconf_conference-2020.114 1608855298.0 As considerable amounts of POI check-in data have been accumulated, successive point-of-interest (POI) recommendation is increasingly popular. Existing successive POI recommendation methods only predict where user will go next, ignoring when this behavior will occur. In this work, we focus on predicting POIs that will be visited by users in the next 24 hours. As check-in data is very sparse, it is challenging to accurately capture user preferences in temporal patterns. To this end, we propose a category-aware deep model CatDM that incorporates POI category and geographical influence to reduce search space to overcome data sparsity. We design two deep encoders based on LSTM to model the time series data. The first encoder captures user preferences in POI categories, whereas the second exploits user preferences in POIs. Considering clock influence in the second encoder, we divide each user's check-in history into several different time windows and develop a personalized attention mechanism for each window to facilitate CatDM to exploit temporal patterns. Moreover, to sort the candidate set, we consider four specific dependencies: user-POI, user-category, POI-time and POI-user current preferences. Extensive experiments are conducted on two large real datasets. The experimental results demonstrate that our CatDM outperforms the state-of-the-art models for successive POI recommendation on sparse check-in data. CCS CONCEPTS \u2022 Information systems \u2192 Data mining; \u2022 Computing methodologies \u2192 Machine learning. A Category-Aware Deep Model for Successive POI Recommendation on Sparse Check-in Data"}, "2013.wwwconf_conference-2013.41": {"doc_id": "2013.wwwconf_conference-2013.41", "default_text": "DBLP:conf/www/2013 22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013 471\u2013482 International World Wide Web Conferences Steering Committee / ACM 2013 https://doi.org/10.1145/2488388.2488430 10.1145/2488388.2488430 https://dblp.org/rec/conf/www/GollapalliCMG13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GollapalliCMG13 inproceedings ['Sujatha Das Gollapalli', 'Cornelia Caragea', 'Prasenjit Mitra', 'C. Lee Giles'] [] WWW 2013.wwwconf_conference-2013.41 1603662470.0 ABSTRACTA classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying \"irrelevant\" pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end, we design novel URL-based features and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.In addition, we propose a novel technique for \"learning a conforming pair of classifiers\" using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make \"similar\" predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Miscellaneous\nGeneral TermsAlgorithms Keywords co-training, consensus maximization, gradient descent\nMOTIVATIONProfessional homepages of researchers, which typically summarize research interests, publications and other metadata related to researchers, are shown to be rich sources of information for digital libraries . Researchers' homepages    Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.WWW 2013, May 13-17, 2013, Rio de Janeiro, Brazil.   ACM 978-1-4503-2035 (also referred to as academic homepages or simply homepages in this paper) have been successfully employed in tasks such as expertise search , extraction of academic networks, author profile extraction and disambiguation  as they provide crucial evidence for improving these tasks in digital libraries.Furthermore, digital library systems such as CiteSeer 1 , ArnetMiner 2 , and Google Scholar 3 are primarily interested in obtaining and tracking researchers' homepages in order to retrieve appropriate scientific research publications. Given the infeasibility of collecting the entire content on the Web, a focused crawler aims to minimize the use of network bandwidth and hardware by selectively crawling only pages relevant to a (specified) set of topics . A key component for such a crawler is a classification module that identifies whether a webpage being accessed during the crawl process is potentially useful to the collection. For digital libraries, the \"yield\" of such crawlers highly depends on the accuracy of researcher homepage classification.Supervised methods for learning homepage classifiers rely on the availability of large amounts of labeled data. A widely used labeled dataset for webpage classification is the WebKB dataset 4 that is built in 1997. However, due to recent changes in the information content on academic websites, this dataset is becoming outdated. For example, there are now pages on academic websites that are related to various activities such as invited talks, news, events that do not occur in the WebKB dataset. We refer to university, department and research center websites as \"academic websites\" in this paper. Compared to few decades back, it is easier now to find faculty information, links to their homepages, information on research groups, course related notes and documents, and research papers from academic websites. Similarly, job postings, seminar announcements and notices are also being uploaded onto departmental websites in recent times .How can a homepage classifier keep up in the face of rapidly changing types of pages on the Web? Specifically, given a classifier that identifies homepages with reasonable accuracy (as measured on the training datasets), how does it perform in the potentially different deployment environment? Semi-supervised methods that can exploit large amounts of unlabeled data together with limited amounts of labeled data for learning accurate classifiers have received significant attention in recent research in machine learning due to Against this background, one question that can be raised is: Can we design techniques to effectively adjust the previouslytrained classifier to the changed content on the Web, while minimizing the human effort required for labeling new data, and under what conditions, can such an adjustment be possible? The research that we describe in this paper addresses specifically this question.Contributions and Organization. We present two approaches to researcher homepage classification using unlabeled data readily available from academic websites. More precisely, we first adapt the well-known co-training approach  to reflect the change in the data distribution over time. Second, we design an iterative algorithm based on the Minibatch Gradient Descent technique for learning a conforming pair of predictors using two different views of the data. We restrict ourselves to homepages of researchers in Computer Science since training datasets are available for this discipline. To the best of our knowledge, the problem of researcher homepage classification using unlabeled data available during focused crawling was not addressed in previous research. The contributions of this work are as follows:\u2022 We show that with the classifiers trained on existing datasets for researcher homepage classification we incorrectly identify pages of types not seen in the training datasets as homepages. This results in unacceptable yield from the perspective of a focused crawler.\u2022 We design novel features based on URL surface patterns and terms to complement term and HTML features extracted from the content of homepages and show that these two sets of features can be treated as two \"views\" for a researcher homepage instance.\u2022 We show that the URL and content-based views can be used successfully in a co-training setup to adapt classifiers to the changing academic environments using unlabeled data. This finding enables us to accurately crawl the new academic website content without having to label a new dataset for re-training the classifier.\u2022 Inspired by the success of co-training on this problem, we investigate loss functions that capture the disparity between the classifiers' predictions on the two views afforded by co-training. We design an iterative algorithm based on the Mini-batch Gradient Descent technique for minimizing this loss and learning a conforming pair of predictors with the two views.\u2022 Finally, we show that minimizing our proposed loss function on unlabeled data closely corresponds to the effect demonstrated by co-training techniques. We posit that this loss can, therefore, be used as a measure, for tracking the progress of co-training schemes even in the absence of a validation dataset.Although in this paper we focus on the design of accurate approaches for researcher homepage classification, our objective is to integrate this classification component in the context of focused crawling, to improve retrieval and indexing of scientific publications in digital libraries such as CiteSeer and ArnetMiner. In these usage environments, since maintaining up-to-date collections of research literature is of primary importance, having an accurate list of homepage URLs for frequent, periodic tracking is both feasible and scalable, compared to examining the entire content at academic websites each time. The rest of this paper is organized as follows: We briefly summarize closely related work in Section 2. Researcher homepage classification is discussed in Section 3. We elaborate on details of our co-training experiments and learning conforming predictor pairs in Section 4. Experimental setup, datasets and results are discussed in Section 5, followed by a summary and future extensions to our work.\nRELATED WORKResearcher homepage classification is a well-studied webpage classification problem in context of digital libraries such as CiteSeer  and ArnetMiner . Typically, contentbased term features and HTML structure-based features are used for classifying webpages . We propose the use of URL features as additional evidence for homepage identification. A smaller set (compared to ours) of URL-based features (presence of part of the name, presence of the character '\u223c', etc.), was used in isolating homepages among the search engine results for researcher name queries by Tang, et  al [40]. URL-based features are widely used in tasks related to the Web. For example, URL strings were used for extracting rules to solve the webpage de-duplication problem . Shih and Karger [38]  used URL features, the visual placements of links in the referring pages to a URL for improving applications such as ad-blocking and recommendation, whereas a preliminary study by Kan and Thi [20]  illustrates the use of URLs in performing fast webpage classification.The problem of collecting a high-quality researcher homepage collection was studied for Japanese websites by Wang, et al. using on-page and anchor text features . Tang, et al. studied homepage acquisition from search engine results using researcher names as queries . In contrast, we seek to apply focused crawling using a seed list of academic websites (where researcher homepages are typically hosted) to acquire such a collection. Focused crawling first proposed by Bra, et al. is a rich area of research on the Web . Chakrabarti, et al. [7]  present a discussion on the main components involved in building a focused crawler. Although focused crawling is our motivating application, this paper deals with the classifier component of the crawler and not with the crawler itself.We show that the focused crawling scenario presents novel challenges in using a pre-trained homepage classifier in identifying relevant pages. Specifically, the classifier needs to be attuned to the changing types of pages on the Web. Cotraining is proposed as a solution for addressing this challenge for homepage classification. Blum and Mitchell [4] first proposed co-training, an approach for semi-supervised learning when the number of labeled examples available for training is limited, and applied it to webpage classification. This approach requires having two views of features for the instances and has been shown to work well when the two views satisfy certain assumptions on \"sufficiency\" and \"independence\" . Recent research addresses techniques for decomposing the feature set into two views when such a split is not naturally available .Multiview learning (of which co-training is a special case, with two views) is typically addressed by maximizing \"consensus\" or agreement among the different views . Most solutions to multiview learning tend to frame the problem in terms of a global optimization problem and simulta-472 neously learn classifiers for all the underlying views. In some cases, the solutions depend on underlying classification algorithm used . Although our proposed algorithm based on mini-batch gradient descent seeks to maximize consensus as well, our approach is a generic technique assuming only that the underlying classifiers output initial \"parameter vectors\" that are altered using a simple, iterative algorithm.\nFEATURES FOR HOMEPAGE CLASSIFICATIONWebpage or text classification is typically handled using \"bag of words\" approaches. Specifically, the frequently occurring and discerning terms are collected from training data to form a feature dictionary that is used to represent instances as normalized term frequency or TFIDF vectors . Homepage classification was previously studied as a text classification problem using term features . Previous work on the same problem also used other content-based features related to the HTML structure of the page such as the number of images/tables on the page, and the terms commonly found in anchor text of homepages . In this study, we extracted content-based and URL-based features from our training sets. These features and the size of feature sets are summarized in . The term dictionaries contain terms that occur in at least three documents (i.e., webpages) and at least five times in the training set.In addition to term dictionaries, we hypothesize that the URL strings of homepages can provide additional evidence for identifying homepages. Hence, we design novel URLbased features based on surface patterns and presence in WordNet 5 . The URL-based features are explained in the next subsection.  \nURL strings as additional evidenceThe idea of using URL strings in academic homepage identification comes from an error analysis of a crawl obtained with the content-based classifier. Consider some example URLs we encountered in our crawl listed in .With some knowledge in academic browsing, one can confidently guess that the webpages at the URLs (1), (2), and (3) are unlikely to be researcher homepages. Similarly, among the URLs (4), and (5), while the former seems to be a homepage, the latter seems to lead to a course page. The above conjectures are based on the presumption that the URL strings are not \"arbitrary\", but, instead conventions are observed that are indicative of the target content at the URL. For instance, in the previous examples, words such as \"projects\", \"events\", alphanumeric patterns of the terms in the URL indicate that the URLs, (1), (2), (3) and (5) are most possibly not researcher homepages.Treating \"/\" as delimiters, we extract features from the URL string following the domain name of a webpage. The list of all unigrams and bigrams from URL strings that occur more than thrice in the training dataset, comprise the URL-term dictionary. For terms in the URL not present in this dictionary, we look for their presence in WordNet to check if they are common words or proper nouns. WordNet is a large, lexical database of nouns, verbs, adjectives and adverbs for English, organized as a concept graph .In addition, we capture the surface patterns of the URLs including the presence of hyphenated or underscored words, alphanumeric patterns, long words (i.e., words having greater than 30 characters), question marks and the presence of characters such as tilde. These features are designed to filter out the URLs that commonly represent course pages, announcements, calendars and other auto-generated content. For instance, a typical homepage URL string in Computer Science departments has the name of the researcher following the \u223c character after the domain name (e.g., http://people.cs.umass.edu/\u223cmccallum/). This pattern is usually captured by our \"TILDENONDICT\" feature, where mccallum is a non-dictionary term. Partial sets of extracted features are shown along with the URLs listed in .The above sets of features perform very well on the training datasets as shown Section 5. We, therefore, do not study other complicated, problem-specific feature design or feature selection. Instead our focus in this work is to study how these classifiers perform \"in the wild\". We also note here that, a classifier that can make accurate predictions using URL features can be quite beneficial from the perspective of efficiency for a focused crawler. A crawler can potentially bypass examining the content of a page if a confident decision can be made based on the URL string. However, we may not be able to always extract features from the URL strings. For instance, consider the following URLs from our crawls:http://john.blitzer.com/ http://clgiles.ist.psu.edu/ http://ben.adida.net/ In these cases, it is not clear from the URL string that the target content refers to academic homepages. Even if complicated name-extraction based features were designed for the above cases, it is rare to find academic homepages with '.com' and '.net' domain suffixes. Based on the URL alone, we cannot be confident if the target content is an academic homepage or a company/personal homepage. For the second case, 'clgiles' could refer to a machine name. In addition to the above cases, given that feature dictionaries typically comprise features that meet a frequency requirement, we may not be able to extract features for all URLs. In our training datasets (Section 5), we were unable to extract URL features for about 27% of the instances. Therefore, contentbased and URL features complement each other while identifying homepage instances and a focused crawler might be required to use either or both of these sets of features.\nHOMEPAGE CLASSIFICATION USING UNLABELED DATAWe show in our experiments (Section 5) that, although content-based features perform extremely well on the training datasets, they are not very successful on the validation and test sets that were collected from the current-day academic websites. On the other hand, URL features show good performance on both training and validation datasets.  However, as pointed out in the previous section, we may not be able to extract URL features for all instances and it is, therefore, imperative to have an accurate content-based classifier as well. We now address the questions: Can we adapt the contentbased classifier to perform well in the deployment environment with the help of the URL-based classifier? Can the two classifiers \"teach\" each other so as to perform better in the new environment, using the co-training approach? Since the URL and content features provide evidence for classifying a webpage instance independently, intuitively, it appears possible that there are instances that the URL classifier makes mistakes on, which the content-based classifier identifies correctly and vice versa.Blum and Mitchell proposed co-training in context of webpage classification . In their datasets, webpages are representable in terms of two distinct views: using terms on webpages and terms in the anchor text of hyperlinks pointing to these pages. When few labeled examples are available for training, they showed that co-training could be used to obtain predictions on the unlabeled data to enlarge the training set. Blum and Mitchell's experiments and the subsequent experiments by Nigam and Ghani  showed that when a natural split of features is available, co-training that explicitly leverages this split has the potential to outperform classifiers that do not.We study the applicability and extension of co-training for our problem. Although the essential motivation is to make use of the naturally available feature split and enable classifiers to learn from each other, we highlight the following aspects of our setup: Previous studies and benefits from co-training were illustrated on datasets where the unlabeled data is arguably from a similar distribution. That is, the positive and negative instances in the labeled datasets are representative of those in the unlabeled data. This is in contrast to our case, where our positive class is fairly welldefined (homepages), whereas the negative class is described in terms of \"not positive\". More precisely, although our training dataset has examples for the negative class, webpages encountered during the crawls can belong to types not encountered in the labeled data. We present an error analysis in Section 5, that illustrates the \"new\" types of webpages encountered in our crawl, potentially causing the pre-trained content-based classifiers to underperform during crawling.The number of negative instances encountered during our crawls is higher in comparison with the number of positive instances. While this aspect was noticed during our experiments, a previous estimation experiment using markrecapture methods had indicated that academic homepages comprise a minute fraction of the Web . We can expect this imbalance to become more prominent as more examples are sampled over the co-training rounds. In the algorithm studied by Blum and Mitchell, the ratio between the number of positive and negative instances added from the unlabeled data is maintained to be the same as that in the training dataset during each iteration of co-training . We argue that avoiding this constraint is better in our scenario since we want the datasets to be more representative of the changing distribution.Most classification algorithms are sensitive to the number of positive and negative instances available in the training data and are known to learn biased classifiers in case of severe imbalance . We employ the idea of altering the mis-classification costs for the underlying classifiers during each round of co-training to handle this problem. For example, if the training dataset has 10 positive and 100 negative instances, we can set the penalty incurred on making mistakes on a negative instance to be 1 10 th of the penalty incurred on making mistakes on a positive instance. For most implementations of classification algorithms, the misclassification costs can be specified as a parameter during the training process .Our co-training setup is detailed in Algorithm 1. L and U represent the labeled and unlabeled datasets, respectively, available at each iteration. They comprise instances with both the views (content-based and URL-based feature sets). For a round of co-training, we train classifiers, C1 and C2, on the two available views, using misclassification costs, \u03c11 and \u03c12, respectively. Next, \"s\" number of examples are sampled without replacement into S from the unlabeled data and C1 and C2 are used to obtain predictions for these instances. The GetConf identEgs method is a generic placeholder that stands for a function that determines what instances from S are chosen for addition in subsequent rounds of co-training. We use the notation L + 1 to represent the positive instances in the set L1 whereas L 1 1 indicates that the view 1 (or feature set 1) of the examples in L1 is being used.Based on previous studies in co-training , we studied the following strategies for this function:\u2022 AddBoth: In this scheme, we add all examples from S that are labeled by C1 or C2 confidently to the training set for the next round. This approach is similar to selftraining used in semi-supervised learning where confidently predicted unlabeled instances are added to the training set for retraining the classifier in subsequent rounds . However, in contrast with self-training that uses a single view, in AddBoth, confident predictions are obtained from two sources (view 1 and 2) for addition into subsequent rounds.\u2022 AddCross: In this scheme, examples from S, confidently labeled by C1 are added to view 2 for the next 474 round and vice versa. That is, we use the examples confidently labeled by one classifier while training the other classifier in the next round. Cross-addition also seems resilient to handling the possibility of cascaded errors over the iterations. If a classifier makes a confident but incorrect prediction, we would like to avoid feeding this example in the next round to the same classifier, a common problem in self-training [45].\u2022 AddCrossRC: This scheme is similar to AddCross with the additional constraint on the number of positive and negative instances added in each round. This constraint was originally studied by Blum and Mitchell and ensures that the ratio of the number of positive and negative instances added in each round is the same as that in the initial labeled dataset .Algorithm 1 Procedure for Co-trainingCompute \u03c1 1 usingThe co-training algorithm is general and can be applied with any choice of classifiers on the two views. Blum and Mitchell provided a PAC-style analysis of co-training with probabilistic classifiers and showed that co-training works when the assumptions on sufficiency and independence are met. That is, each view should be sufficient to predict the class label, and the two views are independent given the class label. Recent studies have proposed relaxed criteria under which co-training techniques still work . However, in practice, it is tricky to judge if co-training works for a problem and to verify if the assumptions are satisfied . These questions are more relevant in context of recent research in obtaining two views from a single view when two views are not naturally available for applying co-training . With this context, we now discuss our formulation of the effect obtained with co-training, in terms of a loss function. This formulation allows us to track whether the co-training process is beneficial for a given problem, even without the use of a validation dataset.\nLearning Conforming Predictors on Unlabeled DataWe assume that classifiers, C1 and C2 trained on the two views are parameterized in terms of their weight vectors, w1 and w2. Most classification algorithms e.g., Support Vector Machines (SVM) and Maximum Entropy (MaxEnt), output weight vectors capturing the importance of each feature as part of the training process .One can expect co-training to benefit a classification problem if one classifier (say, C1) can \"guide\" the other (C2) on examples that the latter makes mistakes on. This guidance is provided by adding examples confidently labeled by C1 to the subsequent round of training C2. This observation hints at the possibility of directly manipulating C2, based on C1's prediction for an example that C2 is not confident about. This effect can be achieved by optimizing a function that directly captures the mismatch in the predictions of the two classifiers.Elaborating further, given that the concept classes, \"positive\" and \"negative\" are still the same on unlabeled data, if C1 and C2 are accurate, they would make similar predictions on the unlabeled data. This intuition is the basis for \"consensus maximization\" widely adopted in multiview learning, of which co-training is a special case with two views . The mismatch in predictions by C1 and C2 on unlabeled data can be quantified using a loss function. The squared error loss function commonly used in machine learning captures this loss as:The above formulation captures the average squared-difference in predictions from the two views on unlabeled data. w1 and w2 correspond to the parameter vectors corresponding to C1 and C2, respectively, and u refers to an example from U , having two views, u1 and u2. For a given example, u = (u1, u2), the functions, f1 and f2 act on u1 and u2 respectively, and make the predictions from C1 and C2 comparable. These functions could be generic (e.g. a function that outputs the probability that the instance is positive) or classifier-dependent (for e.g. a function that outputs scaled distances from the separating hyperplane in case of Support Vector Machines). Minimizing L corresponds to adjusting the weight vectors, w1 and w2, so that they make similar predictions on U . In contrast with multiview learning methods, where learning the classifiers is folded into a global objective function in sophisticated ways , we adopt a simpler approach that works off the initial parameter vectors and iteratively modifies them in a \"co-training like\" manner. Note that this initialization plays a crucial role in avoiding trivial solutions (such as w1, w2 = 0) that are potentially possible since the loss is optimized only on unlabeled instances. Our proposed technique for obtaining the \"pair of conforming classifiers\" is described in Algorithm 2.In Algorithm 2, we start with the original parameter vectors w1 and w2 from classifiers C1 and C2, respectively, and iteratively adjust these vectors so that the values of f1(w1, u1) and f2(w2, u2) look similar for all u \u2208 U . The input parameter, #oIters, refers to the number of times the inner loop comprising of the two gradient descent steps is executed, where as, the #iIters, and \u03b1 are parameters for the gradient descent algorithm. Overall, the values of #oIters, #iIters, and \u03b1 control the rate of convergence of the algorithm and can be set experimentally. These parameters can be set based on the base classifiers used, noting when the decrease in the objective function value is below a threshold. Adaptive tuning of these parameters by tracking the change in the value of the objective function in every iteration is a subject for future study .In each iteration, we employ mini-batch gradient descent to minimize the loss function, once w.r.t. w1 and next w.r.t. w2. The mini-batch gradient descent algorithm is a hybrid approach often used for large-scale machine learning problems. This approach combines the best of stochastic (on- line) gradient descent and batch gradient descent to obtain fast convergence during optimization by running gradient descent on small batches of randomly selected examples .In our algorithm, in each iteration, a small batch of instances are randomly sampled from the unlabeled data, U and the loss function defined using instances for which w1 makes confident predictions from this sampled set. This loss is minimized using gradient descent to adjust w2. A similar process is then applied for adjusting w1 using confident predictions from w2. In effect, as the algorithm proceeds, we are adjusting the parameters of each classifier so that it makes predictions that are aligned with those of the other classifier's confident predictions. Upon convergence, both w1 and w2 are adjusted so that they make conforming predictions on the unlabeled data.In our experiments, we used the differentiable, logistic sigmoid function for f1 and f2. Typically, classifiers use the parameter vector, w, for computing decision values for each instance. That is, given an instance x, the dot product value, w, x , is used for determining the label assignment for the instance. This value can be 'squashed' to a number between 0 and 1 indicating that the probability that instance has a particular label with the logistic function [3]: P (t) = 1 1 + e \u2212t with dP (t) dt = P (t) \u00b7 (1 \u2212 P (t)) Given, the simple form for the derivative, we can directly use the values of f1 and f2 (that we compute anyway), for computing the gradients in Algorithm 2. Although the effect obtained by Algorithm 2 is similar to that of co-training, the conformity loss directly measures the effect of co-training as it is being applied. In contrast, Algorithm 1 is typically terminated either when no more examples are available or by tracking the performance on a validation dataset.We provide a preliminary, experimental demonstration of the connection between co-training and our proposed algorithm in Section 5. A more detailed analysis, study of other choices for the loss function L and the functions, f1 and f2, are a subject of future work. Nevertheless, quantifying the discrepancy in predictions from the two views and an algorithm to directly address this aspect is an exciting step in understanding when co-training works. We show in Section 5 that our method can be used in lieu of a validation dataset for tracking the performance of co-training.\nEXPERIMENTSWe discuss 3 types of experiments: First, we study the performance of content-based and URL-based features on the training and validation datasets. Second, we show that co-training can successfully address the problem of mismatch in the training and deployment environments for homepage classification. Finally, we show that our proposed algorithm (Algorithm 2), achieves the same effect as co-training. Researcher homepage classification using unlabeled data", "text": "DBLP:conf/www/2013 22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013 471\u2013482 International World Wide Web Conferences Steering Committee / ACM 2013 https://doi.org/10.1145/2488388.2488430 10.1145/2488388.2488430 https://dblp.org/rec/conf/www/GollapalliCMG13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GollapalliCMG13 inproceedings ['Sujatha Das Gollapalli', 'Cornelia Caragea', 'Prasenjit Mitra', 'C. Lee Giles'] [] WWW 2013.wwwconf_conference-2013.41 1603662470.0 ABSTRACTA classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying \"irrelevant\" pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end, we design novel URL-based features and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.In addition, we propose a novel technique for \"learning a conforming pair of classifiers\" using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make \"similar\" predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Miscellaneous\nGeneral TermsAlgorithms Keywords co-training, consensus maximization, gradient descent\nMOTIVATIONProfessional homepages of researchers, which typically summarize research interests, publications and other metadata related to researchers, are shown to be rich sources of information for digital libraries . Researchers' homepages    Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.WWW 2013, May 13-17, 2013, Rio de Janeiro, Brazil.   ACM 978-1-4503-2035 (also referred to as academic homepages or simply homepages in this paper) have been successfully employed in tasks such as expertise search , extraction of academic networks, author profile extraction and disambiguation  as they provide crucial evidence for improving these tasks in digital libraries.Furthermore, digital library systems such as CiteSeer 1 , ArnetMiner 2 , and Google Scholar 3 are primarily interested in obtaining and tracking researchers' homepages in order to retrieve appropriate scientific research publications. Given the infeasibility of collecting the entire content on the Web, a focused crawler aims to minimize the use of network bandwidth and hardware by selectively crawling only pages relevant to a (specified) set of topics . A key component for such a crawler is a classification module that identifies whether a webpage being accessed during the crawl process is potentially useful to the collection. For digital libraries, the \"yield\" of such crawlers highly depends on the accuracy of researcher homepage classification.Supervised methods for learning homepage classifiers rely on the availability of large amounts of labeled data. A widely used labeled dataset for webpage classification is the WebKB dataset 4 that is built in 1997. However, due to recent changes in the information content on academic websites, this dataset is becoming outdated. For example, there are now pages on academic websites that are related to various activities such as invited talks, news, events that do not occur in the WebKB dataset. We refer to university, department and research center websites as \"academic websites\" in this paper. Compared to few decades back, it is easier now to find faculty information, links to their homepages, information on research groups, course related notes and documents, and research papers from academic websites. Similarly, job postings, seminar announcements and notices are also being uploaded onto departmental websites in recent times .How can a homepage classifier keep up in the face of rapidly changing types of pages on the Web? Specifically, given a classifier that identifies homepages with reasonable accuracy (as measured on the training datasets), how does it perform in the potentially different deployment environment? Semi-supervised methods that can exploit large amounts of unlabeled data together with limited amounts of labeled data for learning accurate classifiers have received significant attention in recent research in machine learning due to Against this background, one question that can be raised is: Can we design techniques to effectively adjust the previouslytrained classifier to the changed content on the Web, while minimizing the human effort required for labeling new data, and under what conditions, can such an adjustment be possible? The research that we describe in this paper addresses specifically this question.Contributions and Organization. We present two approaches to researcher homepage classification using unlabeled data readily available from academic websites. More precisely, we first adapt the well-known co-training approach  to reflect the change in the data distribution over time. Second, we design an iterative algorithm based on the Minibatch Gradient Descent technique for learning a conforming pair of predictors using two different views of the data. We restrict ourselves to homepages of researchers in Computer Science since training datasets are available for this discipline. To the best of our knowledge, the problem of researcher homepage classification using unlabeled data available during focused crawling was not addressed in previous research. The contributions of this work are as follows:\u2022 We show that with the classifiers trained on existing datasets for researcher homepage classification we incorrectly identify pages of types not seen in the training datasets as homepages. This results in unacceptable yield from the perspective of a focused crawler.\u2022 We design novel features based on URL surface patterns and terms to complement term and HTML features extracted from the content of homepages and show that these two sets of features can be treated as two \"views\" for a researcher homepage instance.\u2022 We show that the URL and content-based views can be used successfully in a co-training setup to adapt classifiers to the changing academic environments using unlabeled data. This finding enables us to accurately crawl the new academic website content without having to label a new dataset for re-training the classifier.\u2022 Inspired by the success of co-training on this problem, we investigate loss functions that capture the disparity between the classifiers' predictions on the two views afforded by co-training. We design an iterative algorithm based on the Mini-batch Gradient Descent technique for minimizing this loss and learning a conforming pair of predictors with the two views.\u2022 Finally, we show that minimizing our proposed loss function on unlabeled data closely corresponds to the effect demonstrated by co-training techniques. We posit that this loss can, therefore, be used as a measure, for tracking the progress of co-training schemes even in the absence of a validation dataset.Although in this paper we focus on the design of accurate approaches for researcher homepage classification, our objective is to integrate this classification component in the context of focused crawling, to improve retrieval and indexing of scientific publications in digital libraries such as CiteSeer and ArnetMiner. In these usage environments, since maintaining up-to-date collections of research literature is of primary importance, having an accurate list of homepage URLs for frequent, periodic tracking is both feasible and scalable, compared to examining the entire content at academic websites each time. The rest of this paper is organized as follows: We briefly summarize closely related work in Section 2. Researcher homepage classification is discussed in Section 3. We elaborate on details of our co-training experiments and learning conforming predictor pairs in Section 4. Experimental setup, datasets and results are discussed in Section 5, followed by a summary and future extensions to our work.\nRELATED WORKResearcher homepage classification is a well-studied webpage classification problem in context of digital libraries such as CiteSeer  and ArnetMiner . Typically, contentbased term features and HTML structure-based features are used for classifying webpages . We propose the use of URL features as additional evidence for homepage identification. A smaller set (compared to ours) of URL-based features (presence of part of the name, presence of the character '\u223c', etc.), was used in isolating homepages among the search engine results for researcher name queries by Tang, et  al [40]. URL-based features are widely used in tasks related to the Web. For example, URL strings were used for extracting rules to solve the webpage de-duplication problem . Shih and Karger [38]  used URL features, the visual placements of links in the referring pages to a URL for improving applications such as ad-blocking and recommendation, whereas a preliminary study by Kan and Thi [20]  illustrates the use of URLs in performing fast webpage classification.The problem of collecting a high-quality researcher homepage collection was studied for Japanese websites by Wang, et al. using on-page and anchor text features . Tang, et al. studied homepage acquisition from search engine results using researcher names as queries . In contrast, we seek to apply focused crawling using a seed list of academic websites (where researcher homepages are typically hosted) to acquire such a collection. Focused crawling first proposed by Bra, et al. is a rich area of research on the Web . Chakrabarti, et al. [7]  present a discussion on the main components involved in building a focused crawler. Although focused crawling is our motivating application, this paper deals with the classifier component of the crawler and not with the crawler itself.We show that the focused crawling scenario presents novel challenges in using a pre-trained homepage classifier in identifying relevant pages. Specifically, the classifier needs to be attuned to the changing types of pages on the Web. Cotraining is proposed as a solution for addressing this challenge for homepage classification. Blum and Mitchell [4] first proposed co-training, an approach for semi-supervised learning when the number of labeled examples available for training is limited, and applied it to webpage classification. This approach requires having two views of features for the instances and has been shown to work well when the two views satisfy certain assumptions on \"sufficiency\" and \"independence\" . Recent research addresses techniques for decomposing the feature set into two views when such a split is not naturally available .Multiview learning (of which co-training is a special case, with two views) is typically addressed by maximizing \"consensus\" or agreement among the different views . Most solutions to multiview learning tend to frame the problem in terms of a global optimization problem and simulta-472 neously learn classifiers for all the underlying views. In some cases, the solutions depend on underlying classification algorithm used . Although our proposed algorithm based on mini-batch gradient descent seeks to maximize consensus as well, our approach is a generic technique assuming only that the underlying classifiers output initial \"parameter vectors\" that are altered using a simple, iterative algorithm.\nFEATURES FOR HOMEPAGE CLASSIFICATIONWebpage or text classification is typically handled using \"bag of words\" approaches. Specifically, the frequently occurring and discerning terms are collected from training data to form a feature dictionary that is used to represent instances as normalized term frequency or TFIDF vectors . Homepage classification was previously studied as a text classification problem using term features . Previous work on the same problem also used other content-based features related to the HTML structure of the page such as the number of images/tables on the page, and the terms commonly found in anchor text of homepages . In this study, we extracted content-based and URL-based features from our training sets. These features and the size of feature sets are summarized in . The term dictionaries contain terms that occur in at least three documents (i.e., webpages) and at least five times in the training set.In addition to term dictionaries, we hypothesize that the URL strings of homepages can provide additional evidence for identifying homepages. Hence, we design novel URLbased features based on surface patterns and presence in WordNet 5 . The URL-based features are explained in the next subsection.  \nURL strings as additional evidenceThe idea of using URL strings in academic homepage identification comes from an error analysis of a crawl obtained with the content-based classifier. Consider some example URLs we encountered in our crawl listed in .With some knowledge in academic browsing, one can confidently guess that the webpages at the URLs (1), (2), and (3) are unlikely to be researcher homepages. Similarly, among the URLs (4), and (5), while the former seems to be a homepage, the latter seems to lead to a course page. The above conjectures are based on the presumption that the URL strings are not \"arbitrary\", but, instead conventions are observed that are indicative of the target content at the URL. For instance, in the previous examples, words such as \"projects\", \"events\", alphanumeric patterns of the terms in the URL indicate that the URLs, (1), (2), (3) and (5) are most possibly not researcher homepages.Treating \"/\" as delimiters, we extract features from the URL string following the domain name of a webpage. The list of all unigrams and bigrams from URL strings that occur more than thrice in the training dataset, comprise the URL-term dictionary. For terms in the URL not present in this dictionary, we look for their presence in WordNet to check if they are common words or proper nouns. WordNet is a large, lexical database of nouns, verbs, adjectives and adverbs for English, organized as a concept graph .In addition, we capture the surface patterns of the URLs including the presence of hyphenated or underscored words, alphanumeric patterns, long words (i.e., words having greater than 30 characters), question marks and the presence of characters such as tilde. These features are designed to filter out the URLs that commonly represent course pages, announcements, calendars and other auto-generated content. For instance, a typical homepage URL string in Computer Science departments has the name of the researcher following the \u223c character after the domain name (e.g., http://people.cs.umass.edu/\u223cmccallum/). This pattern is usually captured by our \"TILDENONDICT\" feature, where mccallum is a non-dictionary term. Partial sets of extracted features are shown along with the URLs listed in .The above sets of features perform very well on the training datasets as shown Section 5. We, therefore, do not study other complicated, problem-specific feature design or feature selection. Instead our focus in this work is to study how these classifiers perform \"in the wild\". We also note here that, a classifier that can make accurate predictions using URL features can be quite beneficial from the perspective of efficiency for a focused crawler. A crawler can potentially bypass examining the content of a page if a confident decision can be made based on the URL string. However, we may not be able to always extract features from the URL strings. For instance, consider the following URLs from our crawls:http://john.blitzer.com/ http://clgiles.ist.psu.edu/ http://ben.adida.net/ In these cases, it is not clear from the URL string that the target content refers to academic homepages. Even if complicated name-extraction based features were designed for the above cases, it is rare to find academic homepages with '.com' and '.net' domain suffixes. Based on the URL alone, we cannot be confident if the target content is an academic homepage or a company/personal homepage. For the second case, 'clgiles' could refer to a machine name. In addition to the above cases, given that feature dictionaries typically comprise features that meet a frequency requirement, we may not be able to extract features for all URLs. In our training datasets (Section 5), we were unable to extract URL features for about 27% of the instances. Therefore, contentbased and URL features complement each other while identifying homepage instances and a focused crawler might be required to use either or both of these sets of features.\nHOMEPAGE CLASSIFICATION USING UNLABELED DATAWe show in our experiments (Section 5) that, although content-based features perform extremely well on the training datasets, they are not very successful on the validation and test sets that were collected from the current-day academic websites. On the other hand, URL features show good performance on both training and validation datasets.  However, as pointed out in the previous section, we may not be able to extract URL features for all instances and it is, therefore, imperative to have an accurate content-based classifier as well. We now address the questions: Can we adapt the contentbased classifier to perform well in the deployment environment with the help of the URL-based classifier? Can the two classifiers \"teach\" each other so as to perform better in the new environment, using the co-training approach? Since the URL and content features provide evidence for classifying a webpage instance independently, intuitively, it appears possible that there are instances that the URL classifier makes mistakes on, which the content-based classifier identifies correctly and vice versa.Blum and Mitchell proposed co-training in context of webpage classification . In their datasets, webpages are representable in terms of two distinct views: using terms on webpages and terms in the anchor text of hyperlinks pointing to these pages. When few labeled examples are available for training, they showed that co-training could be used to obtain predictions on the unlabeled data to enlarge the training set. Blum and Mitchell's experiments and the subsequent experiments by Nigam and Ghani  showed that when a natural split of features is available, co-training that explicitly leverages this split has the potential to outperform classifiers that do not.We study the applicability and extension of co-training for our problem. Although the essential motivation is to make use of the naturally available feature split and enable classifiers to learn from each other, we highlight the following aspects of our setup: Previous studies and benefits from co-training were illustrated on datasets where the unlabeled data is arguably from a similar distribution. That is, the positive and negative instances in the labeled datasets are representative of those in the unlabeled data. This is in contrast to our case, where our positive class is fairly welldefined (homepages), whereas the negative class is described in terms of \"not positive\". More precisely, although our training dataset has examples for the negative class, webpages encountered during the crawls can belong to types not encountered in the labeled data. We present an error analysis in Section 5, that illustrates the \"new\" types of webpages encountered in our crawl, potentially causing the pre-trained content-based classifiers to underperform during crawling.The number of negative instances encountered during our crawls is higher in comparison with the number of positive instances. While this aspect was noticed during our experiments, a previous estimation experiment using markrecapture methods had indicated that academic homepages comprise a minute fraction of the Web . We can expect this imbalance to become more prominent as more examples are sampled over the co-training rounds. In the algorithm studied by Blum and Mitchell, the ratio between the number of positive and negative instances added from the unlabeled data is maintained to be the same as that in the training dataset during each iteration of co-training . We argue that avoiding this constraint is better in our scenario since we want the datasets to be more representative of the changing distribution.Most classification algorithms are sensitive to the number of positive and negative instances available in the training data and are known to learn biased classifiers in case of severe imbalance . We employ the idea of altering the mis-classification costs for the underlying classifiers during each round of co-training to handle this problem. For example, if the training dataset has 10 positive and 100 negative instances, we can set the penalty incurred on making mistakes on a negative instance to be 1 10 th of the penalty incurred on making mistakes on a positive instance. For most implementations of classification algorithms, the misclassification costs can be specified as a parameter during the training process .Our co-training setup is detailed in Algorithm 1. L and U represent the labeled and unlabeled datasets, respectively, available at each iteration. They comprise instances with both the views (content-based and URL-based feature sets). For a round of co-training, we train classifiers, C1 and C2, on the two available views, using misclassification costs, \u03c11 and \u03c12, respectively. Next, \"s\" number of examples are sampled without replacement into S from the unlabeled data and C1 and C2 are used to obtain predictions for these instances. The GetConf identEgs method is a generic placeholder that stands for a function that determines what instances from S are chosen for addition in subsequent rounds of co-training. We use the notation L + 1 to represent the positive instances in the set L1 whereas L 1 1 indicates that the view 1 (or feature set 1) of the examples in L1 is being used.Based on previous studies in co-training , we studied the following strategies for this function:\u2022 AddBoth: In this scheme, we add all examples from S that are labeled by C1 or C2 confidently to the training set for the next round. This approach is similar to selftraining used in semi-supervised learning where confidently predicted unlabeled instances are added to the training set for retraining the classifier in subsequent rounds . However, in contrast with self-training that uses a single view, in AddBoth, confident predictions are obtained from two sources (view 1 and 2) for addition into subsequent rounds.\u2022 AddCross: In this scheme, examples from S, confidently labeled by C1 are added to view 2 for the next 474 round and vice versa. That is, we use the examples confidently labeled by one classifier while training the other classifier in the next round. Cross-addition also seems resilient to handling the possibility of cascaded errors over the iterations. If a classifier makes a confident but incorrect prediction, we would like to avoid feeding this example in the next round to the same classifier, a common problem in self-training [45].\u2022 AddCrossRC: This scheme is similar to AddCross with the additional constraint on the number of positive and negative instances added in each round. This constraint was originally studied by Blum and Mitchell and ensures that the ratio of the number of positive and negative instances added in each round is the same as that in the initial labeled dataset .Algorithm 1 Procedure for Co-trainingCompute \u03c1 1 usingThe co-training algorithm is general and can be applied with any choice of classifiers on the two views. Blum and Mitchell provided a PAC-style analysis of co-training with probabilistic classifiers and showed that co-training works when the assumptions on sufficiency and independence are met. That is, each view should be sufficient to predict the class label, and the two views are independent given the class label. Recent studies have proposed relaxed criteria under which co-training techniques still work . However, in practice, it is tricky to judge if co-training works for a problem and to verify if the assumptions are satisfied . These questions are more relevant in context of recent research in obtaining two views from a single view when two views are not naturally available for applying co-training . With this context, we now discuss our formulation of the effect obtained with co-training, in terms of a loss function. This formulation allows us to track whether the co-training process is beneficial for a given problem, even without the use of a validation dataset.\nLearning Conforming Predictors on Unlabeled DataWe assume that classifiers, C1 and C2 trained on the two views are parameterized in terms of their weight vectors, w1 and w2. Most classification algorithms e.g., Support Vector Machines (SVM) and Maximum Entropy (MaxEnt), output weight vectors capturing the importance of each feature as part of the training process .One can expect co-training to benefit a classification problem if one classifier (say, C1) can \"guide\" the other (C2) on examples that the latter makes mistakes on. This guidance is provided by adding examples confidently labeled by C1 to the subsequent round of training C2. This observation hints at the possibility of directly manipulating C2, based on C1's prediction for an example that C2 is not confident about. This effect can be achieved by optimizing a function that directly captures the mismatch in the predictions of the two classifiers.Elaborating further, given that the concept classes, \"positive\" and \"negative\" are still the same on unlabeled data, if C1 and C2 are accurate, they would make similar predictions on the unlabeled data. This intuition is the basis for \"consensus maximization\" widely adopted in multiview learning, of which co-training is a special case with two views . The mismatch in predictions by C1 and C2 on unlabeled data can be quantified using a loss function. The squared error loss function commonly used in machine learning captures this loss as:The above formulation captures the average squared-difference in predictions from the two views on unlabeled data. w1 and w2 correspond to the parameter vectors corresponding to C1 and C2, respectively, and u refers to an example from U , having two views, u1 and u2. For a given example, u = (u1, u2), the functions, f1 and f2 act on u1 and u2 respectively, and make the predictions from C1 and C2 comparable. These functions could be generic (e.g. a function that outputs the probability that the instance is positive) or classifier-dependent (for e.g. a function that outputs scaled distances from the separating hyperplane in case of Support Vector Machines). Minimizing L corresponds to adjusting the weight vectors, w1 and w2, so that they make similar predictions on U . In contrast with multiview learning methods, where learning the classifiers is folded into a global objective function in sophisticated ways , we adopt a simpler approach that works off the initial parameter vectors and iteratively modifies them in a \"co-training like\" manner. Note that this initialization plays a crucial role in avoiding trivial solutions (such as w1, w2 = 0) that are potentially possible since the loss is optimized only on unlabeled instances. Our proposed technique for obtaining the \"pair of conforming classifiers\" is described in Algorithm 2.In Algorithm 2, we start with the original parameter vectors w1 and w2 from classifiers C1 and C2, respectively, and iteratively adjust these vectors so that the values of f1(w1, u1) and f2(w2, u2) look similar for all u \u2208 U . The input parameter, #oIters, refers to the number of times the inner loop comprising of the two gradient descent steps is executed, where as, the #iIters, and \u03b1 are parameters for the gradient descent algorithm. Overall, the values of #oIters, #iIters, and \u03b1 control the rate of convergence of the algorithm and can be set experimentally. These parameters can be set based on the base classifiers used, noting when the decrease in the objective function value is below a threshold. Adaptive tuning of these parameters by tracking the change in the value of the objective function in every iteration is a subject for future study .In each iteration, we employ mini-batch gradient descent to minimize the loss function, once w.r.t. w1 and next w.r.t. w2. The mini-batch gradient descent algorithm is a hybrid approach often used for large-scale machine learning problems. This approach combines the best of stochastic (on- line) gradient descent and batch gradient descent to obtain fast convergence during optimization by running gradient descent on small batches of randomly selected examples .In our algorithm, in each iteration, a small batch of instances are randomly sampled from the unlabeled data, U and the loss function defined using instances for which w1 makes confident predictions from this sampled set. This loss is minimized using gradient descent to adjust w2. A similar process is then applied for adjusting w1 using confident predictions from w2. In effect, as the algorithm proceeds, we are adjusting the parameters of each classifier so that it makes predictions that are aligned with those of the other classifier's confident predictions. Upon convergence, both w1 and w2 are adjusted so that they make conforming predictions on the unlabeled data.In our experiments, we used the differentiable, logistic sigmoid function for f1 and f2. Typically, classifiers use the parameter vector, w, for computing decision values for each instance. That is, given an instance x, the dot product value, w, x , is used for determining the label assignment for the instance. This value can be 'squashed' to a number between 0 and 1 indicating that the probability that instance has a particular label with the logistic function [3]: P (t) = 1 1 + e \u2212t with dP (t) dt = P (t) \u00b7 (1 \u2212 P (t)) Given, the simple form for the derivative, we can directly use the values of f1 and f2 (that we compute anyway), for computing the gradients in Algorithm 2. Although the effect obtained by Algorithm 2 is similar to that of co-training, the conformity loss directly measures the effect of co-training as it is being applied. In contrast, Algorithm 1 is typically terminated either when no more examples are available or by tracking the performance on a validation dataset.We provide a preliminary, experimental demonstration of the connection between co-training and our proposed algorithm in Section 5. A more detailed analysis, study of other choices for the loss function L and the functions, f1 and f2, are a subject of future work. Nevertheless, quantifying the discrepancy in predictions from the two views and an algorithm to directly address this aspect is an exciting step in understanding when co-training works. We show in Section 5 that our method can be used in lieu of a validation dataset for tracking the performance of co-training.\nEXPERIMENTSWe discuss 3 types of experiments: First, we study the performance of content-based and URL-based features on the training and validation datasets. Second, we show that co-training can successfully address the problem of mismatch in the training and deployment environments for homepage classification. Finally, we show that our proposed algorithm (Algorithm 2), achieves the same effect as co-training. Researcher homepage classification using unlabeled data"}, "2006.wwwconf_conference-2006.32": {"doc_id": "2006.wwwconf_conference-2006.32", "default_text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 287\u2013296 ACM 2006 https://doi.org/10.1145/1135777.1135822 10.1145/1135777.1135822 https://dblp.org/rec/conf/www/GaoLM06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GaoLM06 inproceedings ['Weizheng Gao', 'Hyun Chul Lee', 'Yingbo Miao'] [] WWW 2006.wwwconf_conference-2006.32 1541519829.0 ABSTRACTA collaborative crawler is a group of crawling nodes, in which each crawling node is responsible for a specific portion of the web. We study the problem of collecting geographically-aware pages using collaborative crawling strategies. We first propose several collaborative crawling strategies for the geographically focused crawling, whose goal is to collect web pages about specified geographic locations, by considering features like URL address of page, content of page, extended anchor text of link, and others. Later, we propose various evaluation criteria to qualify the performance of such crawling strategies. Finally, we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling, in which the crawling assignments are determined according to the hash-value computation over URLs. More precisely, features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling. Geographically focused collaborative crawling", "text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 287\u2013296 ACM 2006 https://doi.org/10.1145/1135777.1135822 10.1145/1135777.1135822 https://dblp.org/rec/conf/www/GaoLM06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GaoLM06 inproceedings ['Weizheng Gao', 'Hyun Chul Lee', 'Yingbo Miao'] [] WWW 2006.wwwconf_conference-2006.32 1541519829.0 ABSTRACTA collaborative crawler is a group of crawling nodes, in which each crawling node is responsible for a specific portion of the web. We study the problem of collecting geographically-aware pages using collaborative crawling strategies. We first propose several collaborative crawling strategies for the geographically focused crawling, whose goal is to collect web pages about specified geographic locations, by considering features like URL address of page, content of page, extended anchor text of link, and others. Later, we propose various evaluation criteria to qualify the performance of such crawling strategies. Finally, we experimentally study our crawling strategies by crawling the real web data showing that some of our crawling strategies greatly outperform the simple URL-hash based partition collaborative crawling, in which the crawling assignments are determined according to the hash-value computation over URLs. More precisely, features like URL address of page and extended anchor text of link are shown to yield the best overall performance for the geographically focused crawling. Geographically focused collaborative crawling"}, "2006.wwwconf_conference-2006.195": {"doc_id": "2006.wwwconf_conference-2006.195", "default_text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 1041\u20131042 ACM 2006 https://doi.org/10.1145/1135777.1136005 10.1145/1135777.1136005 https://dblp.org/rec/conf/www/GonzlezMMN06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GonzlezMMN06 inproceedings ['Iv\u00e1n Gonzlez', 'Adam Marcus', 'Daniel N. Meredith', 'Linda A. Nguyen'] [] WWW 2006.wwwconf_conference-2006.195 1541519827.0 ABSTRACTThe web crawler space is often delimited into two general areas: full-web crawling and focused crawling. We present netSifter, a crawler system which integrates features from these two areas to provide an effective mechanism for webscale crawling. netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website. These algorithms score individual web pages to determine the general utility of the overall website. In doing so, netSifter can formulate an indepth opinion of a website (and the entirety of its web pages) with a relative minimum of work. netSifter is then able to bias the future efforts of its crawl towards higher quality websites, and away from the myriad of low quality websites and crawler traps that litter the World Wide Web. Effective web-scale crawling through website analysis", "text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 1041\u20131042 ACM 2006 https://doi.org/10.1145/1135777.1136005 10.1145/1135777.1136005 https://dblp.org/rec/conf/www/GonzlezMMN06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GonzlezMMN06 inproceedings ['Iv\u00e1n Gonzlez', 'Adam Marcus', 'Daniel N. Meredith', 'Linda A. Nguyen'] [] WWW 2006.wwwconf_conference-2006.195 1541519827.0 ABSTRACTThe web crawler space is often delimited into two general areas: full-web crawling and focused crawling. We present netSifter, a crawler system which integrates features from these two areas to provide an effective mechanism for webscale crawling. netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website. These algorithms score individual web pages to determine the general utility of the overall website. In doing so, netSifter can formulate an indepth opinion of a website (and the entirety of its web pages) with a relative minimum of work. netSifter is then able to bias the future efforts of its crawl towards higher quality websites, and away from the myriad of low quality websites and crawler traps that litter the World Wide Web. Effective web-scale crawling through website analysis"}, "2010.wwwconf_conference-2010.62": {"doc_id": "2010.wwwconf_conference-2010.62", "default_text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 611\u2013620 ACM 2010 https://doi.org/10.1145/1772690.1772753 10.1145/1772690.1772753 https://dblp.org/rec/conf/www/LeiCYKFZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeiCYKFZ10 inproceedings ['Tao Lei', 'Rui Cai', 'Jiang-Ming Yang', 'Yan Ke', 'Xiaodong Fan', 'Lei Zhang'] [] WWW 2010.wwwconf_conference-2010.62 1541519829.0 ABSTRACTDuplicate URLs have brought serious troubles to the whole pipeline of a search engine, from crawling, indexing, to result serving. URL normalization is to transform duplicate URLs to a canonical form using a set of rewrite rules. Nowadays URL normalization has attracted significant attention as it is lightweight and can be flexibly integrated into both the online (e.g. crawling) and the offline (e.g. index compression) parts of a search engine. To deal with a large scale of websites, automatic approaches are highly desired to learn rewrite rules for various kinds of duplicate URLs. In this paper, we rethink the problem of URL normalization from a global perspective and propose a pattern treebased approach, which is remarkably different from existing approaches. Most current approaches learn rewrite rules by iteratively inducing local duplicate pairs to more general forms, and inevitably suffer from noisy training data and are practically inefficient. Given a training set of URLs partitioned into duplicate clusters for a targeted website, we develop a simple yet efficient algorithm to automatically construct a URL pattern tree. With the pattern tree, the statistical information from all the training samples is leveraged to make the learning process more robust and reliable. The learning process is also accelerated as rules are directly summarized based on pattern tree nodes. In addition, from an engineering perspective, the pattern tree helps select deployable rules by removing conflicts and redundancies. An evaluation on more than 70 million duplicate URLs from 200 websites showed that the proposed approach achieves very promising performance, in terms of both de-duping effectiveness and computational efficiency. A pattern tree-based approach to learning URL normalization rules", "text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 611\u2013620 ACM 2010 https://doi.org/10.1145/1772690.1772753 10.1145/1772690.1772753 https://dblp.org/rec/conf/www/LeiCYKFZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeiCYKFZ10 inproceedings ['Tao Lei', 'Rui Cai', 'Jiang-Ming Yang', 'Yan Ke', 'Xiaodong Fan', 'Lei Zhang'] [] WWW 2010.wwwconf_conference-2010.62 1541519829.0 ABSTRACTDuplicate URLs have brought serious troubles to the whole pipeline of a search engine, from crawling, indexing, to result serving. URL normalization is to transform duplicate URLs to a canonical form using a set of rewrite rules. Nowadays URL normalization has attracted significant attention as it is lightweight and can be flexibly integrated into both the online (e.g. crawling) and the offline (e.g. index compression) parts of a search engine. To deal with a large scale of websites, automatic approaches are highly desired to learn rewrite rules for various kinds of duplicate URLs. In this paper, we rethink the problem of URL normalization from a global perspective and propose a pattern treebased approach, which is remarkably different from existing approaches. Most current approaches learn rewrite rules by iteratively inducing local duplicate pairs to more general forms, and inevitably suffer from noisy training data and are practically inefficient. Given a training set of URLs partitioned into duplicate clusters for a targeted website, we develop a simple yet efficient algorithm to automatically construct a URL pattern tree. With the pattern tree, the statistical information from all the training samples is leveraged to make the learning process more robust and reliable. The learning process is also accelerated as rules are directly summarized based on pattern tree nodes. In addition, from an engineering perspective, the pattern tree helps select deployable rules by removing conflicts and redundancies. An evaluation on more than 70 million duplicate URLs from 200 websites showed that the proposed approach achieves very promising performance, in terms of both de-duping effectiveness and computational efficiency. A pattern tree-based approach to learning URL normalization rules"}, "2003.wwwconf_conference-2003.3": {"doc_id": "2003.wwwconf_conference-2003.3", "default_text": "DBLP:conf/www/2003 Proceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest, Hungary, May 20-24, 2003 19\u201328 ACM 2003 https://doi.org/10.1145/775152.775156 10.1145/775152.775156 https://dblp.org/rec/conf/www/LempelM03.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LempelM03 inproceedings ['Ronny Lempel', 'Shlomo Moran'] [] WWW 2003.wwwconf_conference-2003.3 1541519829.0 ABSTRACTWe study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53. Predictive caching and prefetching of query results in search engines", "text": "DBLP:conf/www/2003 Proceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest, Hungary, May 20-24, 2003 19\u201328 ACM 2003 https://doi.org/10.1145/775152.775156 10.1145/775152.775156 https://dblp.org/rec/conf/www/LempelM03.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LempelM03 inproceedings ['Ronny Lempel', 'Shlomo Moran'] [] WWW 2003.wwwconf_conference-2003.3 1541519829.0 ABSTRACTWe study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53. Predictive caching and prefetching of query results in search engines"}, "2005.wwwconf_conference-2005.28": {"doc_id": "2005.wwwconf_conference-2005.28", "default_text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 257\u2013266 ACM 2005 https://doi.org/10.1145/1060745.1060785 10.1145/1060745.1060785 https://dblp.org/rec/conf/www/LongS05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LongS05 inproceedings ['Xiaohui Long', 'Torsten Suel'] [] WWW 2005.wwwconf_conference-2005.28 1541519829.0 ABSTRACTLarge web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level.We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance. Three-level caching for efficient query processing in large Web search engines", "text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 257\u2013266 ACM 2005 https://doi.org/10.1145/1060745.1060785 10.1145/1060745.1060785 https://dblp.org/rec/conf/www/LongS05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LongS05 inproceedings ['Xiaohui Long', 'Torsten Suel'] [] WWW 2005.wwwconf_conference-2005.28 1541519829.0 ABSTRACTLarge web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level.We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance. Three-level caching for efficient query processing in large Web search engines"}, "2005.wwwconf_conference-2005.38": {"doc_id": "2005.wwwconf_conference-2005.38", "default_text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 342\u2013351 ACM 2005 https://doi.org/10.1145/1060745.1060797 10.1145/1060745.1060797 https://dblp.org/rec/conf/www/LiuHC05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LiuHC05 inproceedings ['Bing Liu', 'Minqing Hu', 'Junsheng Cheng'] [] WWW 2005.wwwconf_conference-2005.38 1543311638.0 ABSTRACTThe Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly. Opinion observer: analyzing and comparing opinions on the Web", "text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 342\u2013351 ACM 2005 https://doi.org/10.1145/1060745.1060797 10.1145/1060745.1060797 https://dblp.org/rec/conf/www/LiuHC05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LiuHC05 inproceedings ['Bing Liu', 'Minqing Hu', 'Junsheng Cheng'] [] WWW 2005.wwwconf_conference-2005.38 1543311638.0 ABSTRACTThe Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly. Opinion observer: analyzing and comparing opinions on the Web"}, "2016.wwwconf_conference-2016.11": {"doc_id": "2016.wwwconf_conference-2016.11", "default_text": "DBLP:conf/www/2016 Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016 85\u201397 ACM 2016 https://doi.org/10.1145/2872427.2882976 10.1145/2872427.2882976 https://dblp.org/rec/conf/www/LeeH16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeeH16 inproceedings ['Dokyun Lee', 'Kartik Hosanagar'] [] WWW 2016.wwwconf_conference-2016.11 1541519828.0 ABSTRACTWe investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase. When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance", "text": "DBLP:conf/www/2016 Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016 85\u201397 ACM 2016 https://doi.org/10.1145/2872427.2882976 10.1145/2872427.2882976 https://dblp.org/rec/conf/www/LeeH16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeeH16 inproceedings ['Dokyun Lee', 'Kartik Hosanagar'] [] WWW 2016.wwwconf_conference-2016.11 1541519828.0 ABSTRACTWe investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase. When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance"}, "2016.tist_journal-ir0anthology0volumeA8A1.9": {"doc_id": "2016.tist_journal-ir0anthology0volumeA8A1.9", "default_text": "ACM Trans. Intell. Syst. Technol. 8 1 10:1\u201310:21 2016 https://doi.org/10.1145/2901299 10.1145/2901299 https://dblp.org/rec/journals/tist/ChengYKL16.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tist/ChengYKL16 article 2016 Volume 8 Issue 1 ['Chen Cheng', 'Haiqin Yang', 'Irwin King', 'Michael R. Lyu'] [] TIST 2016.tist_journal-ir0anthology0volumeA8A1.9 1589205213.0 Location-based social networks (LBSNs), such as Gowalla, Facebook, Foursquare, Brightkite, and so on, have attracted millions of users to share their social friendship and their locations via check-ins in the past few years. Plenty of valuable information is accumulated based on the check-in behaviors, which makes it possible to learn users' moving patterns as well as their preferences. In LBSNs, point-of-interest (POI) recommendation is one of the most significant tasks because it can help targeted users explore their surroundings as well as help third-party developers provide personalized services. Matrix factorization is a promising method for this task because it can capture users' preferences to locations and is widely adopted in traditional recommender systems such as movie recommendation. However, the sparsity of the check-in data makes it difficult to capture users' preferences accurately. Geographical influence can help alleviate this problem and have a large impact on the final recommendation result. By studying users' moving patterns, we find that users tend to check in around several centers and different users have different numbers of centers. Based on this, we propose a Multi-center Gaussian Model (MGM) to capture this pattern via modeling the probability of a user's check-in on a location. Moreover, users are usually more interested in the top 20 or even top 10 recommended POIs, which makes personalized ranking important in this task. From previous work, directly optimizing for pairwise ranking like Bayesian Personalized Ranking (BPR) achieves better performance in the top-k recommendation than directly using matrix matrix factorization that aims to minimize the point-wise rating error. To consider users' preferences, geographical influence and personalized ranking, we propose a unified POI recommendation framework, which unifies all of them together. Specifically, we first fuse MGM with matrix factorization methods and further with BPR using two different approaches. We conduct experiments on Gowalla and Foursquare datasets, which are two large-scale real-world LBSN datasets publicly available online. The results on both datasets show that our unified POI recommendation framework can produce better performance. A Unified Point-of-Interest Recommendation Framework in Location-Based Social Networks", "text": "ACM Trans. Intell. Syst. Technol. 8 1 10:1\u201310:21 2016 https://doi.org/10.1145/2901299 10.1145/2901299 https://dblp.org/rec/journals/tist/ChengYKL16.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tist/ChengYKL16 article 2016 Volume 8 Issue 1 ['Chen Cheng', 'Haiqin Yang', 'Irwin King', 'Michael R. Lyu'] [] TIST 2016.tist_journal-ir0anthology0volumeA8A1.9 1589205213.0 Location-based social networks (LBSNs), such as Gowalla, Facebook, Foursquare, Brightkite, and so on, have attracted millions of users to share their social friendship and their locations via check-ins in the past few years. Plenty of valuable information is accumulated based on the check-in behaviors, which makes it possible to learn users' moving patterns as well as their preferences. In LBSNs, point-of-interest (POI) recommendation is one of the most significant tasks because it can help targeted users explore their surroundings as well as help third-party developers provide personalized services. Matrix factorization is a promising method for this task because it can capture users' preferences to locations and is widely adopted in traditional recommender systems such as movie recommendation. However, the sparsity of the check-in data makes it difficult to capture users' preferences accurately. Geographical influence can help alleviate this problem and have a large impact on the final recommendation result. By studying users' moving patterns, we find that users tend to check in around several centers and different users have different numbers of centers. Based on this, we propose a Multi-center Gaussian Model (MGM) to capture this pattern via modeling the probability of a user's check-in on a location. Moreover, users are usually more interested in the top 20 or even top 10 recommended POIs, which makes personalized ranking important in this task. From previous work, directly optimizing for pairwise ranking like Bayesian Personalized Ranking (BPR) achieves better performance in the top-k recommendation than directly using matrix matrix factorization that aims to minimize the point-wise rating error. To consider users' preferences, geographical influence and personalized ranking, we propose a unified POI recommendation framework, which unifies all of them together. Specifically, we first fuse MGM with matrix factorization methods and further with BPR using two different approaches. We conduct experiments on Gowalla and Foursquare datasets, which are two large-scale real-world LBSN datasets publicly available online. The results on both datasets show that our unified POI recommendation framework can produce better performance. A Unified Point-of-Interest Recommendation Framework in Location-Based Social Networks"}, "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1": {"doc_id": "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1", "default_text": "World Wide Web 9 4 369\u2013395 2006 https://doi.org/10.1007/s11280-006-0221-0 10.1007/s11280-006-0221-0 https://dblp.org/rec/journals/www/LongS06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LongS06 article 2006 Volume 9 Issue 4 ['Xiaohui Long', 'Torsten Suel'] [] WWWJ 2006.wwwjournals_journal-ir0anthology0volumeA9A4.1 1495232734.0 Abstract Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395 Three-Level Caching for Efficient Query Processing in Large Web Search Engines", "text": "World Wide Web 9 4 369\u2013395 2006 https://doi.org/10.1007/s11280-006-0221-0 10.1007/s11280-006-0221-0 https://dblp.org/rec/journals/www/LongS06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LongS06 article 2006 Volume 9 Issue 4 ['Xiaohui Long', 'Torsten Suel'] [] WWWJ 2006.wwwjournals_journal-ir0anthology0volumeA9A4.1 1495232734.0 Abstract Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395 Three-Level Caching for Efficient Query Processing in Large Web Search Engines"}, "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2": {"doc_id": "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2", "default_text": "World Wide Web 15 3 285\u2013323 2012 https://doi.org/10.1007/s11280-011-0134-4 10.1007/s11280-011-0134-4 https://dblp.org/rec/journals/www/SensoyY12.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SensoyY12 article 2012 Volume 15 Issue 3 ['Murat Sensoy', 'Pinar Yolum'] [] WWWJ 2012.wwwjournals_journal-ir0anthology0volumeA15A3.2 1496999016.0 AbstractThe Web is becoming a global market place, where the same services and products are offered by different providers. When obtaining a service, consumers have to select one provider among many alternatives to receive a service or buy a product. In real life, when obtaining a service, many consumers depend on the user reviews. User reviews-presumably written by other consumers-provide details on the consumers' experiences and thus are more informative than ratings. The down side is that such user reviews are written in natural language, making it extremely difficult to be interpreted by computers. Therefore, current technologies do not allow automation of user reviews and require too much human effort for tasks such as writing and reading reviews for the providers, aggregating existing information, and finally choosing among the possible candidates. In this paper, we represent consumers' reviews as machine processable structures using ontologies and develop a layered multiagent framework to enable consumers to find satisfactory service providers for their needs automatically. The framework can still function successfully when consumers evolve their language and when deceptive reviewers enter the system. We show the flexibility of the framework by employing different algorithms for various tasks and evaluate them for different circumstances. Automating user reviews using ontologies: an agent-based approach", "text": "World Wide Web 15 3 285\u2013323 2012 https://doi.org/10.1007/s11280-011-0134-4 10.1007/s11280-011-0134-4 https://dblp.org/rec/journals/www/SensoyY12.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SensoyY12 article 2012 Volume 15 Issue 3 ['Murat Sensoy', 'Pinar Yolum'] [] WWWJ 2012.wwwjournals_journal-ir0anthology0volumeA15A3.2 1496999016.0 AbstractThe Web is becoming a global market place, where the same services and products are offered by different providers. When obtaining a service, consumers have to select one provider among many alternatives to receive a service or buy a product. In real life, when obtaining a service, many consumers depend on the user reviews. User reviews-presumably written by other consumers-provide details on the consumers' experiences and thus are more informative than ratings. The down side is that such user reviews are written in natural language, making it extremely difficult to be interpreted by computers. Therefore, current technologies do not allow automation of user reviews and require too much human effort for tasks such as writing and reading reviews for the providers, aggregating existing information, and finally choosing among the possible candidates. In this paper, we represent consumers' reviews as machine processable structures using ontologies and develop a layered multiagent framework to enable consumers to find satisfactory service providers for their needs automatically. The framework can still function successfully when consumers evolve their language and when deceptive reviewers enter the system. We show the flexibility of the framework by employing different algorithms for various tasks and evaluate them for different circumstances. Automating user reviews using ontologies: an agent-based approach"}, "2019.wwwjournals_journal-ir0anthology0volumeA22A3.10": {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.10", "default_text": "World Wide Web 22 3 1135\u20131150 2019 https://doi.org/10.1007/s11280-018-0579-9 10.1007/s11280-018-0579-9 https://dblp.org/rec/journals/www/ZhangLZS19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ZhangLZS19 article 2019 Volume 22 Issue 3 ['Zhiyuan Zhang', 'Yun Liu', 'Zhenjiang Zhang', 'Bo Shen'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.10 1576674651.0 Abstract We make plans to provid a point-of-interests (POI) recommendation method for location-based social networks (LBSNs) in the paper. LBSNs contain a special three layers network structure based on relevance of location information in the physical world. The available information of three layers of LBSN makes it possible to mine the internal correlations between different layers and to analze users' preference on locations. We try to model the muti-tag, social and geographical influences separately form three layers of LBSN. We first model the mutli-tag influences via extracting a user-tag matrix from intial uer-POI matrix. Next, we introduce social regularization to model the social influences, Thirdly, we use a normalized function to model the geographical influences. Accordingly, we include multi-tag influences and fuse the social regularization, geographical influence into a famous matrix factorization(MF) framework. Finally, we conduct extensive performance evaluations on the large-scale Yelp datasets. As a result, the fusion framework outperforms other state-of-the-art approaches on recommendation.World Wide Web Fused matrix factorization with multi-tag, social and geographical influences for POI recommendation", "text": "World Wide Web 22 3 1135\u20131150 2019 https://doi.org/10.1007/s11280-018-0579-9 10.1007/s11280-018-0579-9 https://dblp.org/rec/journals/www/ZhangLZS19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/ZhangLZS19 article 2019 Volume 22 Issue 3 ['Zhiyuan Zhang', 'Yun Liu', 'Zhenjiang Zhang', 'Bo Shen'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.10 1576674651.0 Abstract We make plans to provid a point-of-interests (POI) recommendation method for location-based social networks (LBSNs) in the paper. LBSNs contain a special three layers network structure based on relevance of location information in the physical world. The available information of three layers of LBSN makes it possible to mine the internal correlations between different layers and to analze users' preference on locations. We try to model the muti-tag, social and geographical influences separately form three layers of LBSN. We first model the mutli-tag influences via extracting a user-tag matrix from intial uer-POI matrix. Next, we introduce social regularization to model the social influences, Thirdly, we use a normalized function to model the geographical influences. Accordingly, we include multi-tag influences and fuse the social regularization, geographical influence into a famous matrix factorization(MF) framework. Finally, we conduct extensive performance evaluations on the large-scale Yelp datasets. As a result, the fusion framework outperforms other state-of-the-art approaches on recommendation.World Wide Web Fused matrix factorization with multi-tag, social and geographical influences for POI recommendation"}, "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11": {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11", "default_text": "World Wide Web 22 3 1151\u20131173 2019 https://doi.org/10.1007/s11280-018-0599-5 10.1007/s11280-018-0599-5 https://dblp.org/rec/journals/www/LuSGCH19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LuSGCH19 article 2019 Volume 22 Issue 3 ['Yi-Shu Lu', 'Wen-Yueh Shih', 'Hung-Yi Gau', 'Kuan-Chieh Chung', 'Jiun-Long Huang'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.11 1559288933.0 Abstract With the increasing popularity of location-based social networks (LBSNs), users are able to share the Point-of-Interests (POIs) they visited by check-ins. By analyzing the users' historical check-in records, POI recommendation can help users get better visiting experience by recommending POIs which users may be interested in. Although recent successive POI recommendation methods consider geographical influence by measuring the distances among POIs, most of them ignore the influence of the regions where the POIs are located. Therefore, we propose in this paper two models to tackle the problem of successive POI recommendation. First, a feature-based successive POI recommendation method, named UGSE-LR, is proposed to take the influence of regions, named regional influence, into consideration when recommending POIs. UGSE-LR first splits an area into grids for estimating regional influence. Then, UGSE-LR applies Edge-weighted Personalized PageRank (EdgePPR) for modeling the successive transitions among POIs. Finally, UGSE-LR fuses user preference, regional influence and successive transition influence into a unified recommendation framework. In addition, with the aid of Recurrent Neural Network (RNN), we propose a latent-factor based successive POI recommendation method, named PEU-RNN, to integrate the sequential visits of POIs and user preference to recommend POIs. First, PEU-RNN adopts the word embedding technique to transform each POI into a latent vector. Then, RNN is used to recommend the POIs depend on the users' historical check-in records. Experimental results on two real LBSN datasets show that our methods are more accurate than the state-of-the-art successive POI recommendation methods in terms of precision and recall. In addition, experimental results also show that PEU-RNN is suitable for On successive point-of-interest recommendation", "text": "World Wide Web 22 3 1151\u20131173 2019 https://doi.org/10.1007/s11280-018-0599-5 10.1007/s11280-018-0599-5 https://dblp.org/rec/journals/www/LuSGCH19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LuSGCH19 article 2019 Volume 22 Issue 3 ['Yi-Shu Lu', 'Wen-Yueh Shih', 'Hung-Yi Gau', 'Kuan-Chieh Chung', 'Jiun-Long Huang'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.11 1559288933.0 Abstract With the increasing popularity of location-based social networks (LBSNs), users are able to share the Point-of-Interests (POIs) they visited by check-ins. By analyzing the users' historical check-in records, POI recommendation can help users get better visiting experience by recommending POIs which users may be interested in. Although recent successive POI recommendation methods consider geographical influence by measuring the distances among POIs, most of them ignore the influence of the regions where the POIs are located. Therefore, we propose in this paper two models to tackle the problem of successive POI recommendation. First, a feature-based successive POI recommendation method, named UGSE-LR, is proposed to take the influence of regions, named regional influence, into consideration when recommending POIs. UGSE-LR first splits an area into grids for estimating regional influence. Then, UGSE-LR applies Edge-weighted Personalized PageRank (EdgePPR) for modeling the successive transitions among POIs. Finally, UGSE-LR fuses user preference, regional influence and successive transition influence into a unified recommendation framework. In addition, with the aid of Recurrent Neural Network (RNN), we propose a latent-factor based successive POI recommendation method, named PEU-RNN, to integrate the sequential visits of POIs and user preference to recommend POIs. First, PEU-RNN adopts the word embedding technique to transform each POI into a latent vector. Then, RNN is used to recommend the POIs depend on the users' historical check-in records. Experimental results on two real LBSN datasets show that our methods are more accurate than the state-of-the-art successive POI recommendation methods in terms of precision and recall. In addition, experimental results also show that PEU-RNN is suitable for On successive point-of-interest recommendation"}, "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3": {"doc_id": "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3", "default_text": "World Wide Web 17 6 1321\u20131342 2014 https://doi.org/10.1007/s11280-013-0239-z 10.1007/s11280-013-0239-z https://dblp.org/rec/journals/www/SiLQD14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SiLQD14 article 2014 Volume 17 Issue 6 ['Jianfeng Si', 'Qing Li', 'Tieyun Qian', 'Xiaotie Deng'] [] WWWJ 2014.wwwjournals_journal-ir0anthology0volumeA17A6.3 1585294593.0 Abstract Large volume of online review data can reveal consumers' major interests on domain product, which attracts great research interests from the academic community. Most of the existing works focus on the problems of review summarization, aspect identification or opinion mining from an item's point of view such as the quality or popularity of products. Considering the fact that users who generate those review texts draw different attentions to product aspects with respect to their own interests, in this article, we aim to learn K users' interest groups indicated by their review writings. Such K interest groups' identification can facilitate better understanding of major and potential consumers' concerns which are crucial for applications like product improvement on customer-oriented design or diverse marketing strategies. Instead of using a traditional text clustering approach, we treat the groupId/clusterId as a hidden variable and use a permutation-based structural topic model called KMM. Through this model, we infer K interest groups' distribution by discovering not only the frequency of product aspects (Topic Frequency), but also the occurrence Users' interest grouping from online reviews based on topic frequency and order", "text": "World Wide Web 17 6 1321\u20131342 2014 https://doi.org/10.1007/s11280-013-0239-z 10.1007/s11280-013-0239-z https://dblp.org/rec/journals/www/SiLQD14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SiLQD14 article 2014 Volume 17 Issue 6 ['Jianfeng Si', 'Qing Li', 'Tieyun Qian', 'Xiaotie Deng'] [] WWWJ 2014.wwwjournals_journal-ir0anthology0volumeA17A6.3 1585294593.0 Abstract Large volume of online review data can reveal consumers' major interests on domain product, which attracts great research interests from the academic community. Most of the existing works focus on the problems of review summarization, aspect identification or opinion mining from an item's point of view such as the quality or popularity of products. Considering the fact that users who generate those review texts draw different attentions to product aspects with respect to their own interests, in this article, we aim to learn K users' interest groups indicated by their review writings. Such K interest groups' identification can facilitate better understanding of major and potential consumers' concerns which are crucial for applications like product improvement on customer-oriented design or diverse marketing strategies. Instead of using a traditional text clustering approach, we treat the groupId/clusterId as a hidden variable and use a permutation-based structural topic model called KMM. Through this model, we infer K interest groups' distribution by discovering not only the frequency of product aspects (Topic Frequency), but also the occurrence Users' interest grouping from online reviews based on topic frequency and order"}, "2011.tweb_journal-ir0anthology0volumeA5A2.3": {"doc_id": "2011.tweb_journal-ir0anthology0volumeA5A2.3", "default_text": "ACM Trans. Web 5 2 9:1\u20139:25 2011 https://doi.org/10.1145/1961659.1961663 10.1145/1961659.1961663 https://dblp.org/rec/journals/tweb/OzcanAU11.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanAU11 article 2011 Volume 5 Issue 2 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2011.tweb_journal-ir0anthology0volumeA5A2.3 1609262376.0 Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time. Cost-Aware Strategies for Query Result Caching in Web Search Engines", "text": "ACM Trans. Web 5 2 9:1\u20139:25 2011 https://doi.org/10.1145/1961659.1961663 10.1145/1961659.1961663 https://dblp.org/rec/journals/tweb/OzcanAU11.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanAU11 article 2011 Volume 5 Issue 2 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2011.tweb_journal-ir0anthology0volumeA5A2.3 1609262376.0 Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time. Cost-Aware Strategies for Query Result Caching in Web Search Engines"}, "2008.tweb_journal-ir0anthology0volumeA2A4.2": {"doc_id": "2008.tweb_journal-ir0anthology0volumeA2A4.2", "default_text": "ACM Trans. Web 2 4 20:1\u201320:28 2008 https://doi.org/10.1145/1409220.1409223 10.1145/1409220.1409223 https://dblp.org/rec/journals/tweb/Baeza-YatesGJMPS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/Baeza-YatesGJMPS08 article 2008 Volume 2 Issue 4 ['Ricardo Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] TWEB 2008.tweb_journal-ir0anthology0volumeA2A4.2 1619422013.0 In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Design trade-offs for search engine caching", "text": "ACM Trans. Web 2 4 20:1\u201320:28 2008 https://doi.org/10.1145/1409220.1409223 10.1145/1409220.1409223 https://dblp.org/rec/journals/tweb/Baeza-YatesGJMPS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/Baeza-YatesGJMPS08 article 2008 Volume 2 Issue 4 ['Ricardo Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] TWEB 2008.tweb_journal-ir0anthology0volumeA2A4.2 1619422013.0 In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Design trade-offs for search engine caching"}, "2013.tweb_journal-ir0anthology0volumeA8A1.2": {"doc_id": "2013.tweb_journal-ir0anthology0volumeA8A1.2", "default_text": "ACM Trans. Web 8 1 3:1\u20133:22 2013 https://doi.org/10.1145/2536777 10.1145/2536777 https://dblp.org/rec/journals/tweb/OzcanACU13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanACU13 article 2013 Volume 8 Issue 1 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', 'Berkant Barla Cambazoglu', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2013.tweb_journal-ir0anthology0volumeA8A1.2 1609262376.0 Web search engines are known to cache the results of previously issued queries. The stored results typically contain the document summaries and some data that is used to construct the final search result page returned to the user. An alternative strategy is to store in the cache only the result document IDs, which take much less space, allowing results of more queries to be cached. These two strategies lead to an interesting trade-off between the hit rate and the average query response latency. In this work, in order to exploit this trade-off, we propose a hybrid result caching strategy where a dynamic result cache is split into two sections: an HTML cache and a docID cache. Moreover, using a realistic cost model, we evaluate the performance of different result prefetching strategies for the proposed hybrid cache and the baseline HTML-only cache. Finally, we propose a machine learning approach to predict singleton queries, which occur only once in the query stream. We show that when the proposed hybrid result caching strategy is coupled with the singleton query predictor, the hit rate is further improved. Second Chance: A Hybrid Approach for Dynamic Result Caching and Prefetching in Search Engines", "text": "ACM Trans. Web 8 1 3:1\u20133:22 2013 https://doi.org/10.1145/2536777 10.1145/2536777 https://dblp.org/rec/journals/tweb/OzcanACU13.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanACU13 article 2013 Volume 8 Issue 1 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', 'Berkant Barla Cambazoglu', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2013.tweb_journal-ir0anthology0volumeA8A1.2 1609262376.0 Web search engines are known to cache the results of previously issued queries. The stored results typically contain the document summaries and some data that is used to construct the final search result page returned to the user. An alternative strategy is to store in the cache only the result document IDs, which take much less space, allowing results of more queries to be cached. These two strategies lead to an interesting trade-off between the hit rate and the average query response latency. In this work, in order to exploit this trade-off, we propose a hybrid result caching strategy where a dynamic result cache is split into two sections: an HTML cache and a docID cache. Moreover, using a realistic cost model, we evaluate the performance of different result prefetching strategies for the proposed hybrid cache and the baseline HTML-only cache. Finally, we propose a machine learning approach to predict singleton queries, which occur only once in the query stream. We show that when the proposed hybrid result caching strategy is coupled with the singleton query predictor, the hit rate is further improved. Second Chance: A Hybrid Approach for Dynamic Result Caching and Prefetching in Search Engines"}, "2015.ipm_journal-ir0anthology0volumeA51A1.3": {"doc_id": "2015.ipm_journal-ir0anthology0volumeA51A1.3", "default_text": "Inf. Process. Manag. 51 1 58\u201367 2015 https://doi.org/10.1016/j.ipm.2014.08.005 10.1016/j.ipm.2014.08.005 https://dblp.org/rec/journals/ipm/LiZL15.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiZL15 article 2015 Volume 51 Issue 1 ['Shi Li', 'Lina Zhou', 'Yijun Li'] [] IPM 2015.ipm_journal-ir0anthology0volumeA51A1.3 1582287091.0 a b s t r a c tOnline review mining has been used to help manufacturers and service providers improve their products and services, and to provide valuable support for consumer decision making. Product aspect extraction is fundamental to online review mining. This research is aimed to improve the performance of aspect extraction from online consumer reviews. To this end, we augment a frequency-based extraction method with PMI-IR, which utilizes web search in measuring the semantic similarity between aspect candidates and target entities. In addition, we extend RCut, an algorithm originally developed for text classification, to learn the threshold for selecting candidate aspects. Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency-based method for aspect extraction but also generalizes across different product domains and various data sizes. Improving aspect extraction by augmenting a frequency-based method with web-based similarity measures", "text": "Inf. Process. Manag. 51 1 58\u201367 2015 https://doi.org/10.1016/j.ipm.2014.08.005 10.1016/j.ipm.2014.08.005 https://dblp.org/rec/journals/ipm/LiZL15.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiZL15 article 2015 Volume 51 Issue 1 ['Shi Li', 'Lina Zhou', 'Yijun Li'] [] IPM 2015.ipm_journal-ir0anthology0volumeA51A1.3 1582287091.0 a b s t r a c tOnline review mining has been used to help manufacturers and service providers improve their products and services, and to provide valuable support for consumer decision making. Product aspect extraction is fundamental to online review mining. This research is aimed to improve the performance of aspect extraction from online consumer reviews. To this end, we augment a frequency-based extraction method with PMI-IR, which utilizes web search in measuring the semantic similarity between aspect candidates and target entities. In addition, we extend RCut, an algorithm originally developed for text classification, to learn the threshold for selecting candidate aspects. Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency-based method for aspect extraction but also generalizes across different product domains and various data sizes. Improving aspect extraction by augmenting a frequency-based method with web-based similarity measures"}, "2007.ipm_journal-ir0anthology0volumeA43A4.3": {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A4.3", "default_text": "Inf. Process. Manag. 43 4 902\u2013913 2007 https://doi.org/10.1016/j.ipm.2006.08.010 10.1016/j.ipm.2006.08.010 https://dblp.org/rec/journals/ipm/LeeK07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LeeK07 article 2007 Volume 43 Issue 4 ['Kyung-Soon Lee', 'Kyo Kageura'] [] IPM 2007.ipm_journal-ir0anthology0volumeA43A4.3 1582287026.0 AbstractThis paper explores the incorporation of prior knowledge into support vector machines as a means of compensating for a shortage of training data in text categorization. The prior knowledge about transformation invariance is generated by a virtual document method. The method applies a simple transformation to documents, i.e., making virtual documents by combining relevant document pairs for a topic in the training set. The virtual document thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting relevant terms that are not given high importance in individual real documents. Artificially generated documents result in the change in the distribution of training data without the randomization. Experiments with support vector machines based on linear, polynomial and radial-basis function kernels showed the effectiveness on Reuters-21578 set for the topics with a small number of relevant documents. The proposed method achieved 131%, 34%, 12% improvements in micro-averaged F 1 for 25, 46, and 58 topics with less than 10, 30, and 50 relevant documents in learning, respectively. The result analysis indicates that incorporating virtual documents contributes to a steady improvement on the performance. Virtual relevant documents in text categorization with support vector machines", "text": "Inf. Process. Manag. 43 4 902\u2013913 2007 https://doi.org/10.1016/j.ipm.2006.08.010 10.1016/j.ipm.2006.08.010 https://dblp.org/rec/journals/ipm/LeeK07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LeeK07 article 2007 Volume 43 Issue 4 ['Kyung-Soon Lee', 'Kyo Kageura'] [] IPM 2007.ipm_journal-ir0anthology0volumeA43A4.3 1582287026.0 AbstractThis paper explores the incorporation of prior knowledge into support vector machines as a means of compensating for a shortage of training data in text categorization. The prior knowledge about transformation invariance is generated by a virtual document method. The method applies a simple transformation to documents, i.e., making virtual documents by combining relevant document pairs for a topic in the training set. The virtual document thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting relevant terms that are not given high importance in individual real documents. Artificially generated documents result in the change in the distribution of training data without the randomization. Experiments with support vector machines based on linear, polynomial and radial-basis function kernels showed the effectiveness on Reuters-21578 set for the topics with a small number of relevant documents. The proposed method achieved 131%, 34%, 12% improvements in micro-averaged F 1 for 25, 46, and 58 topics with less than 10, 30, and 50 relevant documents in learning, respectively. The result analysis indicates that incorporating virtual documents contributes to a steady improvement on the performance. Virtual relevant documents in text categorization with support vector machines"}, "2017.ipm_journal-ir0anthology0volumeA53A4.5": {"doc_id": "2017.ipm_journal-ir0anthology0volumeA53A4.5", "default_text": "Inf. Process. Manag. 53 4 834\u2013850 2017 https://doi.org/10.1016/j.ipm.2017.02.006 10.1016/j.ipm.2017.02.006 https://dblp.org/rec/journals/ipm/KucukyilmazCAB17.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KucukyilmazCAB17 article 2017 Volume 53 Issue 4 ['Tayfun Kucukyilmaz', 'Berkant Barla Cambazoglu', 'Cevdet Aykanat', 'Ricardo Baeza-Yates'] [] IPM 2017.ipm_journal-ir0anthology0volumeA53A4.5 1603697497.0 a b s t r a c tA commonly used technique for improving search engine performance is result caching. In result caching, precomputed results (e.g., URLs and snippets of best matching pages) of certain queries are stored in a fast-access storage. The future occurrences of a query whose results are already stored in the cache can be directly served by the result cache, eliminating the need to process the query using costly computing resources. Although other performance metrics are possible, the main performance metric for evaluating the success of a result cache is hit rate. In this work, we present a machine learning approach to improve the hit rate of a result cache by facilitating a large number of features extracted from search engine query logs. We then apply the proposed machine learning approach to static, dynamic, and static-dynamic caching. Compared to the previous methods in the literature, the proposed approach improves the hit rate of the result cache up to 0.66%, which corresponds to 9.60% of the potential room for improvement. A machine learning approach for result caching in web search engines", "text": "Inf. Process. Manag. 53 4 834\u2013850 2017 https://doi.org/10.1016/j.ipm.2017.02.006 10.1016/j.ipm.2017.02.006 https://dblp.org/rec/journals/ipm/KucukyilmazCAB17.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KucukyilmazCAB17 article 2017 Volume 53 Issue 4 ['Tayfun Kucukyilmaz', 'Berkant Barla Cambazoglu', 'Cevdet Aykanat', 'Ricardo Baeza-Yates'] [] IPM 2017.ipm_journal-ir0anthology0volumeA53A4.5 1603697497.0 a b s t r a c tA commonly used technique for improving search engine performance is result caching. In result caching, precomputed results (e.g., URLs and snippets of best matching pages) of certain queries are stored in a fast-access storage. The future occurrences of a query whose results are already stored in the cache can be directly served by the result cache, eliminating the need to process the query using costly computing resources. Although other performance metrics are possible, the main performance metric for evaluating the success of a result cache is hit rate. In this work, we present a machine learning approach to improve the hit rate of a result cache by facilitating a large number of features extracted from search engine query logs. We then apply the proposed machine learning approach to static, dynamic, and static-dynamic caching. Compared to the previous methods in the literature, the proposed approach improves the hit rate of the result cache up to 0.66%, which corresponds to 9.60% of the potential room for improvement. A machine learning approach for result caching in web search engines"}, "2006.tois_journal-ir0anthology0volumeA24A1.1": {"doc_id": "2006.tois_journal-ir0anthology0volumeA24A1.1", "default_text": "ACM Trans. Inf. Syst. 24 1 51\u201378 2006 https://doi.org/10.1145/1125857.1125859 10.1145/1125857.1125859 https://dblp.org/rec/journals/tois/FagniPSO06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/FagniPSO06 article 2006 Volume 24 Issue 1 ['Tiziano Fagni', 'Raffaele Perego', 'Fabrizio Silvestri', 'Salvatore Orlando'] [] TOIS 2006.tois_journal-ir0anthology0volumeA24A1.1 1603696857.0 This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users. Boosting the performance of Web search engines: Caching and prefetching query results by exploiting historical usage data", "text": "ACM Trans. Inf. Syst. 24 1 51\u201378 2006 https://doi.org/10.1145/1125857.1125859 10.1145/1125857.1125859 https://dblp.org/rec/journals/tois/FagniPSO06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/FagniPSO06 article 2006 Volume 24 Issue 1 ['Tiziano Fagni', 'Raffaele Perego', 'Fabrizio Silvestri', 'Salvatore Orlando'] [] TOIS 2006.tois_journal-ir0anthology0volumeA24A1.1 1603696857.0 This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users. Boosting the performance of Web search engines: Caching and prefetching query results by exploiting historical usage data"}}};
  </script>
  <script type="text/javascript">
    var allWeightsA = {};
    var allWeightsB = {};
    var mergedWeights = {};
    var COLOR_A = '236, 154, 8';
    var COLOR_B = '121, 196, 121';
    var singleRunView = (data.meta.run2_name === null);

    function markup(text, weights) {
      weights = weights.filter(function (e) {
        return (e[2] > 0 || typeof e[2] === 'string');
      })
      var $result = $('<div></div>');
      if (weights.length === 0) {
        $result.text(text);
      } else {
        $result.append($('<span></span>').text(text.substring(0, weights[0][0])));
        $.each(weights, function (i, weight) {
          if (typeof weight[2] === 'string') {
            var weightColor = weight[2];
          } else {
            var weightColor = 'rgba(255, 237, 140, ' + weight[2].toString() + ')';
          }
          $result.append($('<mark></mark>').text(text.substring(weight[0], weight[1])).css('background', weightColor).attr("run1", weight[3]).attr("run2", weight[4]));
          if (i + 1 < weights.length) {
            $result.append($('<span></span>').text(text.substring(weight[1], weights[i + 1][0])));
          }
        });
        $result.append($('<span></span>').text(text.substring(weights[weights.length - 1][1], text.length)));
      }
      return $result;
    }

    function colorizeWeights(mergedWeights) {
      // deep copu & handle if doesn't exist
      mergedWeights = mergedWeights ? JSON.parse(JSON.stringify(mergedWeights)) : [];
      var results = mergedWeights.map((segment) => {
        if (!("run2" in segment[2]) || segment[2].run2 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')', segment[2].run1, segment[2].run2];
        } else if (!("run" in segment[2]) || segment[2].run1 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')', segment[2].run1, segment[2].run2];
        } else {
          var nil = 'rgba(0, 0, 0, 0)'
          var colorA = 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')';
          var colorB = 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')';
          var overlapColors = 'linear-gradient(' + colorA + ', ' + nil + '), linear-gradient(' + nil + ', ' + colorB + ')'
          return [segment[0], segment[1], overlapColors, segment[2].run1, segment[2].run2];
        }
      })
      return results;
    }
    function generateDocListSingleView(run, container, oneRunWeights) {
      $(container).empty();
      $(container).css("padding-left", "15%").css("padding-right", "15%");
      $.each(run, function (i, doc) {
        oneRunWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text(' ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css("right", '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        var newEl = $('<div></div>')
          .append($('<div class="card"></div>')
            .attr('data-docid', doc.doc_id)
            .attr('run1-rank', doc.rank)
            .append($('<div class="card-header"></div>')
              // .css('padding-' + docIdFloat, '30px')
              .append(doc.rank)
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }
    function generateDocList(run, otherRun, container, docIdFloat, allWeights) {
      $(container).empty();
      $.each(run, function (i, doc) {
        allWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text(' ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css(docIdFloat, '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        // $text.append(' ').append('<a href="#" class="doc-info" role="button">See more</a>');
        var otherRank = null;
        $.each(otherRun, function (i, otherDoc) {
          if (otherDoc.doc_id === doc.doc_id) {
            otherRank = otherDoc.rank;
            return false; // break
          }
        });
        if (otherRank === null) {
          var symbol = '';
          var tip = 'not ranked in other run';
        }
        else if (doc.rank === otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked equally in other run'
        } else if (doc.rank < otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked lower in other run (' + otherRank + ')'
        } else if (doc.rank > otherRank) {
          var symbol = docIdFloat === 'right' ? '' : '';
          var tip = 'ranked higher in other run (' + otherRank + ')'
        }
        var newEl = $('<div></div>')
          // .append($('<span class="other-rank"></span>').text(symbol).css('float', docIdFloat).css('text-align', docIdFloat === 'right' ? 'left' : 'right').attr('title', tip))
          .append($('<div class="card"></div>')
            .attr("run1-rank", docIdFloat === 'right' ? doc.rank : (otherRank === null ? "No" : otherRank))
            .attr("run2-rank", docIdFloat === 'right' ? (otherRank === null ? "No" : otherRank): doc.rank)
            .attr('data-docid', doc.doc_id)
            .append($('<div class="card-header"></div>')
              .css('padding-' + docIdFloat, '30px')
              .append($("<span class='border badge' style='min-width: 50px; font-weight: normal;color: grey;'></span>").html('<span style="font-size: 1.2em;font-weight:bold; color: black;">'+doc.rank +'</span> '+symbol + (otherRank === null ? '': otherRank)).attr('title', tip))
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }

    function selectQuery() {
      var $select = $('#Queries');
      var query_id = $select.val();
      var query = data.queries.filter(query => query.fields.query_id === query_id);
      mergedWeights = query[0].mergedWeights
      var $query = $('#Query');
      $query.empty();
      var $table = $('<table class="fields"></table>').appendTo($query);
      if (query.length > 0) {
        query = query[0];
        $.each(query.fields, function (fname, fvalue) {
          if (fname == "contrast"){
            fvalue = fvalue.name +" (" + fvalue.value.toFixed(3)+")";
            $("#contrast-measure").text("Contrast measure: "+fvalue);
          } else {
            $('<tr></tr>')
            .append($('<th></th>').text(fname))
            .append($('<td></td>').text(fvalue))
            .appendTo($table);
          }
        });
        colors = ["badge-secondary", "badge-info", "badge-warning", "badge-primary"]
        $summary = $("<ul></ul>")
        $.each(query.summary, function(index, data){
          $l=$("<li></li>");
          $.each(data, function(idx, statement){
            $("<span class='badge "+ colors[index] +"'></span>").text(statement).appendTo($l);
          });
          $summary.append($l);
        });
        $("#ranking-summary").empty().append($summary)
        if (singleRunView) {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Value</td>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0]==null? "No" : metric_value[0].toFixed(3)))
            .appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);          
          allWeightsA = {}
          generateDocListSingleView(query.run_1, "#docList", allWeightsA);
        } else {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Run1</td> <td>Run2</td></tr></thead>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0] == null? "No" : metric_value[0].toFixed(3)))
            .append($('<td></td>').text(metric_value[1] == null? "No" : metric_value[1].toFixed(3))).appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);
          allWeightsA = {};
          allWeightsB = {};          
          generateDocList(query.run_1, query.run_2, '#Run1Docs', 'right', allWeightsA);
          generateDocList(query.run_2, query.run_1, '#Run2Docs', 'left', allWeightsB);          
        }
      }
      var extraFields = $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse");
      // Don't show expand/collapse button if there are not fields to expand/collapse
      $('#query-collapse-btn').toggle(extraFields.length > 0);
    }

    function checkThreshold(value, threshold) {
      if (typeof value !== 'undefined') {
        return parseFloat(value) < threshold;
      } else return true;
    }

    function onChangeWeightThreshold() {
      $("#DocumentDetails mark").removeClass("nobackground")
      var run1Threshold = parseFloat($("#run1Threshold").text());
      var run2Threshold = parseFloat($("#run2Threshold").text());
      $("#DocumentDetails mark").each(function () {
        var run1w = $(this).attr("run1");
        var run2w = $(this).attr("run2");
        if (checkThreshold(run1w, run1Threshold) && checkThreshold(run2w, run2Threshold)) {
          $(this).addClass("nobackground");
        } else if (checkThreshold(run1w, run1Threshold)) {
          $(this).css("background", "rgba(" + COLOR_B + "," + run2w + ")");
        } else if (checkThreshold(run2w, run2Threshold)) {
          $(this).css("background", "rgba(" + COLOR_A + "," + run1w + ")");
        } else {
          $(this).css("background", 'linear-gradient(rgba('+ COLOR_A + "," + run1w + '),  rgba( '+ COLOR_B + "," + run2w + '))');
        }
      })
    }

    function onCardEnter() {
      var did = $(this).attr('data-docid');
      $('.card[data-docid="' + did + '"').addClass('highlight');
    }

    function onCardLeave() {
      var did = $(this).attr('data-docid');
      $('.card.highlight').removeClass('highlight');
    }

    function onDocInfoClick() {
      var docid = $(this).closest('[data-docid]').attr('data-docid');
      var doc = data.docs[docid];
      $('<div id="DocumentOverlay"></div>').appendTo(document.body)
      var page = $('<div id="DocumentDetails" class="sticky-top"></div>')
        .append($('<div class="close-overlay">X</div>').click(closeDoc))
        .appendTo(document.body);
      var legendTable = $('<table class="fields"></table>')
        .appendTo(page);
      var run1Rank = $(this).closest('[run1-rank]').attr('run1-rank');
      legendTable.append($('<tr></tr>')
        .append($('<th></th>').text(data.meta.run1_name))
        .append($('<td></td>')
          .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_A + ')'))
        )
        .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run1Rank)))
        .append($('<td></td>').append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdA"></div></form>').attr("title", "slide to change weight threshold")))
        .append($('<td><span id="run1Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
      );
      if (!singleRunView) {
        var run2Rank = $(this).closest('[run2-rank]').attr('run2-rank');
        legendTable.append($('<tr></tr>')
          .append($('<th></th>').text(data.meta.run2_name))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_B + ')'))
          )
          .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run2Rank)))
          .append($('<td></td>')
            .append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdB"></div></form>').attr("title", "slide to change weight threshold"))
          ).append($('<td><span id="run2Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
        );
        legendTable.append($('<tr></tr>')
          .append($('<th>both</th>'))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background', 'linear-gradient(rgb(' + COLOR_A + '), rgb(' + COLOR_B + '))'))
          ));
      }
      var fieldTable = $('<table class="fields"></table>')
        .appendTo(page);
      var weightsA = allWeightsA[docid] || {};
      var weightsB = allWeightsB[docid] || {};
      var mweights = mergedWeights[docid] || {};

      $.each(doc, function (fname, fvalue) {
        if (singleRunView) {
          if (!(fname in weightsA)) {
            mweights[fname] = [];
          } else {
            mweights[fname] = weightsA[fname].map(segment => {
              return [segment[0], segment[1], { "run1": segment[2] }];
            });
          }
        }
        var weights = colorizeWeights(mweights[fname]);
        $('<tr></tr>')
          .append($('<th></th>').text(fname))
          .append($('<td></td>').append(markup(fvalue, weights)))
          .appendTo(fieldTable);
      });

      $("input").change(function () {
        var threshold = $(this).closest("form :input").val();
        if ($(this).attr("id") === "weightThresholdA") {
          $("#run1Threshold").text(threshold);
        } else {
          $("#run2Threshold").text(threshold);
        }
        onChangeWeightThreshold();
      });
      onChangeWeightThreshold();
      return false; // prevent nav
    }

    function closeDoc() {
      $('#DocumentOverlay,#DocumentDetails').remove();
    }
    function ding() {
      console.log("Reaching limits! Alert");
    }

    $(function () {
      if (singleRunView) {
        $("#runName").empty();
        $("#runName").append($('<h6 style="text-align: center;"></h6>').text(data.meta.run1_name));
      } else {
        $('#Run1Name').text(data.meta.run1_name);
        $('#Run2Name').text(data.meta.run2_name);
      }
      var $select = $('#Queries');
      var queryDisplayField = null;
      $.each(data.meta.queryFields, function (i, e) {
        if (e !== 'query_id') {
          queryDisplayField = e;
          return false; // break
        }
      });
      $.each(data.queries, function (_, query) {
        if (!singleRunView){
          $('<option>').attr('value', query['fields']['query_id']).attr("data-tokens", query.fields.query_id + " " + query.fields[queryDisplayField]).attr('data-subtext', query.fields.contrast.name+': '+query.fields.contrast.value.toFixed(3)).text(query.fields[queryDisplayField]).appendTo($select);
        } else {
          $('<option>').attr('value', query['fields']['query_id']).text(query.fields[queryDisplayField]).appendTo($select);
        }
      });
      $select.change(selectQuery).change();
      $(document).on('mouseenter', '.card', onCardEnter);
      $(document).on('mouseleave', '.card', onCardLeave);
      $(document).on('click', '.card', onDocInfoClick);
      $(document).on('click', '#DocumentOverlay', closeDoc);
      $(document).keyup(function (e) {
        if (e.key === "Escape") {
          closeDoc();
        }
        if (e.key === "ArrowLeft" && !$("#Queries").is(":focus")) {
          var prev_val = $("#Queries option:selected").prev().val();
          if (typeof prev_val != "undefined") {
            $select.val(prev_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
        if (e.key === "ArrowRight" && !$("#Queries").is(":focus")) {
          var next_val = $("#Queries option:selected").next().val();
          if (typeof next_val != "undefined") {
            $select.val(next_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
      });
    });
    $(document).ready(function () {
      var $select = $('#Queries');
      if (data.queries.length > 20)
        $select.attr("data-live-search","true")
      $select.selectpicker();
      $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse")
      $(".query_collapse").on("shown.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-contract" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M.172 15.828a.5.5 0 0 0 .707 0l4.096-4.096V14.5a.5.5 0 1 0 1 0v-3.975a.5.5 0 0 0-.5-.5H1.5a.5.5 0 0 0 0 1h2.768L.172 15.121a.5.5 0 0 0 0 .707zM15.828.172a.5.5 0 0 0-.707 0l-4.096 4.096V1.5a.5.5 0 1 0-1 0v3.975a.5.5 0 0 0 .5.5H14.5a.5.5 0 0 0 0-1h-2.768L15.828.879a.5.5 0 0 0 0-.707z"/></svg>';
        $("#query-collapse-btn").html(text);
      })
      $(".query_collapse").on("hidden.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-expand" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z"></svg>';
        $("#query-collapse-btn").html(text);
      })
    })
  </script>
</body>

</html>

