<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <link rel="icon"
    href="https://raw.githubusercontent.com/capreolus-ir/diffir/master/docs/images/icon.png">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/css/bootstrap-select.min.css">
  <title>diffir: IR model comparision</title>
  <style>
    .card {
      margin: 5px !important;
    }

    .highlight {
      background-color: #ffffd3;
    }

    #DocumentOverlay {
      position: fixed;
      top: 0;
      bottom: 0;
      left: 0;
      right: 0;
      background-color: rgba(0, 0, 0, .25);
    }

    #DocumentDetails {
      position: fixed;
      top: 60px;
      left: 10%;
      right: 10%;
      bottom: 60px;
      background-color: white;
      padding: 20px;
      border: 1px solid rgba(0, 0, 0, .125);
      border-radius: 0.25rem;
      box-shadow: 0 0 16px black;
      overflow: auto;
    }

    .close-overlay {
      position: absolute;
      top: 4px;
      right: 4px;
      border-radius: 100%;
      background-color: #111;
      font-size: 17px;
      padding: 9px;
      color: white;
      width: 30px;
      height: 30px;
      text-align: center;
      font-weight: normal;
      line-height: 11px;
      cursor: pointer;
    }

    .docid {
      background-color: rgb(224, 135, 55);
      position: absolute;
      top: 0;
      bottom: 0;
      width: 20px;
      overflow: hidden;
      margin-bottom: 0;
      border-radius: 0;
      font-weight: normal;
      white-space: nowrap;
    }

    .docid-value {
      transform: rotate(90deg);
      font-size: 0.7em;
      padding-left: 8px;
    }

    .fields th {
      vertical-align: top;
      text-align: right;
      padding-right: 12px;
      color: #999;
      font-weight: normal;
    }

    #query-container {
      max-width: 600px;
      border: 1px solid #999;
      border-radius: 0.25rem;
      margin: 20px auto;
      padding: 10px;
    }

    .other-rank {
      font-size: 1.2em;
      display: inline-block;
      width: 20px;
      margin-top: 46px;
      margin-left: 3px;
      margin-right: 3px;
      cursor: help;
    }

    mark {
      padding: 0;
      font-weight: bold;
    }

    .snippet {
      font-size: 0.9em;
      line-height: 1.2;
    }

    .elip {
      text-align: center;
      margin: 16px;
      color: gray;
    }

    .doc-info {
      white-space: nowrap;
    }

    .card-header {
      min-height: 128px;
      cursor: pointer;
    }

    .swatch {
      display: inline-block;
      width: 16px;
      height: 16px;
      vertical-align: middle;
    }

    .form-group {
      width: 150px;
      height: 20px;
      padding-left: 10px;
      padding-top: 10px;
    }

    .nobackground {
      background: transparent !important;
      font-weight: normal;
    }
    .styled-table {
      margin-left: 0px;
      margin-top: 10px;
      border-collapse: collapse;    
      font-size: 0.9em;
      /* font-family: sans-serif;       */
      min-width: 350px;      
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    }
    .styled-table thead tr {
      background-color: #17a2b8;
      color: #ffffff;
      text-align: left;
    }    
    /* .styled-table th, */
    .styled-table td {
      /* padding: 12px 15px; */
      text-align: center;
    }    
    .styled-table tbody tr {
      color: #ffffff;
    }
    /*
    .styled-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3;
    } */

    .styled-table tbody tr:last-of-type {
      border-bottom: 2px solid #009879;
    }

    #ranking-summary ul li span {
      margin-right: 5px;
    }    
  </style>
</head>

<body>
  <header>
    <div class="collapse bg-dark" id="navbarHeader">
      <div class="container">
        <div class="row">
          <div class="col-sm-6 col-md-6 py-4">
            <h6 class="text-white">Summary</h6>
            <p class="text-white" id="ranking-summary"></p>
          </div>
          <div class="col-sm-5 col-md-5 py-4"> 
            <h6 class="text-white">Ranking statistics</h6>                                   
            <ul>
              <li class="text-white"><span id="contrast-measure"></span></li>
              <li class="text-white"> <span>Relevance metrics</span> <div id="metrics"></div></li>
            </ul>                        
          </div>
        </div>
      </div>
    </div>
    <div class="navbar navbar-dark bg-dark box-shadow navbar-fixed-top">
      <div class="container d-flex justify-content-between">
        <a href="#" class="navbar-brand d-flex align-items-center">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-search"
            viewBox="0 0 16 16">
            <path
              d="M11.742 10.344a6.5 6.5 0 1 0-1.397 1.398h-.001c.03.04.062.078.098.115l3.85 3.85a1 1 0 0 0 1.415-1.414l-3.85-3.85a1.007 1.007 0 0 0-.115-.1zM12 6.5a5.5 5.5 0 1 1-11 0 5.5 5.5 0 0 1 11 0z" />
          </svg>
          <strong style="padding-left:  5px;">DiffIR</strong>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarHeader"
          aria-controls="navbarHeader" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
      </div>
    </div>
  </header>
  <div class="container">
    <div class="input-group" style="padding-top: 50px; padding-bottom: 10px; background-color: white;">
      <select id="Queries" data-width="100%" data-style="border" data-container="body"></select>
    </div>
    <div id="query-container" class="sticky-top" style="background-color: white;">
      <div style="position:relative">
        <button id="query-collapse-btn" style="position: absolute; top: 12px; right: 8px;" type="button"
          class="btn btn-outline-info btn-sm" data-toggle="collapse" data-target=".query_collapse" aria-expanded="false"
          aria-controls="query_collapse">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor"
            class="bi bi-arrows-angle-expand" viewBox="0 0 16 16">
            <path fill-rule="evenodd"
              d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z" />
          </svg>
        </button>
      </div>
      <div id="Query" style="padding-right: 45px;">
      </div>
    </div>
    <div class="row justify-content-center" id="runName">
      <div class="col">
        <h6 id="Run1Name" style="text-align: center;"></h6>
      </div>
      <div class="col">
        <h6 id="Run2Name" style="text-align: center;"></h6>
      </div>
    </div>
    <div class="row" id="docList">
      <div id="Run1Docs" class="col">
      </div>
      <div id="Run2Docs" class="col">
      </div>
    </div>    
  </div>
  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
    integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
    crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap-select@1.13.14/dist/js/bootstrap-select.min.js"></script>
  <script type="text/javascript">
    var data = {"meta": {"run1_name": "/tira-data/output/run.txt", "run2_name": null, "dataset": "iranthology-dnc-limited", "measure": "topk", "qrelDefs": {"0": "Not Relevant", "1": "Relevant"}, "queryFields": ["query_id", "title", "description", "narrative"], "docFields": ["doc_id", "text"], "relevanceColors": {"null": "#888888", "0": "#d54541", "1": "#52b262"}}, "queries": [{"fields": {"query_id": "1", "title": "machine learning for more relevant results", "description": "Which papers describe methods to find more relevant results using machine learning?", "narrative": "Relevant papers describe one or more methods to find more relevant results using machine learning. Papers about just machine learning in IR in general or papers just about finding more relevant results are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [0.8], "P@10": [0.7], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [0.8539316501572937], "nDCG@10": [0.7753366239771086]}, "run_1": [{"doc_id": "2007.sigirconf_conference-2007.39", "score": 17.399095481293905, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[675, 683, 1.0], [807, 815, 1.0], [833, 840, 1.0], [841, 849, 1.0], [1003, 1010, 1.0], [1092, 1099, 1.0], [1440, 1448, 1.0], [1500, 1507, 1.0], [1586, 1593, 1.0], [1703, 1711, 1.0]]}, "snippet": {"field": "default_text", "start": 670, "stop": 870, "weights": [[5, 13, 1.0], [137, 145, 1.0], [163, 170, 1.0], [171, 179, 1.0]]}}, {"doc_id": "2012.sigirconf_conference-2012.5", "score": 17.21152499522403, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[663, 670, 1.0], [671, 679, 1.0], [769, 777, 1.0], [831, 838, 1.0], [839, 847, 1.0], [1058, 1065, 1.0], [1066, 1074, 1.0], [1134, 1142, 1.0], [1535, 1543, 1.0], [1558, 1565, 1.0], [1566, 1574, 1.0]]}, "snippet": {"field": "default_text", "start": 658, "stop": 858, "weights": [[5, 12, 1.0], [13, 21, 1.0], [111, 119, 1.0], [173, 180, 1.0], [181, 189, 1.0]]}}, {"doc_id": "2009.cikm_conference-2009.190", "score": 17.166249368619784, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[562, 569, 1.0], [639, 646, 1.0], [709, 717, 1.0], [925, 932, 1.0], [979, 986, 1.0], [1170, 1177, 1.0]]}, "snippet": {"field": "default_text", "start": 557, "stop": 757, "weights": [[5, 12, 1.0], [82, 89, 1.0], [152, 160, 1.0]]}}, {"doc_id": "2016.cikm_conference-2016.93", "score": 16.68938640125049, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[719, 726, 1.0], [727, 735, 1.0], [1080, 1087, 1.0], [1088, 1096, 1.0], [1219, 1226, 1.0], [1227, 1235, 1.0], [1350, 1358, 1.0], [1359, 1366, 1.0], [1499, 1506, 1.0], [1813, 1821, 1.0], [3758, 3765, 1.0], [3766, 3774, 1.0]]}, "snippet": {"field": "default_text", "start": 1075, "stop": 1275, "weights": [[5, 12, 1.0], [13, 21, 1.0], [144, 151, 1.0], [152, 160, 1.0]]}}, {"doc_id": "2018.cikm_conference-2018.299", "score": 16.534006183903603, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[827, 834, 1.0], [1577, 1584, 1.0]]}, "snippet": {"field": "default_text", "start": 822, "stop": 1022, "weights": [[5, 12, 1.0]]}}, {"doc_id": "2018.wwwconf_conference-2018c.298", "score": 16.51098522173773, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[714, 722, 1.0], [879, 887, 1.0], [967, 974, 1.0], [975, 983, 1.0], [1050, 1058, 1.0], [1066, 1073, 1.0], [1179, 1186, 1.0], [1252, 1260, 1.0]]}, "snippet": {"field": "default_text", "start": 874, "stop": 1074, "weights": [[5, 13, 1.0], [93, 100, 1.0], [101, 109, 1.0], [176, 184, 1.0], [192, 199, 1.0]]}}, {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A4.3", "score": 16.300945932040772, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[732, 740, 1.0], [925, 933, 1.0], [1310, 1318, 1.0], [1465, 1473, 1.0], [1487, 1495, 1.0], [1642, 1650, 1.0]]}, "snippet": {"field": "default_text", "start": 1305, "stop": 1505, "weights": [[5, 13, 1.0], [160, 168, 1.0], [182, 190, 1.0]]}}, {"doc_id": "2010.cikm_conference-2010.163", "score": 16.217080572177608, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[626, 633, 1.0], [771, 778, 1.0], [1181, 1188, 1.0], [1203, 1210, 1.0], [1211, 1219, 1.0], [1284, 1291, 1.0], [1378, 1385, 1.0], [1409, 1417, 1.0]]}, "snippet": {"field": "default_text", "start": 1176, "stop": 1376, "weights": [[5, 12, 1.0], [27, 34, 1.0], [35, 43, 1.0], [108, 115, 1.0]]}}, {"doc_id": "2014.sigirconf_conference-2014.17", "score": 16.137814713691515, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[624, 631, 1.0], [632, 640, 1.0], [985, 993, 1.0], [1168, 1175, 1.0], [1365, 1373, 1.0], [1502, 1510, 1.0], [1525, 1532, 1.0], [1533, 1541, 1.0], [1620, 1628, 1.0], [1805, 1813, 1.0], [1841, 1849, 1.0], [1900, 1907, 1.0], [1925, 1933, 1.0], [2090, 2097, 1.0], [2098, 2106, 1.0]]}, "snippet": {"field": "default_text", "start": 1360, "stop": 1560, "weights": [[5, 13, 1.0], [142, 150, 1.0], [165, 172, 1.0], [173, 181, 1.0]]}}, {"doc_id": "2010.sigirconf_conference-2010.89", "score": 15.973994023982682, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[585, 592, 1.0], [682, 689, 1.0], [782, 790, 1.0], [824, 831, 1.0], [878, 886, 1.0], [949, 956, 1.0], [1331, 1339, 1.0]]}, "snippet": {"field": "default_text", "start": 677, "stop": 877, "weights": [[5, 12, 1.0], [105, 113, 1.0], [147, 154, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "2", "title": "Crawling websites using machine learning", "description": "Papers that describe how to use AI to crawl the context of websites more efficient.", "narrative": "Papers in this topic describe methods and algorithms to use machine learning for crawling. They also contain information on the latest research findings on the topic. Papers about crawling methods without AI are not relevant for this topic.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.3333333333333333], "P@5": [0.2], "P@10": [0.4], "nDCG@1": [0.0], "nDCG@3": [0.2960819109658652], "nDCG@5": [0.21398626473452756], "nDCG@10": [0.37411416077270787]}, "run_1": [{"doc_id": "2006.wwwconf_conference-2006.195", "score": 23.584285872308484, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[570, 578, 1.0], [591, 599, 1.0], [734, 742, 1.0], [1206, 1214, 1.0], [1256, 1264, 1.0], [1335, 1343, 1.0]]}, "snippet": {"field": "default_text", "start": 565, "stop": 765, "weights": [[5, 13, 1.0], [26, 34, 1.0], [169, 177, 1.0]]}}, {"doc_id": "2014.wwwconf_conference-2014c.198", "score": 23.234805224726763, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[506, 514, 1.0], [619, 627, 1.0], [804, 812, 1.0], [1839, 1844, 1.0], [1889, 1894, 1.0], [1897, 1904, 1.0], [1905, 1913, 1.0], [2309, 2317, 1.0], [2563, 2571, 1.0]]}, "snippet": {"field": "default_text", "start": 1834, "stop": 2034, "weights": [[5, 10, 1.0], [55, 60, 1.0], [63, 70, 1.0], [71, 79, 1.0]]}}, {"doc_id": "2010.wwwconf_conference-2010.62", "score": 21.51597863347626, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[596, 604, 1.0], [704, 709, 1.0], [876, 884, 1.0], [983, 991, 1.0], [1755, 1763, 1.0], [1802, 1810, 1.0], [2096, 2104, 1.0], [2280, 2288, 1.0]]}, "snippet": {"field": "default_text", "start": 591, "stop": 791, "weights": [[5, 13, 1.0], [113, 118, 1.0]]}}, {"doc_id": "2010.cikm_conference-2010.192", "score": 21.17483151605503, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[516, 524, 1.0], [700, 708, 1.0], [1077, 1085, 1.0], [1172, 1180, 1.0], [1536, 1541, 1.0], [1648, 1653, 1.0], [1856, 1864, 1.0], [1865, 1870, 1.0]]}, "snippet": {"field": "default_text", "start": 511, "stop": 711, "weights": [[5, 13, 1.0], [189, 197, 1.0]]}}, {"doc_id": "2011.cikm_conference-2011.113", "score": 20.34474446660209, "relevance": 0, "rank": 5, "weights": {"doc_id": [], "default_text": [[901, 908, 1.0], [978, 986, 1.0], [1249, 1257, 1.0], [1594, 1602, 1.0], [1613, 1621, 1.0], [1697, 1705, 1.0], [1805, 1813, 1.0], [2113, 2121, 1.0], [2172, 2180, 1.0]]}, "snippet": {"field": "default_text", "start": 1589, "stop": 1789, "weights": [[5, 13, 1.0], [24, 32, 1.0], [108, 116, 1.0]]}}, {"doc_id": "2013.wwwconf_conference-2013.41", "score": 20.33817583334026, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[667, 675, 1.0], [895, 903, 1.0], [1075, 1083, 1.0], [1267, 1275, 1.0], [1544, 1552, 1.0], [1600, 1608, 1.0], [1643, 1648, 1.0], [3536, 3544, 1.0], [3919, 3927, 1.0], [4181, 4189, 1.0], [4271, 4279, 1.0], [4450, 4458, 1.0], [4472, 4480, 1.0], [4701, 4709, 1.0], [4812, 4820, 1.0], [5255, 5263, 1.0], [5343, 5350, 1.0], [5351, 5359, 1.0], [5849, 5854, 1.0], [5902, 5910, 1.0], [6133, 6141, 1.0], [6174, 6179, 1.0], [6422, 6427, 1.0], [6468, 6476, 1.0], [7235, 7240, 1.0], [7710, 7718, 1.0], [8274, 8282, 1.0], [8675, 8683, 1.0], [8920, 8928, 1.0], [10245, 10253, 1.0], [10270, 10275, 1.0], [10380, 10385, 1.0], [10454, 10462, 1.0], [10463, 10468, 1.0], [10493, 10501, 1.0], [10590, 10598, 1.0], [10795, 10803, 1.0], [10954, 10962, 1.0], [11001, 11006, 1.0], [11342, 11350, 1.0], [11791, 11799, 1.0], [11975, 11983, 1.0], [12446, 12451, 1.0], [12571, 12576, 1.0], [12890, 12895, 1.0], [13770, 13775, 1.0], [16304, 16309, 1.0], [17184, 17191, 1.0], [17700, 17705, 1.0], [17957, 17965, 1.0], [18512, 18517, 1.0], [18974, 18979, 1.0], [20545, 20553, 1.0], [20767, 20772, 1.0], [22422, 22427, 1.0], [23283, 23291, 1.0], [25643, 25651, 1.0], [26848, 26856, 1.0], [26991, 26996, 1.0], [27063, 27070, 1.0], [27071, 27079, 1.0], [27907, 27915, 1.0], [27931, 27939, 1.0], [29520, 29527, 1.0], [29528, 29536, 1.0], [29913, 29918, 1.0], [30016, 30021, 1.0], [30104, 30109, 1.0], [32470, 32475, 1.0]]}, "snippet": {"field": "default_text", "start": 10265, "stop": 10465, "weights": [[5, 10, 1.0], [115, 120, 1.0], [189, 197, 1.0], [198, 203, 1.0]]}}, {"doc_id": "2016.wwwjournals_journal-ir0anthology0volumeA19A1.4", "score": 20.057230822784206, "relevance": 0, "rank": 7, "weights": {"doc_id": [], "default_text": [[452, 460, 1.0], [502, 507, 1.0], [508, 516, 1.0], [1049, 1057, 1.0], [1263, 1271, 1.0], [1361, 1369, 1.0], [1524, 1532, 1.0], [1565, 1573, 1.0], [1787, 1792, 1.0], [1843, 1851, 1.0], [1945, 1953, 1.0], [1968, 1976, 1.0], [2062, 2070, 1.0]]}, "snippet": {"field": "default_text", "start": 1782, "stop": 1982, "weights": [[5, 10, 1.0], [61, 69, 1.0], [163, 171, 1.0], [186, 194, 1.0]]}}, {"doc_id": "2001.tois_journal-ir0anthology0volumeA19A3.3", "score": 19.88277022140512, "relevance": 1, "rank": 8, "weights": {"doc_id": [], "default_text": [[734, 742, 1.0], [925, 933, 1.0], [1040, 1048, 1.0], [1346, 1354, 1.0], [1506, 1514, 1.0], [1664, 1672, 1.0], [1844, 1852, 1.0], [2025, 2033, 1.0], [2137, 2145, 1.0], [2293, 2301, 1.0]]}, "snippet": {"field": "default_text", "start": 729, "stop": 929, "weights": [[5, 13, 1.0], [196, 204, 1.0]]}}, {"doc_id": "2005.tois_journal-ir0anthology0volumeA23A4.2", "score": 19.781190943720105, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[374, 382, 1.0], [696, 701, 1.0], [846, 854, 1.0], [1163, 1170, 1.0], [1446, 1454, 1.0]]}, "snippet": {"field": "default_text", "start": 691, "stop": 891, "weights": [[5, 10, 1.0], [155, 163, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020.16", "score": 19.45051409144448, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[665, 673, 1.0], [888, 893, 1.0], [1664, 1672, 1.0], [1851, 1859, 1.0]]}, "snippet": {"field": "default_text", "start": 1659, "stop": 1859, "weights": [[5, 13, 1.0], [192, 200, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "3", "title": "Recommenders influence on users", "description": "Papers that describe the change in user behaviour because of recommenders?", "narrative": "Relevant papers describe how users are affected by recommenders, papers about the recommenders from a technological point of view are not relevant", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.0], "P@5": [0.0], "P@10": [0.1], "nDCG@1": [0.0], "nDCG@3": [0.0], "nDCG@5": [0.0], "nDCG@10": [0.07336392209936005]}, "run_1": [{"doc_id": "2011.sigirconf_conference-2011.35", "score": 21.53654936762043, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[739, 748, 1.0], [766, 775, 1.0], [906, 915, 1.0], [972, 981, 1.0], [1102, 1111, 1.0], [1301, 1310, 1.0], [1449, 1458, 1.0], [1476, 1485, 1.0], [1827, 1836, 1.0]]}, "snippet": {"field": "default_text", "start": 734, "stop": 934, "weights": [[5, 14, 1.0], [32, 41, 1.0], [172, 181, 1.0]]}}, {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11", "score": 20.851722515963075, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[503, 508, 1.0], [601, 606, 1.0], [665, 670, 1.0], [729, 734, 1.0], [833, 842, 1.0], [906, 915, 1.0], [1162, 1171, 1.0], [1199, 1208, 1.0], [1317, 1326, 1.0], [1497, 1506, 1.0], [1533, 1542, 1.0], [1962, 1967, 1.0]]}, "snippet": {"field": "default_text", "start": 498, "stop": 698, "weights": [[5, 10, 1.0], [103, 108, 1.0], [167, 172, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020c.114", "score": 20.80636121479523, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[457, 466, 1.0], [547, 552, 1.0], [589, 598, 1.0], [824, 829, 1.0], [875, 884, 1.0], [1126, 1135, 1.0], [1289, 1298, 1.0]]}, "snippet": {"field": "default_text", "start": 452, "stop": 652, "weights": [[5, 14, 1.0], [95, 100, 1.0], [137, 146, 1.0]]}}, {"doc_id": "2018.sigirconf_conference-2018.21", "score": 20.760350565623465, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[683, 692, 1.0], [699, 704, 1.0], [929, 934, 1.0], [970, 979, 1.0], [1028, 1033, 1.0], [1196, 1205, 1.0], [1293, 1302, 1.0], [1539, 1544, 1.0], [1620, 1629, 1.0], [1636, 1641, 1.0], [1798, 1803, 1.0], [1917, 1926, 1.0], [2113, 2122, 1.0], [2129, 2134, 1.0]]}, "snippet": {"field": "default_text", "start": 924, "stop": 1124, "weights": [[5, 10, 1.0], [46, 55, 1.0], [104, 109, 1.0]]}}, {"doc_id": "2018.cikm_conference-2018.98", "score": 20.684286645616066, "relevance": 0, "rank": 5, "weights": {"doc_id": [], "default_text": [[899, 908, 1.0], [912, 917, 1.0], [1033, 1038, 1.0], [1141, 1150, 1.0], [1154, 1159, 1.0], [1254, 1259, 1.0], [1455, 1464, 1.0], [1468, 1473, 1.0], [1507, 1516, 1.0], [1540, 1549, 1.0], [2042, 2047, 1.0], [2080, 2089, 1.0], [2106, 2115, 1.0]]}, "snippet": {"field": "default_text", "start": 1450, "stop": 1650, "weights": [[5, 14, 1.0], [18, 23, 1.0], [57, 66, 1.0], [90, 99, 1.0]]}}, {"doc_id": "2014.cikm_conference-2014.67", "score": 20.36498241049288, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[756, 761, 1.0], [818, 823, 1.0], [862, 867, 1.0], [951, 956, 1.0], [1014, 1019, 1.0], [1439, 1448, 1.0], [1462, 1471, 1.0]]}, "snippet": {"field": "default_text", "start": 751, "stop": 951, "weights": [[5, 10, 1.0], [67, 72, 1.0], [111, 116, 1.0]]}}, {"doc_id": "2013.cikm_workshop-2013ueo.2", "score": 20.222705029379696, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[585, 590, 1.0], [777, 786, 1.0], [1023, 1032, 1.0], [1069, 1078, 1.0]]}, "snippet": {"field": "default_text", "start": 580, "stop": 780, "weights": [[5, 10, 1.0], [197, 206, 1.0]]}}, {"doc_id": "2012.sigirconf_conference-2012.69", "score": 19.89571339777118, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[711, 720, 1.0], [978, 987, 1.0], [1043, 1048, 1.0], [1287, 1296, 1.0], [1555, 1564, 1.0], [1761, 1770, 1.0]]}, "snippet": {"field": "default_text", "start": 973, "stop": 1173, "weights": [[5, 14, 1.0], [70, 75, 1.0]]}}, {"doc_id": "2016.cikm_conference-2016.202", "score": 19.79424826045941, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[785, 794, 1.0], [989, 998, 1.0], [1214, 1223, 1.0], [1438, 1447, 1.0]]}, "snippet": {"field": "default_text", "start": 780, "stop": 980, "weights": [[5, 14, 1.0]]}}, {"doc_id": "2012.wsdm_conference-2012.59", "score": 19.5108942230643, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[712, 717, 1.0], [995, 1004, 1.0], [1005, 1010, 1.0], [1350, 1355, 1.0], [1597, 1602, 1.0], [1611, 1620, 1.0], [1945, 1954, 1.0], [1973, 1982, 1.0]]}, "snippet": {"field": "default_text", "start": 990, "stop": 1190, "weights": [[5, 14, 1.0], [15, 20, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "4", "title": "Search engine caching effects", "description": "Papers that describe the effects and/or efficient use of search engine caching in terms of result freshness, query latency and other potential advantages or disadvantages ", "narrative": "Papers in this topic will describe the design trade-off between low latency querying and returning the most recently available results as well as different architectures to create efficient caching systems. Results should not contain any other caching related topics (e.g. hardware or web browsers)", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [1.0], "P@5": [1.0], "P@10": [0.9], "nDCG@1": [1.0], "nDCG@3": [1.0], "nDCG@5": [1.0], "nDCG@10": [0.9305687780632227]}, "run_1": [{"doc_id": "2003.wwwconf_conference-2003.3", "score": 25.066159688027348, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[447, 454, 1.0], [484, 490, 1.0], [508, 514, 1.0], [587, 594, 1.0], [765, 772, 1.0], [773, 779, 1.0], [831, 837, 1.0], [838, 844, 1.0], [919, 925, 1.0], [926, 932, 1.0], [1002, 1009, 1.0], [1129, 1135, 1.0], [1365, 1372, 1.0], [1409, 1415, 1.0]]}, "snippet": {"field": "default_text", "start": 760, "stop": 960, "weights": [[5, 12, 1.0], [13, 19, 1.0], [71, 77, 1.0], [78, 84, 1.0], [159, 165, 1.0], [166, 172, 1.0]]}}, {"doc_id": "2014.tois_journal-ir0anthology0volumeA32A4.5", "score": 24.959239511072063, "relevance": 1, "rank": 2, "weights": {"doc_id": [], "default_text": [[414, 421, 1.0], [454, 460, 1.0], [461, 467, 1.0], [492, 499, 1.0], [515, 521, 1.0], [522, 528, 1.0], [841, 847, 1.0], [1098, 1104, 1.0], [1105, 1111, 1.0], [1169, 1175, 1.0], [1279, 1285, 1.0], [1286, 1292, 1.0], [1422, 1429, 1.0], [1453, 1459, 1.0], [1495, 1501, 1.0], [1502, 1508, 1.0]]}, "snippet": {"field": "default_text", "start": 409, "stop": 609, "weights": [[5, 12, 1.0], [45, 51, 1.0], [52, 58, 1.0], [83, 90, 1.0], [106, 112, 1.0], [113, 119, 1.0]]}}, {"doc_id": "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1", "score": 24.83929256806437, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[379, 385, 1.0], [695, 701, 1.0], [799, 806, 1.0], [913, 920, 1.0], [1122, 1129, 1.0], [1172, 1179, 1.0], [1297, 1304, 1.0], [1457, 1464, 1.0], [1592, 1598, 1.0], [1599, 1605, 1.0], [1726, 1733, 1.0], [1877, 1883, 1.0], [1886, 1892, 1.0], [1893, 1899, 1.0], [1915, 1921, 1.0], [1922, 1928, 1.0], [1965, 1972, 1.0], [2018, 2025, 1.0], [2070, 2076, 1.0]]}, "snippet": {"field": "default_text", "start": 1872, "stop": 2072, "weights": [[5, 11, 1.0], [14, 20, 1.0], [21, 27, 1.0], [43, 49, 1.0], [50, 56, 1.0], [93, 100, 1.0], [146, 153, 1.0], [198, 204, 1.0]]}}, {"doc_id": "2008.tweb_journal-ir0anthology0volumeA2A4.2", "score": 24.83155992028378, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[534, 541, 1.0], [558, 564, 1.0], [648, 655, 1.0], [661, 668, 1.0], [687, 694, 1.0], [781, 788, 1.0], [813, 820, 1.0], [869, 876, 1.0], [930, 937, 1.0], [1197, 1204, 1.0], [1478, 1484, 1.0], [1485, 1491, 1.0], [1492, 1499, 1.0]]}, "snippet": {"field": "default_text", "start": 529, "stop": 729, "weights": [[5, 12, 1.0], [29, 35, 1.0], [119, 126, 1.0], [132, 139, 1.0], [158, 165, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.149", "score": 24.466400930598844, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[589, 595, 1.0], [878, 885, 1.0], [1019, 1025, 1.0], [1026, 1032, 1.0], [1203, 1210, 1.0], [1223, 1230, 1.0], [1448, 1455, 1.0], [1800, 1807, 1.0], [1825, 1832, 1.0], [1849, 1855, 1.0]]}, "snippet": {"field": "default_text", "start": 1014, "stop": 1214, "weights": [[5, 11, 1.0], [12, 18, 1.0], [189, 196, 1.0]]}}, {"doc_id": "2007.sigirconf_conference-2007.26", "score": 24.383222657642953, "relevance": 1, "rank": 6, "weights": {"doc_id": [], "default_text": [[692, 699, 1.0], [716, 722, 1.0], [806, 813, 1.0], [819, 826, 1.0], [845, 852, 1.0], [938, 945, 1.0], [970, 977, 1.0], [1026, 1033, 1.0], [1087, 1094, 1.0], [1351, 1358, 1.0], [1624, 1631, 1.0], [1635, 1641, 1.0]]}, "snippet": {"field": "default_text", "start": 687, "stop": 887, "weights": [[5, 12, 1.0], [29, 35, 1.0], [119, 126, 1.0], [132, 139, 1.0], [158, 165, 1.0]]}}, {"doc_id": "2013.sigirconf_conference-2013.71", "score": 24.227771065275437, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[593, 599, 1.0], [600, 606, 1.0], [631, 638, 1.0], [654, 660, 1.0], [661, 667, 1.0], [980, 986, 1.0], [1237, 1243, 1.0], [1244, 1250, 1.0], [1308, 1314, 1.0], [1416, 1422, 1.0], [1423, 1429, 1.0], [1566, 1573, 1.0], [1596, 1602, 1.0], [1647, 1653, 1.0], [1654, 1660, 1.0]]}, "snippet": {"field": "default_text", "start": 588, "stop": 788, "weights": [[5, 11, 1.0], [12, 18, 1.0], [43, 50, 1.0], [66, 72, 1.0], [73, 79, 1.0]]}}, {"doc_id": "2012.wwwconf_conference-2012.18", "score": 24.14670689607012, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[879, 886, 1.0], [911, 917, 1.0], [918, 924, 1.0], [1051, 1058, 1.0], [1106, 1112, 1.0], [1113, 1119, 1.0], [1161, 1167, 1.0], [1168, 1174, 1.0]]}, "snippet": {"field": "default_text", "start": 906, "stop": 1106, "weights": [[5, 11, 1.0], [12, 18, 1.0], [145, 152, 1.0]]}}, {"doc_id": "2011.tweb_journal-ir0anthology0volumeA5A2.3", "score": 23.981550026008314, "relevance": 1, "rank": 9, "weights": {"doc_id": [], "default_text": [[383, 389, 1.0], [513, 520, 1.0], [684, 691, 1.0], [1096, 1103, 1.0], [1261, 1268, 1.0], [1509, 1516, 1.0], [1524, 1530, 1.0]]}, "snippet": {"field": "default_text", "start": 378, "stop": 578, "weights": [[5, 11, 1.0], [135, 142, 1.0]]}}, {"doc_id": "2011.ecir_conference-2011.12", "score": 23.950895240341236, "relevance": 1, "rank": 10, "weights": {"doc_id": [], "default_text": [[562, 568, 1.0], [660, 666, 1.0], [686, 692, 1.0], [786, 792, 1.0], [813, 819, 1.0], [839, 845, 1.0], [1180, 1187, 1.0], [1230, 1236, 1.0], [1513, 1519, 1.0], [1520, 1526, 1.0], [2176, 2183, 1.0], [2197, 2203, 1.0]]}, "snippet": {"field": "default_text", "start": 655, "stop": 855, "weights": [[5, 11, 1.0], [31, 37, 1.0], [131, 137, 1.0], [158, 164, 1.0], [184, 190, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "5", "title": "Consumer Product reviews", "description": "Papers that describe the effects of product reviews on consumer decisions", "narrative": "Relevant papers would describe the influence that reviews have on individual decisions of the consumer on whether to buy a product or not. Not relevant papers, would contain other studies about reviews, that are not pertaining to human psychology", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [0.0], "P@3": [0.0], "P@5": [0.2], "P@10": [0.1], "nDCG@1": [0.0], "nDCG@3": [0.0], "nDCG@5": [0.13120507751234178], "nDCG@10": [0.08514311764162098]}, "run_1": [{"doc_id": "2011.wwwconf_conference-2011c.86", "score": 30.136770159614155, "relevance": 0, "rank": 1, "weights": {"doc_id": [], "default_text": [[560, 567, 1.0], [662, 670, 1.0], [671, 678, 1.0], [759, 767, 1.0], [768, 775, 1.0], [808, 815, 1.0], [831, 839, 1.0], [942, 950, 1.0], [951, 958, 1.0], [1008, 1016, 1.0], [1017, 1024, 1.0], [1127, 1134, 1.0], [1190, 1197, 1.0], [1301, 1308, 1.0], [1450, 1458, 1.0], [1459, 1466, 1.0]]}, "snippet": {"field": "default_text", "start": 657, "stop": 857, "weights": [[5, 13, 1.0], [14, 21, 1.0], [102, 110, 1.0], [111, 118, 1.0], [151, 158, 1.0], [174, 182, 1.0]]}}, {"doc_id": "2015.sigirconf_conference-2015.41", "score": 30.035404597854818, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[583, 590, 1.0], [591, 598, 1.0], [661, 668, 1.0], [758, 765, 1.0], [822, 829, 1.0], [871, 878, 1.0], [899, 906, 1.0], [923, 930, 1.0], [1009, 1016, 1.0], [1078, 1085, 1.0], [1124, 1131, 1.0], [1157, 1164, 1.0], [1194, 1201, 1.0], [1294, 1302, 1.0], [1388, 1395, 1.0], [1423, 1430, 1.0], [1703, 1710, 1.0]]}, "snippet": {"field": "default_text", "start": 753, "stop": 953, "weights": [[5, 12, 1.0], [69, 76, 1.0], [118, 125, 1.0], [146, 153, 1.0], [170, 177, 1.0]]}}, {"doc_id": "2009.ecir_conference-2009.41", "score": 28.19098321188773, "relevance": 0, "rank": 3, "weights": {"doc_id": [], "default_text": [[568, 575, 1.0], [576, 583, 1.0], [824, 831, 1.0], [961, 969, 1.0], [970, 977, 1.0], [1097, 1104, 1.0], [1186, 1193, 1.0], [1555, 1562, 1.0], [1640, 1647, 1.0], [1648, 1655, 1.0]]}, "snippet": {"field": "default_text", "start": 819, "stop": 1019, "weights": [[5, 12, 1.0], [142, 150, 1.0], [151, 158, 1.0]]}}, {"doc_id": "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2", "score": 27.305501716979588, "relevance": 0, "rank": 4, "weights": {"doc_id": [], "default_text": [[608, 615, 1.0], [691, 698, 1.0], [705, 712, 1.0], [873, 880, 1.0], [1040, 1047, 1.0], [1120, 1127, 1.0], [1272, 1279, 1.0], [1750, 1757, 1.0]]}, "snippet": {"field": "default_text", "start": 603, "stop": 803, "weights": [[5, 12, 1.0], [88, 95, 1.0], [102, 109, 1.0]]}}, {"doc_id": "2016.wwwconf_conference-2016.11", "score": 27.243104491120334, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[478, 485, 1.0], [501, 509, 1.0], [510, 517, 1.0], [867, 874, 1.0], [890, 898, 1.0], [1616, 1623, 1.0], [1795, 1802, 1.0], [1870, 1877, 1.0], [2285, 2292, 1.0], [2308, 2316, 1.0], [2317, 2324, 1.0]]}, "snippet": {"field": "default_text", "start": 473, "stop": 673, "weights": [[5, 12, 1.0], [28, 36, 1.0], [37, 44, 1.0]]}}, {"doc_id": "2005.wwwconf_conference-2005.38", "score": 27.060586379202295, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[497, 505, 1.0], [590, 597, 1.0], [687, 694, 1.0], [801, 809, 1.0], [1042, 1049, 1.0], [1096, 1103, 1.0], [1172, 1179, 1.0], [1295, 1303, 1.0], [1368, 1375, 1.0], [1390, 1397, 1.0], [1482, 1489, 1.0], [1596, 1603, 1.0], [1656, 1663, 1.0]]}, "snippet": {"field": "default_text", "start": 1290, "stop": 1490, "weights": [[5, 13, 1.0], [78, 85, 1.0], [100, 107, 1.0], [192, 199, 1.0]]}}, {"doc_id": "2008.wwwconf_conference-2008.153", "score": 25.58873001417166, "relevance": 0, "rank": 7, "weights": {"doc_id": [], "default_text": [[584, 591, 1.0], [727, 734, 1.0], [847, 854, 1.0], [906, 913, 1.0], [1046, 1053, 1.0], [1086, 1093, 1.0], [1375, 1382, 1.0], [1525, 1532, 1.0], [1640, 1647, 1.0]]}, "snippet": {"field": "default_text", "start": 722, "stop": 922, "weights": [[5, 12, 1.0], [125, 132, 1.0], [184, 191, 1.0]]}}, {"doc_id": "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3", "score": 25.32912526700668, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[482, 489, 1.0], [843, 850, 1.0], [1148, 1155, 1.0], [1503, 1510, 1.0], [1599, 1606, 1.0]]}, "snippet": {"field": "default_text", "start": 1498, "stop": 1698, "weights": [[5, 12, 1.0], [101, 108, 1.0]]}}, {"doc_id": "2018.ipm_journal-ir0anthology0volumeA54A6.5", "score": 24.43521308923151, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[417, 424, 1.0], [455, 462, 1.0], [483, 490, 1.0], [498, 505, 1.0], [542, 549, 1.0], [565, 573, 1.0], [848, 855, 1.0], [1052, 1059, 1.0], [1398, 1405, 1.0], [2126, 2133, 1.0]]}, "snippet": {"field": "default_text", "start": 412, "stop": 612, "weights": [[5, 12, 1.0], [43, 50, 1.0], [71, 78, 1.0], [86, 93, 1.0], [130, 137, 1.0], [153, 161, 1.0]]}}, {"doc_id": "2015.ipm_journal-ir0anthology0volumeA51A1.3", "score": 24.253426977951168, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[526, 534, 1.0], [552, 559, 1.0], [701, 709, 1.0], [710, 717, 1.0], [1085, 1092, 1.0], [1248, 1255, 1.0]]}, "snippet": {"field": "default_text", "start": 521, "stop": 721, "weights": [[5, 13, 1.0], [31, 38, 1.0], [180, 188, 1.0], [189, 196, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}, {"fields": {"query_id": "6", "title": "Limitations machine learning", "description": "Which papers describe the limitations of machine learning?", "narrative": "Relevant papers describe the limitations of machine learning ( e.g. dependence on data quality and quantity, limited ability to handle complex tasks, vulnerability to disturbances and attacks, need for resources and energy). Papers that contains machine learning but not its limitations are not relevant.", "contrast": {"name": "singlerun", "value": 0}}, "metrics": {"P@1": [1.0], "P@3": [0.6666666666666666], "P@5": [0.8], "P@10": [0.5], "nDCG@1": [1.0], "nDCG@3": [0.7039180890341347], "nDCG@5": [0.7860137352654724], "nDCG@10": [0.8021574504628255]}, "run_1": [{"doc_id": "2021.wsdm_conference-2021.78", "score": 17.53758837303082, "relevance": 1, "rank": 1, "weights": {"doc_id": [], "default_text": [[549, 556, 1.0], [557, 565, 1.0], [679, 686, 1.0], [687, 695, 1.0], [1424, 1432, 1.0], [1970, 1978, 1.0]]}, "snippet": {"field": "default_text", "start": 544, "stop": 744, "weights": [[5, 12, 1.0], [13, 21, 1.0], [135, 142, 1.0], [143, 151, 1.0]]}}, {"doc_id": "2021.wsdm_conference-2021.156", "score": 17.52527154521976, "relevance": 0, "rank": 2, "weights": {"doc_id": [], "default_text": [[588, 595, 1.0], [596, 604, 1.0], [625, 633, 1.0], [802, 810, 1.0], [903, 911, 1.0], [1313, 1320, 1.0], [1592, 1599, 1.0], [1736, 1743, 1.0], [1769, 1776, 1.0]]}, "snippet": {"field": "default_text", "start": 583, "stop": 783, "weights": [[5, 12, 1.0], [13, 21, 1.0], [42, 50, 1.0]]}}, {"doc_id": "2017.wsdm_conference-2017.57", "score": 17.317056496655585, "relevance": 1, "rank": 3, "weights": {"doc_id": [], "default_text": [[514, 521, 1.0], [522, 530, 1.0], [585, 592, 1.0], [593, 601, 1.0], [744, 751, 1.0], [752, 760, 1.0], [975, 983, 1.0], [1358, 1366, 1.0], [1441, 1449, 1.0], [1649, 1657, 1.0], [1764, 1771, 1.0], [1772, 1780, 1.0], [1824, 1831, 1.0], [1992, 1999, 1.0], [2000, 2008, 1.0], [2035, 2042, 1.0], [2043, 2051, 1.0]]}, "snippet": {"field": "default_text", "start": 509, "stop": 709, "weights": [[5, 12, 1.0], [13, 21, 1.0], [76, 83, 1.0], [84, 92, 1.0]]}}, {"doc_id": "2020.wwwconf_conference-2020c.108", "score": 17.245100258490382, "relevance": 1, "rank": 4, "weights": {"doc_id": [], "default_text": [[484, 491, 1.0], [492, 500, 1.0], [988, 999, 1.0], [1053, 1060, 1.0], [1061, 1069, 1.0], [1142, 1153, 1.0], [1400, 1408, 1.0], [1483, 1494, 1.0]]}, "snippet": {"field": "default_text", "start": 983, "stop": 1183, "weights": [[5, 16, 1.0], [70, 77, 1.0], [78, 86, 1.0], [159, 170, 1.0]]}}, {"doc_id": "2018.wsdm_conference-2018.3", "score": 17.127889287052408, "relevance": 1, "rank": 5, "weights": {"doc_id": [], "default_text": [[452, 459, 1.0], [460, 468, 1.0], [776, 784, 1.0], [1005, 1012, 1.0], [1013, 1021, 1.0], [1127, 1134, 1.0], [1135, 1143, 1.0]]}, "snippet": {"field": "default_text", "start": 1000, "stop": 1200, "weights": [[5, 12, 1.0], [13, 21, 1.0], [127, 134, 1.0], [135, 143, 1.0]]}}, {"doc_id": "2016.cikm_conference-2016.93", "score": 16.677127965969134, "relevance": 0, "rank": 6, "weights": {"doc_id": [], "default_text": [[719, 726, 1.0], [727, 735, 1.0], [925, 936, 1.0], [1080, 1087, 1.0], [1088, 1096, 1.0], [1219, 1226, 1.0], [1227, 1235, 1.0], [1459, 1470, 1.0], [1499, 1506, 1.0], [1813, 1821, 1.0], [3758, 3765, 1.0], [3766, 3774, 1.0]]}, "snippet": {"field": "default_text", "start": 1075, "stop": 1275, "weights": [[5, 12, 1.0], [13, 21, 1.0], [144, 151, 1.0], [152, 160, 1.0]]}}, {"doc_id": "2007.cikm_conference-2007.3", "score": 16.2826347309991, "relevance": 1, "rank": 7, "weights": {"doc_id": [], "default_text": [[773, 780, 1.0], [781, 789, 1.0], [973, 981, 1.0], [1143, 1154, 1.0], [1370, 1378, 1.0], [1558, 1566, 1.0], [1792, 1799, 1.0], [1800, 1808, 1.0], [2308, 2315, 1.0], [2316, 2324, 1.0], [2501, 2508, 1.0], [2816, 2823, 1.0], [2824, 2832, 1.0], [3206, 3214, 1.0]]}, "snippet": {"field": "default_text", "start": 768, "stop": 968, "weights": [[5, 12, 1.0], [13, 21, 1.0]]}}, {"doc_id": "2009.ipm_journal-ir0anthology0volumeA45A3.2", "score": 15.511067493002134, "relevance": 0, "rank": 8, "weights": {"doc_id": [], "default_text": [[405, 412, 1.0], [413, 421, 1.0], [633, 641, 1.0], [818, 826, 1.0], [1070, 1077, 1.0], [1078, 1086, 1.0], [1249, 1256, 1.0], [1257, 1265, 1.0], [1517, 1525, 1.0], [1652, 1660, 1.0]]}, "snippet": {"field": "default_text", "start": 1065, "stop": 1265, "weights": [[5, 12, 1.0], [13, 21, 1.0], [184, 191, 1.0], [192, 200, 1.0]]}}, {"doc_id": "2018.sigirconf_conference-2018.238", "score": 15.471567959488688, "relevance": 0, "rank": 9, "weights": {"doc_id": [], "default_text": [[558, 565, 1.0], [566, 574, 1.0], [1057, 1064, 1.0], [1065, 1073, 1.0], [1204, 1211, 1.0], [1212, 1220, 1.0], [1293, 1301, 1.0], [1657, 1665, 1.0]]}, "snippet": {"field": "default_text", "start": 1052, "stop": 1252, "weights": [[5, 12, 1.0], [13, 21, 1.0], [152, 159, 1.0], [160, 168, 1.0]]}}, {"doc_id": "2018.cikm_conference-2018.112", "score": 15.4630087706059, "relevance": 0, "rank": 10, "weights": {"doc_id": [], "default_text": [[640, 647, 1.0], [648, 656, 1.0], [783, 790, 1.0], [791, 799, 1.0], [1322, 1330, 1.0]]}, "snippet": {"field": "default_text", "start": 635, "stop": 835, "weights": [[5, 12, 1.0], [13, 21, 1.0], [148, 155, 1.0], [156, 164, 1.0]]}}], "run_2": [], "summary": [], "mergedWeights": {}}], "docs": {"2013.cikm_workshop-2013ueo.2": {"doc_id": "2013.cikm_workshop-2013ueo.2", "default_text": "DBLP:conf/cikm/2013ueo Proceedings of the 1st workshop on User engagement optimization, UEO@CIKM 2013, San Francisco, California, USA, November 1, 2013 3\u20136 ACM 2013 https://doi.org/10.1145/2512875.2512876 10.1145/2512875.2512876 https://dblp.org/rec/conf/cikm/HuangH13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/HuangH13 inproceedings ['Jia Huang', 'Xiaohua Hu'] [] CIKM 2013.cikm_workshop-2013ueo.2 1617981077.0 ABSTRACTIn this paper, we analyze the information passing in an online recommendation system. Our dataset consists of a \"read\" network between users and books, and a user-follower network. We first investigate in general, if one's recommendations have impacts on her followers' decisions. We then analyze the correlation between one's influence and her network centrality. Finally, we investigate how recommendation effectiveness changes with the recommendation number. This investigation is taken from both senders' and receivers' perspectives. Results show that a user does have influence over her followers' decisions. Such influence is not correlated with her centrality. The more a book is recommended, the more likely that one will accept it. However, there is a saturate point beyond which more recommendations will have no more impact. On the other hand, the more recommendations one makes, the more likely that her recommendations will be accepted. This trend has no saturate point. Information passing in online recommendation", "text": "DBLP:conf/cikm/2013ueo Proceedings of the 1st workshop on User engagement optimization, UEO@CIKM 2013, San Francisco, California, USA, November 1, 2013 3\u20136 ACM 2013 https://doi.org/10.1145/2512875.2512876 10.1145/2512875.2512876 https://dblp.org/rec/conf/cikm/HuangH13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/HuangH13 inproceedings ['Jia Huang', 'Xiaohua Hu'] [] CIKM 2013.cikm_workshop-2013ueo.2 1617981077.0 ABSTRACTIn this paper, we analyze the information passing in an online recommendation system. Our dataset consists of a \"read\" network between users and books, and a user-follower network. We first investigate in general, if one's recommendations have impacts on her followers' decisions. We then analyze the correlation between one's influence and her network centrality. Finally, we investigate how recommendation effectiveness changes with the recommendation number. This investigation is taken from both senders' and receivers' perspectives. Results show that a user does have influence over her followers' decisions. Such influence is not correlated with her centrality. The more a book is recommended, the more likely that one will accept it. However, there is a saturate point beyond which more recommendations will have no more impact. On the other hand, the more recommendations one makes, the more likely that her recommendations will be accepted. This trend has no saturate point. Information passing in online recommendation"}, "2011.sigirconf_conference-2011.35": {"doc_id": "2011.sigirconf_conference-2011.35", "default_text": "DBLP:conf/sigir/2011 Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 325\u2013334 ACM 2011 https://doi.org/10.1145/2009916.2009962 10.1145/2009916.2009962 https://dblp.org/rec/conf/sigir/YeYLL11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeYLL11 inproceedings ['Mao Ye', 'Peifeng Yin', 'Wang-Chien Lee', 'Dik Lun Lee'] [] SIGIR 2011.sigirconf_conference-2011.35 1569168938.0 ABSTRACTIn this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches. Exploiting geographical influence for collaborative point-of-interest recommendation", "text": "DBLP:conf/sigir/2011 Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011 325\u2013334 ACM 2011 https://doi.org/10.1145/2009916.2009962 10.1145/2009916.2009962 https://dblp.org/rec/conf/sigir/YeYLL11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeYLL11 inproceedings ['Mao Ye', 'Peifeng Yin', 'Wang-Chien Lee', 'Dik Lun Lee'] [] SIGIR 2011.sigirconf_conference-2011.35 1569168938.0 ABSTRACTIn this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches. Exploiting geographical influence for collaborative point-of-interest recommendation"}, "2015.sigirconf_conference-2015.41": {"doc_id": "2015.sigirconf_conference-2015.41", "default_text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 393\u2013402 ACM 2015 https://doi.org/10.1145/2766462.2767748 10.1145/2766462.2767748 https://dblp.org/rec/conf/sigir/ParkKZG15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ParkKZG15 inproceedings ['Dae Hoon Park', 'Hyun Duk Kim', 'ChengXiang Zhai', 'Lifan Guo'] [] SIGIR 2015.sigirconf_conference-2015.41 1569168938.0 ABSTRACTWith the rapid development of Internet and E-commerce, abundant product reviews have been written by consumers who bought the products. These reviews are very useful for consumers to optimize their purchasing decisions. However, since the reviews are all written by consumers who have bought and used a product, there are generally very few or even no reviews available for a new product or an unpopular product. We study the novel problem of retrieving relevant opinion sentences from the reviews of other products using specifications of a new or unpopular product as query. Our key idea is to leverage product specifications to assess product similarity between the query product and other products and extract relevant opinion sentences from the similar products where a consumer may find useful discussions. Then, we provide ranked opinion sentences for the query product that has no user-generated reviews. We first propose a popular summarization method and its modified version to solve the problem. Then, we propose our novel probabilistic methods. Experiment results show that the proposed methods can effectively retrieve useful opinion sentences for products that have no reviews. Retrieval of Relevant Opinion Sentences for New Products", "text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 393\u2013402 ACM 2015 https://doi.org/10.1145/2766462.2767748 10.1145/2766462.2767748 https://dblp.org/rec/conf/sigir/ParkKZG15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ParkKZG15 inproceedings ['Dae Hoon Park', 'Hyun Duk Kim', 'ChengXiang Zhai', 'Lifan Guo'] [] SIGIR 2015.sigirconf_conference-2015.41 1569168938.0 ABSTRACTWith the rapid development of Internet and E-commerce, abundant product reviews have been written by consumers who bought the products. These reviews are very useful for consumers to optimize their purchasing decisions. However, since the reviews are all written by consumers who have bought and used a product, there are generally very few or even no reviews available for a new product or an unpopular product. We study the novel problem of retrieving relevant opinion sentences from the reviews of other products using specifications of a new or unpopular product as query. Our key idea is to leverage product specifications to assess product similarity between the query product and other products and extract relevant opinion sentences from the similar products where a consumer may find useful discussions. Then, we provide ranked opinion sentences for the query product that has no user-generated reviews. We first propose a popular summarization method and its modified version to solve the problem. Then, we propose our novel probabilistic methods. Experiment results show that the proposed methods can effectively retrieve useful opinion sentences for products that have no reviews. Retrieval of Relevant Opinion Sentences for New Products"}, "2015.sigirconf_conference-2015.149": {"doc_id": "2015.sigirconf_conference-2015.149", "default_text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 1015\u20131018 ACM 2015 https://doi.org/10.1145/2766462.2767764 10.1145/2766462.2767764 https://dblp.org/rec/conf/sigir/ZhangSTSWL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhangSTSWL15 inproceedings ['Rui Zhang', 'Pengyu Sun', 'Jiancong Tong', 'Rebecca Jane Stones', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2015.sigirconf_conference-2015.149 1614258237.0 ABSTRACTIn response to a user query, search engines return the topk relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding document. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to reduce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memory the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve comparable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching. Compact Snippet Caching for Flash-based Search Engines", "text": "DBLP:conf/sigir/2015 Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015 1015\u20131018 ACM 2015 https://doi.org/10.1145/2766462.2767764 10.1145/2766462.2767764 https://dblp.org/rec/conf/sigir/ZhangSTSWL15.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhangSTSWL15 inproceedings ['Rui Zhang', 'Pengyu Sun', 'Jiancong Tong', 'Rebecca Jane Stones', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2015.sigirconf_conference-2015.149 1614258237.0 ABSTRACTIn response to a user query, search engines return the topk relevant results, each of which contains a small piece of text, called a snippet, extracted from the corresponding document. Obtaining a snippet is time consuming as it requires both document retrieval (disk access) and string matching (CPU computation), so caching of snippets is used to reduce latency. With the trend of using flash-based solid state drives (SSDs) instead of hard disk drives for search engine storage, the bottleneck of snippet generation shifts from I/O to computation. We propose a simple, but effective method for exploiting this trend, which we call fragment caching: instead of caching the whole snippet, we only cache snippet metadata which describe how to retrieve the snippet from the document. While this approach increases I/O time, the cost is insignificant on SSDs. The major benefit of fragment caching is the ability to cache the same snippets (without loss of quality) while only using a fraction of the memory the traditional method requires. In our experiments, we find around 10 times less memory is required to achieve comparable snippet generation times for dynamic memory, and we consistently achieve a vastly greater hit ratio for static caching. Compact Snippet Caching for Flash-based Search Engines"}, "2012.sigirconf_conference-2012.5": {"doc_id": "2012.sigirconf_conference-2012.5", "default_text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 25\u201334 ACM 2012 https://doi.org/10.1145/2348283.2348290 10.1145/2348283.2348290 https://dblp.org/rec/conf/sigir/OzertemCDV12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/OzertemCDV12 inproceedings ['Umut Ozertem', 'Olivier Chapelle', 'Pinar Donmez', 'Emre Velipasaoglu'] [] SIGIR 2012.sigirconf_conference-2012.5 1543248348.0 ABSTRACTWe consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines. Learning to suggest: a machine learning framework for ranking query suggestions", "text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 25\u201334 ACM 2012 https://doi.org/10.1145/2348283.2348290 10.1145/2348283.2348290 https://dblp.org/rec/conf/sigir/OzertemCDV12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/OzertemCDV12 inproceedings ['Umut Ozertem', 'Olivier Chapelle', 'Pinar Donmez', 'Emre Velipasaoglu'] [] SIGIR 2012.sigirconf_conference-2012.5 1543248348.0 ABSTRACTWe consider the task of suggesting related queries to users after they issue their initial query to a web search engine. We propose a machine learning approach to learn the probability that a user may find a follow-up query both useful and relevant, given his initial query. Our approach is based on a machine learning model which enables us to generalize to queries that have never occurred in the logs as well. The model is trained on co-occurrences mined from the search logs, with novel utility and relevance models, and the machine learning step is done without any labeled data by human judges. The learning step allows us to generalize from the past observations and generate query suggestions that are beyond the past co-occurred queries. This brings significant gains in coverage while yielding modest gains in relevance. Both offline (based on human judges) and online (based on millions of user interactions) evaluations demonstrate that our approach significantly outperforms strong baselines. Learning to suggest: a machine learning framework for ranking query suggestions"}, "2012.sigirconf_conference-2012.69": {"doc_id": "2012.sigirconf_conference-2012.69", "default_text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 671\u2013680 ACM 2012 https://doi.org/10.1145/2348283.2348373 10.1145/2348283.2348373 https://dblp.org/rec/conf/sigir/YeLL12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeLL12 inproceedings ['Mao Ye', 'Xingjie Liu', 'Wang-Chien Lee'] [] SIGIR 2012.sigirconf_conference-2012.69 1542189490.0 ABSTRACTSocial friendship has been shown beneficial for item recommendation for years. However, existing approaches mostly incorporate social friendship into recommender systems by heuristics. In this paper, we argue that social influence between friends can be captured quantitatively and propose a probabilistic generative model, called social influenced selection(SIS), to model the decision making of item selection (e.g., what book to buy or where to dine). Based on SIS, we mine the social influence between linked friends and the personal preferences of users through statistical inference. To address the challenges arising from multiple layers of hidden factors in SIS, we develop a new parameter learning algorithm based on expectation maximization (EM). Moreover, we show that the mined social influence and user preferences are valuable for group recommendation and viral marketing. Finally, we conduct a comprehensive performance evaluation using real datasets crawled from last.fm and whrrl.com to validate our proposal. Experimental results show that social influence captured based on our SIS model is effective for enhancing both item recommendation and group recommendation, essential for viral marketing, and useful for various user analysis. Exploring social influence for recommendation: a generative model approach", "text": "DBLP:conf/sigir/2012 The 35th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '12, Portland, OR, USA, August 12-16, 2012 671\u2013680 ACM 2012 https://doi.org/10.1145/2348283.2348373 10.1145/2348283.2348373 https://dblp.org/rec/conf/sigir/YeLL12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/YeLL12 inproceedings ['Mao Ye', 'Xingjie Liu', 'Wang-Chien Lee'] [] SIGIR 2012.sigirconf_conference-2012.69 1542189490.0 ABSTRACTSocial friendship has been shown beneficial for item recommendation for years. However, existing approaches mostly incorporate social friendship into recommender systems by heuristics. In this paper, we argue that social influence between friends can be captured quantitatively and propose a probabilistic generative model, called social influenced selection(SIS), to model the decision making of item selection (e.g., what book to buy or where to dine). Based on SIS, we mine the social influence between linked friends and the personal preferences of users through statistical inference. To address the challenges arising from multiple layers of hidden factors in SIS, we develop a new parameter learning algorithm based on expectation maximization (EM). Moreover, we show that the mined social influence and user preferences are valuable for group recommendation and viral marketing. Finally, we conduct a comprehensive performance evaluation using real datasets crawled from last.fm and whrrl.com to validate our proposal. Experimental results show that social influence captured based on our SIS model is effective for enhancing both item recommendation and group recommendation, essential for viral marketing, and useful for various user analysis. Exploring social influence for recommendation: a generative model approach"}, "2014.sigirconf_conference-2014.17": {"doc_id": "2014.sigirconf_conference-2014.17", "default_text": "DBLP:conf/sigir/2014 The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014 153\u2013162 ACM 2014 https://doi.org/10.1145/2600428.2609601 10.1145/2600428.2609601 https://dblp.org/rec/conf/sigir/CormackG14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/CormackG14 inproceedings ['Gordon V. Cormack', 'Maura R. Grossman'] [] SIGIR 2014.sigirconf_conference-2014.17 1541498844.0 ABSTRACTUsing a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -four derived from the TREC 2009 Legal Track and four derived from actual legal matters -recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P < 0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P < 0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of \"stabilization\" -determining when training is adequate, and therefore may stop. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery", "text": "DBLP:conf/sigir/2014 The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014 153\u2013162 ACM 2014 https://doi.org/10.1145/2600428.2609601 10.1145/2600428.2609601 https://dblp.org/rec/conf/sigir/CormackG14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/CormackG14 inproceedings ['Gordon V. Cormack', 'Maura R. Grossman'] [] SIGIR 2014.sigirconf_conference-2014.17 1541498844.0 ABSTRACTUsing a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -four derived from the TREC 2009 Legal Track and four derived from actual legal matters -recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P < 0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P < 0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of \"stabilization\" -determining when training is adequate, and therefore may stop. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery"}, "2018.sigirconf_conference-2018.21": {"doc_id": "2018.sigirconf_conference-2018.21", "default_text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 185\u2013194 ACM 2018 https://doi.org/10.1145/3209978.3210023 10.1145/3209978.3210023 https://dblp.org/rec/conf/sigir/SunW018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SunW018 inproceedings ['Peijie Sun', 'Le Wu', 'Meng Wang'] [] SIGIR 2018.sigirconf_conference-2018.21 1600256062.0 ABSTRACTCollaborative filtering (CF) is one of the most popular techniques for building recommender systems. To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance. In these systems, uses' preferences over time are determined by their temporal dynamic interests as well as the general static interests. In the meantime, the complex interplay between users' internal interests and the social influence from the social network drives the evolution of users' preferences over time. Nevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation. Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open. To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation. In the proposed approach, we model users' complex dynamic and general static preferences over time by fusing social influence among users with two attention networks. Specifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture users' complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time. In the general static preference modeling process, we characterize each user's static interest by introducing a static social attention network to model the stationary social influence among users. The output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task. Finally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baselines. Attentive Recurrent Social Recommendation", "text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 185\u2013194 ACM 2018 https://doi.org/10.1145/3209978.3210023 10.1145/3209978.3210023 https://dblp.org/rec/conf/sigir/SunW018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/SunW018 inproceedings ['Peijie Sun', 'Le Wu', 'Meng Wang'] [] SIGIR 2018.sigirconf_conference-2018.21 1600256062.0 ABSTRACTCollaborative filtering (CF) is one of the most popular techniques for building recommender systems. To alleviate the data sparsity issue in CF, social recommendation has emerged by leveraging social influence among users for better recommendation performance. In these systems, uses' preferences over time are determined by their temporal dynamic interests as well as the general static interests. In the meantime, the complex interplay between users' internal interests and the social influence from the social network drives the evolution of users' preferences over time. Nevertheless, traditional approaches either neglected the social network structure for temporal recommendation or assumed a static social influence strength for static social recommendation. Thus, the problem of how to leverage social influence to enhance temporal social recommendation performance remains pretty much open. To this end, in this paper, we present an attentive recurrent network based approach for temporal social recommendation. In the proposed approach, we model users' complex dynamic and general static preferences over time by fusing social influence among users with two attention networks. Specifically, in the dynamic preference modeling process, we design a dynamic social aware recurrent neural network to capture users' complex latent interests over time, where a temporal attention network is proposed to learn the temporal social influence over time. In the general static preference modeling process, we characterize each user's static interest by introducing a static social attention network to model the stationary social influence among users. The output of the dynamic preferences and the static preferences are combined together in a unified end-to-end framework for the temporal social recommendation task. Finally, experimental results on two real-world datasets clearly show the superiority of our proposed model compared to the baselines. Attentive Recurrent Social Recommendation"}, "2018.sigirconf_conference-2018.238": {"doc_id": "2018.sigirconf_conference-2018.238", "default_text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 1439\u20131440 ACM 2018 https://doi.org/10.1145/3209978.3210200 10.1145/3209978.3210200 https://dblp.org/rec/conf/sigir/Zamani00LC18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Zamani00LC18 inproceedings ['Hamed Zamani', 'Mostafa Dehghani', 'Fernando Diaz', 'Hang Li', 'Nick Craswell'] [] SIGIR 2018.sigirconf_conference-2018.238 1600256062.0 ABSTRACTIn recent years, machine learning approaches, and in particular deep neural networks, have yielded significant improvements on several natural language processing and computer vision tasks; however, such breakthroughs have not yet been observed in the area of information retrieval. Besides the complexity of IR tasks, such as understanding the user's information needs, a main reason is the lack of high-quality and/or large-scale training data for many IR tasks. This necessitates studying how to design and train machine learning algorithms where there is no large-scale or highquality data in hand. Therefore, considering the quick progress in development of machine learning models, this is an ideal time for a workshop that especially focuses on learning in such an important and challenging setting for IR tasks.The goal of this workshop is to bring together researchers from industry-where data is plentiful but noisy-with researchers from academia-where data is sparse but clean to discuss solutions to these related problems. website: https://lnd4ir.github.io/ ACM Reference Format: SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval", "text": "DBLP:conf/sigir/2018 The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018 1439\u20131440 ACM 2018 https://doi.org/10.1145/3209978.3210200 10.1145/3209978.3210200 https://dblp.org/rec/conf/sigir/Zamani00LC18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Zamani00LC18 inproceedings ['Hamed Zamani', 'Mostafa Dehghani', 'Fernando Diaz', 'Hang Li', 'Nick Craswell'] [] SIGIR 2018.sigirconf_conference-2018.238 1600256062.0 ABSTRACTIn recent years, machine learning approaches, and in particular deep neural networks, have yielded significant improvements on several natural language processing and computer vision tasks; however, such breakthroughs have not yet been observed in the area of information retrieval. Besides the complexity of IR tasks, such as understanding the user's information needs, a main reason is the lack of high-quality and/or large-scale training data for many IR tasks. This necessitates studying how to design and train machine learning algorithms where there is no large-scale or highquality data in hand. Therefore, considering the quick progress in development of machine learning models, this is an ideal time for a workshop that especially focuses on learning in such an important and challenging setting for IR tasks.The goal of this workshop is to bring together researchers from industry-where data is plentiful but noisy-with researchers from academia-where data is sparse but clean to discuss solutions to these related problems. website: https://lnd4ir.github.io/ ACM Reference Format: SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval"}, "2007.sigirconf_conference-2007.26": {"doc_id": "2007.sigirconf_conference-2007.26", "default_text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 183\u2013190 ACM 2007 https://doi.org/10.1145/1277741.1277775 10.1145/1277741.1277775 https://dblp.org/rec/conf/sigir/Baeza-YatesGJMPS07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Baeza-YatesGJMPS07 inproceedings ['Ricardo A. Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] SIGIR 2007.sigirconf_conference-2007.26 1619422021.0 ABSTRACTIn this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. The impact of caching on search engines", "text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 183\u2013190 ACM 2007 https://doi.org/10.1145/1277741.1277775 10.1145/1277741.1277775 https://dblp.org/rec/conf/sigir/Baeza-YatesGJMPS07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/Baeza-YatesGJMPS07 inproceedings ['Ricardo A. Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] SIGIR 2007.sigirconf_conference-2007.26 1619422021.0 ABSTRACTIn this paper we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log affect the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. The impact of caching on search engines"}, "2007.sigirconf_conference-2007.39": {"doc_id": "2007.sigirconf_conference-2007.39", "default_text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 287\u2013294 ACM 2007 https://doi.org/10.1145/1277741.1277792 10.1145/1277741.1277792 https://dblp.org/rec/conf/sigir/ZhengCSZ07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhengCSZ07 inproceedings ['Zhaohui Zheng', 'Keke Chen', 'Gordon Sun', 'Hongyuan Zha'] [] SIGIR 2007.sigirconf_conference-2007.39 1541498845.0 ABSTRACTEffective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods. A regression framework for learning ranking functions using relative relevance judgments", "text": "DBLP:conf/sigir/2007 SIGIR 2007: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands, July 23-27, 2007 287\u2013294 ACM 2007 https://doi.org/10.1145/1277741.1277792 10.1145/1277741.1277792 https://dblp.org/rec/conf/sigir/ZhengCSZ07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ZhengCSZ07 inproceedings ['Zhaohui Zheng', 'Keke Chen', 'Gordon Sun', 'Hongyuan Zha'] [] SIGIR 2007.sigirconf_conference-2007.39 1541498845.0 ABSTRACTEffective ranking functions are an essential part of commercial search engines. We focus on developing a regression framework for learning ranking functions for improving relevance of search engines serving diverse streams of user queries. We explore supervised learning methodology from machine learning, and we distinguish two types of relevance judgments used as the training data: 1) absolute relevance judgments arising from explicit labeling of search results; and 2) relative relevance judgments extracted from user clickthroughs of search results or converted from the absolute relevance judgments. We propose a novel optimization framework emphasizing the use of relative relevance judgments. The main contribution is the development of an algorithm based on regression that can be applied to objective functions involving preference data, i.e., data indicating that a document is more relevant than another with respect to a query. Experimental results are carried out using data sets obtained from a commercial search engine. Our results show significant improvements of our proposed methods over some existing methods. A regression framework for learning ranking functions using relative relevance judgments"}, "2010.sigirconf_conference-2010.89": {"doc_id": "2010.sigirconf_conference-2010.89", "default_text": "DBLP:conf/sigir/2010 Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010, Geneva, Switzerland, July 19-23, 2010 691\u2013698 ACM 2010 https://doi.org/10.1145/1835449.1835564 10.1145/1835449.1835564 https://dblp.org/rec/conf/sigir/ArguelloDP10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ArguelloDP10 inproceedings ['Jaime Arguello', 'Fernando Diaz', 'Jean-Fran\u00e7ois Paiement'] [] SIGIR 2010.sigirconf_conference-2010.89 1541498845.0 ABSTRACTVertical aggregation is the task of incorporating results from specialized search engines or verticals (e.g., images, video, news) into Web search results. Vertical selection is the subtask of deciding, given a query, which verticals, if any, are relevant. State of the art approaches use machine learned models to predict which verticals are relevant to a query. When trained using a large set of labeled data, a machine learned vertical selection model outperforms baselines which require no training data. Unfortunately, whenever a new vertical is introduced, a costly new set of editorial data must be gathered. In this paper, we propose methods for reusing training data from a set of existing (source) verticals to learn a predictive model for a new (target) vertical. We study methods for learning robust, portable, and adaptive crossvertical models. Experiments show the need to focus on different types of features when maximizing portability (the ability for a single model to make accurate predictions across multiple verticals) than when maximizing adaptability (the ability for a single model to make accurate predictions for a specific vertical ). We demonstrate the efficacy of our methods through extensive experimentation for 11 verticals. Vertical selection in the presence of unlabeled verticals", "text": "DBLP:conf/sigir/2010 Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2010, Geneva, Switzerland, July 19-23, 2010 691\u2013698 ACM 2010 https://doi.org/10.1145/1835449.1835564 10.1145/1835449.1835564 https://dblp.org/rec/conf/sigir/ArguelloDP10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/ArguelloDP10 inproceedings ['Jaime Arguello', 'Fernando Diaz', 'Jean-Fran\u00e7ois Paiement'] [] SIGIR 2010.sigirconf_conference-2010.89 1541498845.0 ABSTRACTVertical aggregation is the task of incorporating results from specialized search engines or verticals (e.g., images, video, news) into Web search results. Vertical selection is the subtask of deciding, given a query, which verticals, if any, are relevant. State of the art approaches use machine learned models to predict which verticals are relevant to a query. When trained using a large set of labeled data, a machine learned vertical selection model outperforms baselines which require no training data. Unfortunately, whenever a new vertical is introduced, a costly new set of editorial data must be gathered. In this paper, we propose methods for reusing training data from a set of existing (source) verticals to learn a predictive model for a new (target) vertical. We study methods for learning robust, portable, and adaptive crossvertical models. Experiments show the need to focus on different types of features when maximizing portability (the ability for a single model to make accurate predictions across multiple verticals) than when maximizing adaptability (the ability for a single model to make accurate predictions for a specific vertical ). We demonstrate the efficacy of our methods through extensive experimentation for 11 verticals. Vertical selection in the presence of unlabeled verticals"}, "2013.sigirconf_conference-2013.71": {"doc_id": "2013.sigirconf_conference-2013.71", "default_text": "DBLP:conf/sigir/2013 The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '13, Dublin, Ireland - July 28 - August 01, 2013 693\u2013702 ACM 2013 https://doi.org/10.1145/2484028.2484046 10.1145/2484028.2484046 https://dblp.org/rec/conf/sigir/WangLYTWL13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/WangLYTWL13 inproceedings ['Jianguo Wang', 'Eric Lo', 'Man Lung Yiu', 'Jiancong Tong', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2013.sigirconf_conference-2013.71 1614258237.0 ABSTRACTCaching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSDbased search engines. The impact of solid state drive on search engine cache management", "text": "DBLP:conf/sigir/2013 The 36th International ACM SIGIR conference on research and development in Information Retrieval, SIGIR '13, Dublin, Ireland - July 28 - August 01, 2013 693\u2013702 ACM 2013 https://doi.org/10.1145/2484028.2484046 10.1145/2484028.2484046 https://dblp.org/rec/conf/sigir/WangLYTWL13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/sigir/WangLYTWL13 inproceedings ['Jianguo Wang', 'Eric Lo', 'Man Lung Yiu', 'Jiancong Tong', 'Gang Wang', 'Xiaoguang Liu'] [] SIGIR 2013.sigirconf_conference-2013.71 1614258237.0 ABSTRACTCaching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this paper, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. The results give insights to practitioners and researchers on how to adapt the infrastructure and how to redesign the caching policies for SSDbased search engines. The impact of solid state drive on search engine cache management"}, "2012.wsdm_conference-2012.59": {"doc_id": "2012.wsdm_conference-2012.59", "default_text": "DBLP:conf/wsdm/2012 Proceedings of the Fifth International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, WA, USA, February 8-12, 2012 573\u2013582 ACM 2012 https://doi.org/10.1145/2124295.2124365 10.1145/2124295.2124365 https://dblp.org/rec/conf/wsdm/HuangCSZJ12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/HuangCSZJ12 inproceedings ['Junming Huang', 'Xueqi Cheng', 'Huawei Shen', 'Tao Zhou', 'Xiaolong Jin'] [] WSDM 2012.wsdm_conference-2012.59 1558431513.0 ABSTRACTWord-of-mouth has proven an effective strategy for promoting products through social relations. Particularly, existing studies have convincingly demonstrated that word-ofmouth recommendations can boost users' prior expectation and hence encourage them to adopt a certain innovation, such as buying a book or watching a movie. However, less attention has been paid to studying the posterior effect of word-of-mouth recommendations, i.e., whether or not wordof-mouth recommendations can influence users' posterior evaluation on the products or services recommended to them, the answer to which is critical to estimating user satisfaction when proposing a word-of-mouth marketing strategy. In order to fill this gap, in this paper we empirically study the above issue and verify that word-of-mouth recommendations are strongly associated with users' posterior evaluation. Through elaborately designed statistical hypothesis tests we prove the causality that word-of-mouth recommendations directly prompt the posterior evaluation of receivers. Finally, we propose a method for investigating users' social influence, namely, their ability to affect followers' posterior evaluation via word-of-mouth recommendations, by examining the number of their followers and their sensitivity of discovering good items. The experimental results on real datasets show that our method can successfully identify 78% influential friends with strong social influence. Exploring social influence via posterior effect of word-of-mouth recommendations", "text": "DBLP:conf/wsdm/2012 Proceedings of the Fifth International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, WA, USA, February 8-12, 2012 573\u2013582 ACM 2012 https://doi.org/10.1145/2124295.2124365 10.1145/2124295.2124365 https://dblp.org/rec/conf/wsdm/HuangCSZJ12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/HuangCSZJ12 inproceedings ['Junming Huang', 'Xueqi Cheng', 'Huawei Shen', 'Tao Zhou', 'Xiaolong Jin'] [] WSDM 2012.wsdm_conference-2012.59 1558431513.0 ABSTRACTWord-of-mouth has proven an effective strategy for promoting products through social relations. Particularly, existing studies have convincingly demonstrated that word-ofmouth recommendations can boost users' prior expectation and hence encourage them to adopt a certain innovation, such as buying a book or watching a movie. However, less attention has been paid to studying the posterior effect of word-of-mouth recommendations, i.e., whether or not wordof-mouth recommendations can influence users' posterior evaluation on the products or services recommended to them, the answer to which is critical to estimating user satisfaction when proposing a word-of-mouth marketing strategy. In order to fill this gap, in this paper we empirically study the above issue and verify that word-of-mouth recommendations are strongly associated with users' posterior evaluation. Through elaborately designed statistical hypothesis tests we prove the causality that word-of-mouth recommendations directly prompt the posterior evaluation of receivers. Finally, we propose a method for investigating users' social influence, namely, their ability to affect followers' posterior evaluation via word-of-mouth recommendations, by examining the number of their followers and their sensitivity of discovering good items. The experimental results on real datasets show that our method can successfully identify 78% influential friends with strong social influence. Exploring social influence via posterior effect of word-of-mouth recommendations"}, "2018.wsdm_conference-2018.3": {"doc_id": "2018.wsdm_conference-2018.3", "default_text": "DBLP:conf/wsdm/2018 Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 3 ACM 2018 https://doi.org/10.1145/3159652.3176182 10.1145/3159652.3176182 https://dblp.org/rec/conf/wsdm/Pearl18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Pearl18 inproceedings ['Judea Pearl'] [] WSDM 2018.wsdm_conference-2018.3 1597335218.0 ABSTRACTCurrent machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference. Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution", "text": "DBLP:conf/wsdm/2018 Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, WSDM 2018, Marina Del Rey, CA, USA, February 5-9, 2018 3 ACM 2018 https://doi.org/10.1145/3159652.3176182 10.1145/3159652.3176182 https://dblp.org/rec/conf/wsdm/Pearl18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Pearl18 inproceedings ['Judea Pearl'] [] WSDM 2018.wsdm_conference-2018.3 1597335218.0 ABSTRACTCurrent machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference. Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution"}, "2021.wsdm_conference-2021.78": {"doc_id": "2021.wsdm_conference-2021.78", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 680\u2013688 ACM 2021 https://doi.org/10.1145/3437963.3441752 10.1145/3437963.3441752 https://dblp.org/rec/conf/wsdm/DaiW21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/DaiW21 inproceedings ['Enyan Dai', 'Suhang Wang'] [] WSDM 2021.wsdm_conference-2021.78 1617805064.0 Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy. Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 680\u2013688 ACM 2021 https://doi.org/10.1145/3437963.3441752 10.1145/3437963.3441752 https://dblp.org/rec/conf/wsdm/DaiW21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/DaiW21 inproceedings ['Enyan Dai', 'Suhang Wang'] [] WSDM 2021.wsdm_conference-2021.78 1617805064.0 Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make predictions biased on protected sensitive attributes, e.g., skin color and gender. Because machine learning algorithms including GNNs are trained to reflect the distribution of the training data which often contains historical bias towards sensitive attributes. In addition, the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism. As a result, the applications of GNNs in sensitive domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Furthermore, the practical scenario of sparse annotations in sensitive attributes is rarely considered in existing works. Therefore, we study the novel and important problem of learning fair GNNs with limited sensitive attribute information. FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high node classification accuracy by leveraging graph structures and limited sensitive information. Our theoretical analysis shows that FairGNN can ensure the fairness of GNNs under mild conditions given limited nodes with known sensitive attributes. Extensive experiments on real-world datasets also demonstrate the effectiveness of FairGNN in debiasing and keeping high accuracy. Say No to the Discrimination: Learning Fair Graph Neural Networks with Limited Sensitive Attribute Information"}, "2021.wsdm_conference-2021.156": {"doc_id": "2021.wsdm_conference-2021.156", "default_text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1161\u20131162 ACM 2021 https://doi.org/10.1145/3437963.3441838 10.1145/3437963.3441838 https://dblp.org/rec/conf/wsdm/ZhangZC0CGSD21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhangZC0CGSD21 inproceedings ['Yongfeng Zhang', 'Min Zhang', 'Hanxiong Chen', 'Xu Chen', 'Xianjie Chen', 'Chuang Gan', 'Tong Sun', 'Xin Luna Dong'] [] WSDM 2021.wsdm_conference-2021.156 1617805064.0 Recent years have witnessed the success of machine learning and especially deep learning in many research areas such as Vision and Language Processing, Information Retrieval and Recommender Systems, Social Networks and Conversational Agents. Though various learning approaches have demonstrated satisfying performance in perceptual tasks such as associative learning and matching by extracting useful similarity patterns from data, the area still sees a large amount of research needed to advance the ability of reasoning towards cognitive intelligence in the coming years. This includes but is not limited to neural logical reasoning, neural-symbolic reasoning, causal reasoning, knowledge reasoning and commonsense reasoning. The workshop focuses on the research of machine reasoning techniques and their application in various intelligent tasks. It will gather researchers as well as practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent progress in machine intelligence to a broader community, including but not limited to CV, IR, NLP, ML, DM, AI and beyond. The 1st International Workshop on Machine Reasoning: International Machine Reasoning Conference (MRC 2021)", "text": "DBLP:conf/wsdm/2021 WSDM '21, The Fourteenth ACM International Conference on Web Search and Data Mining, Virtual Event, Israel, March 8-12, 2021 1161\u20131162 ACM 2021 https://doi.org/10.1145/3437963.3441838 10.1145/3437963.3441838 https://dblp.org/rec/conf/wsdm/ZhangZC0CGSD21.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/ZhangZC0CGSD21 inproceedings ['Yongfeng Zhang', 'Min Zhang', 'Hanxiong Chen', 'Xu Chen', 'Xianjie Chen', 'Chuang Gan', 'Tong Sun', 'Xin Luna Dong'] [] WSDM 2021.wsdm_conference-2021.156 1617805064.0 Recent years have witnessed the success of machine learning and especially deep learning in many research areas such as Vision and Language Processing, Information Retrieval and Recommender Systems, Social Networks and Conversational Agents. Though various learning approaches have demonstrated satisfying performance in perceptual tasks such as associative learning and matching by extracting useful similarity patterns from data, the area still sees a large amount of research needed to advance the ability of reasoning towards cognitive intelligence in the coming years. This includes but is not limited to neural logical reasoning, neural-symbolic reasoning, causal reasoning, knowledge reasoning and commonsense reasoning. The workshop focuses on the research of machine reasoning techniques and their application in various intelligent tasks. It will gather researchers as well as practitioners in the field for discussions, idea communications, and research promotions. It will also generate insightful debates about the recent progress in machine intelligence to a broader community, including but not limited to CV, IR, NLP, ML, DM, AI and beyond. The 1st International Workshop on Machine Reasoning: International Machine Reasoning Conference (MRC 2021)"}, "2017.wsdm_conference-2017.57": {"doc_id": "2017.wsdm_conference-2017.57", "default_text": "DBLP:conf/wsdm/2017 Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 535 ACM 2017 https://doi.org/10.1145/3018661.3022764 10.1145/3018661.3022764 https://dblp.org/rec/conf/wsdm/Herbrich17.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Herbrich17 inproceedings ['Ralf Herbrich'] [] WSDM 2017.wsdm_conference-2017.57 1558364082.0 ABSTRACTIn this talk I will give an introduction into the field of machine learning and discuss why it is a crucial technology for Amazon.Machine learning is the science of automatically extracting patterns from data in order to make automated predictions of future data. One way to differentiate machine learning tasks is by the following two factors: (1) How much noise is contained in the data? and (2) How far into the future is the prediction task? The former presents a limit to the learnability of task -regardless which learning algorithm is used -whereas the latter has a crucial implication on the representation of the predictions: while most tasks in search and advertising typically only forecast minutes into the future, tasks in e-commerce can require predictions up to a year into the future. The further the forecast horizon, the more important it is to take account of uncertainty in both the learning algorithm and the representation of the predictions. I will discuss which learning frameworks are best suited for the various scenarios, that is, short-term predictions with little noise vs. long-term predictions with lots of noise, and present some ideas to combine representation learning with probabilistic methods.In the second half of the talk, I will give an overview of the applications of machine learning at Amazon ranging from demand forecasting, machine translation to automation of computer vision tasks and robotics. I will also discuss the importance of tools for data scientist and share learnings on bringing machine learning algorithms into products. Machine Learning at Amazon", "text": "DBLP:conf/wsdm/2017 Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM 2017, Cambridge, United Kingdom, February 6-10, 2017 535 ACM 2017 https://doi.org/10.1145/3018661.3022764 10.1145/3018661.3022764 https://dblp.org/rec/conf/wsdm/Herbrich17.bib dblp computer science bibliography, https://dblp.org DBLP:conf/wsdm/Herbrich17 inproceedings ['Ralf Herbrich'] [] WSDM 2017.wsdm_conference-2017.57 1558364082.0 ABSTRACTIn this talk I will give an introduction into the field of machine learning and discuss why it is a crucial technology for Amazon.Machine learning is the science of automatically extracting patterns from data in order to make automated predictions of future data. One way to differentiate machine learning tasks is by the following two factors: (1) How much noise is contained in the data? and (2) How far into the future is the prediction task? The former presents a limit to the learnability of task -regardless which learning algorithm is used -whereas the latter has a crucial implication on the representation of the predictions: while most tasks in search and advertising typically only forecast minutes into the future, tasks in e-commerce can require predictions up to a year into the future. The further the forecast horizon, the more important it is to take account of uncertainty in both the learning algorithm and the representation of the predictions. I will discuss which learning frameworks are best suited for the various scenarios, that is, short-term predictions with little noise vs. long-term predictions with lots of noise, and present some ideas to combine representation learning with probabilistic methods.In the second half of the talk, I will give an overview of the applications of machine learning at Amazon ranging from demand forecasting, machine translation to automation of computer vision tasks and robotics. I will also discuss the importance of tools for data scientist and share learnings on bringing machine learning algorithms into products. Machine Learning at Amazon"}, "2011.ecir_conference-2011.12": {"doc_id": "2011.ecir_conference-2011.12", "default_text": "DBLP:conf/ecir/2011 Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011, Dublin, Ireland, April 18-21, 2011. Proceedings Lecture Notes in Computer Science 6611 104\u2013116 Springer 2011 https://doi.org/10.1007/978-3-642-20161-5_12 10.1007/978-3-642-20161-5_12 https://dblp.org/rec/conf/ecir/BortnikovLV11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BortnikovLV11 inproceedings ['Edward Bortnikov', 'Ronny Lempel', 'Kolman Vornovitsky'] [] ECIR 2011.ecir_conference-2011.12 1619422016.0 Abstract. Modern search engines feature real-time indices, which incorporate changes to content within seconds. As search engines also cache search results for reducing user latency and back-end load, without careful real-time management of search results caches, the engine might return stale search results to users despite the efforts invested in keeping the underlying index up to date. A recent paper proposed an architectural component called CIP -the cache invalidation predictor. CIPs invalidate supposedly stale cache entries upon index modifications. Initial evaluation showed the ability to keep the performance benefits of caching without sacrificing much the freshness of search results returned to users. However, it was conducted on a synthetic workload in a simplified setting, using many assumptions. We propose new CIP heuristics, and evaluate them in an authentic environment -on the real evolving corpus and query stream of a large commercial news search engine. Our CIPs operate in conjunction with realistic cache settings, and we use standard metrics for evaluating cache performance. We show that a classical cache replacement policy, LRU, completely fails to guarantee freshness over time, whereas our CIPs serve 97% of the queries with fresh results. Our policies incur a negligible impact on the baseline's cache hit rate, in contrast with traditional age-based invalidation, which must severely reduce the cache performance in order to achieve the same freshness. We demonstrate that the computational overhead of our algorithms is minor, and that they even allow reducing the cache's memory footprint. Caching for Realtime Search", "text": "DBLP:conf/ecir/2011 Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011, Dublin, Ireland, April 18-21, 2011. Proceedings Lecture Notes in Computer Science 6611 104\u2013116 Springer 2011 https://doi.org/10.1007/978-3-642-20161-5_12 10.1007/978-3-642-20161-5_12 https://dblp.org/rec/conf/ecir/BortnikovLV11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BortnikovLV11 inproceedings ['Edward Bortnikov', 'Ronny Lempel', 'Kolman Vornovitsky'] [] ECIR 2011.ecir_conference-2011.12 1619422016.0 Abstract. Modern search engines feature real-time indices, which incorporate changes to content within seconds. As search engines also cache search results for reducing user latency and back-end load, without careful real-time management of search results caches, the engine might return stale search results to users despite the efforts invested in keeping the underlying index up to date. A recent paper proposed an architectural component called CIP -the cache invalidation predictor. CIPs invalidate supposedly stale cache entries upon index modifications. Initial evaluation showed the ability to keep the performance benefits of caching without sacrificing much the freshness of search results returned to users. However, it was conducted on a synthetic workload in a simplified setting, using many assumptions. We propose new CIP heuristics, and evaluate them in an authentic environment -on the real evolving corpus and query stream of a large commercial news search engine. Our CIPs operate in conjunction with realistic cache settings, and we use standard metrics for evaluating cache performance. We show that a classical cache replacement policy, LRU, completely fails to guarantee freshness over time, whereas our CIPs serve 97% of the queries with fresh results. Our policies incur a negligible impact on the baseline's cache hit rate, in contrast with traditional age-based invalidation, which must severely reduce the cache performance in order to achieve the same freshness. We demonstrate that the computational overhead of our algorithms is minor, and that they even allow reducing the cache's memory footprint. Caching for Realtime Search"}, "2009.ecir_conference-2009.41": {"doc_id": "2009.ecir_conference-2009.41", "default_text": "DBLP:conf/ecir/2009 Advances in Information Retrieval, 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings Lecture Notes in Computer Science 5478 461\u2013472 Springer 2009 https://doi.org/10.1007/978-3-642-00958-7_41 10.1007/978-3-642-00958-7_41 https://dblp.org/rec/conf/ecir/BaccianellaES09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BaccianellaES09 inproceedings ['Stefano Baccianella', 'Andrea Esuli', 'Fabrizio Sebastiani'] [] ECIR 2009.ecir_conference-2009.41 1557820837.0 Abstract. Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site. Multi-facet Rating of Product Reviews", "text": "DBLP:conf/ecir/2009 Advances in Information Retrieval, 31th European Conference on IR Research, ECIR 2009, Toulouse, France, April 6-9, 2009. Proceedings Lecture Notes in Computer Science 5478 461\u2013472 Springer 2009 https://doi.org/10.1007/978-3-642-00958-7_41 10.1007/978-3-642-00958-7_41 https://dblp.org/rec/conf/ecir/BaccianellaES09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/ecir/BaccianellaES09 inproceedings ['Stefano Baccianella', 'Andrea Esuli', 'Fabrizio Sebastiani'] [] ECIR 2009.ecir_conference-2009.41 1557820837.0 Abstract. Online product reviews are becoming increasingly available, and are being used more and more frequently by consumers in order to choose among competing products. Tools that rank competing products in terms of the satisfaction of consumers that have purchased the product before, are thus also becoming popular. We tackle the problem of rating (i.e., attributing a numerical score of satisfaction to) consumer reviews based on their textual content. We here focus on multi-facet review rating, i.e., on the case in which the review of a product (e.g., a hotel) must be rated several times, according to several aspects of the product (for a hotel: cleanliness, centrality of location, etc.). We explore several aspects of the problem, with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning. We present the results of experiments conducted on a dataset of more than 15,000 reviews that we have crawled from a popular hotel review site. Multi-facet Rating of Product Reviews"}, "2011.cikm_conference-2011.113": {"doc_id": "2011.cikm_conference-2011.113", "default_text": "DBLP:conf/cikm/2011 Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, Glasgow, United Kingdom, October 24-28, 2011 957\u2013966 ACM 2011 https://doi.org/10.1145/2063576.2063713 10.1145/2063576.2063713 https://dblp.org/rec/conf/cikm/DhillonSS11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/DhillonSS11 inproceedings ['Paramveer S. Dhillon', 'Sundararajan Sellamanickam', 'Sathiya Keerthi Selvaraj'] [] CIKM 2011.cikm_conference-2011.113 1541519871.0 ABSTRACTExtracting information from web pages is an important problem; it has several applications such as providing improved search results and construction of databases to serve user queries. In this paper we propose a novel structured prediction method to address two important aspects of the extraction problem: (1) labeled data is available only for a small number of sites and (2) a machine learned global model does not generalize adequately well across many websites. For this purpose, we propose a weight space based graph regularization method. This method has several advantages. First, it can use unlabeled data to address the limited labeled data problem and falls in the class of graph regularization based semi-supervised learning approaches. Second, to address the generalization inadequacy of a global model, this method builds a local model for each website. Viewing the problem of building a local model for each website as a task, we learn the models for a collection of sites jointly; thus our method can also be seen as a graph regularization based multi-task learning approach. Learning the models jointly with the proposed method is very useful in two ways:(1) learning a local model for a website can be effectively influenced by labeled and unlabeled data from other websites; and (2) even for a website with only unlabeled examples it is possible to learn a decent local model. We demonstrate the efficacy of our method on several real-life data; experimental results show that significant performance improvement can be obtained by combining semi-supervised and multi-task learning in a single framework. Semi-supervised multi-task learning of structured prediction models for web information extraction", "text": "DBLP:conf/cikm/2011 Proceedings of the 20th ACM Conference on Information and Knowledge Management, CIKM 2011, Glasgow, United Kingdom, October 24-28, 2011 957\u2013966 ACM 2011 https://doi.org/10.1145/2063576.2063713 10.1145/2063576.2063713 https://dblp.org/rec/conf/cikm/DhillonSS11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/DhillonSS11 inproceedings ['Paramveer S. Dhillon', 'Sundararajan Sellamanickam', 'Sathiya Keerthi Selvaraj'] [] CIKM 2011.cikm_conference-2011.113 1541519871.0 ABSTRACTExtracting information from web pages is an important problem; it has several applications such as providing improved search results and construction of databases to serve user queries. In this paper we propose a novel structured prediction method to address two important aspects of the extraction problem: (1) labeled data is available only for a small number of sites and (2) a machine learned global model does not generalize adequately well across many websites. For this purpose, we propose a weight space based graph regularization method. This method has several advantages. First, it can use unlabeled data to address the limited labeled data problem and falls in the class of graph regularization based semi-supervised learning approaches. Second, to address the generalization inadequacy of a global model, this method builds a local model for each website. Viewing the problem of building a local model for each website as a task, we learn the models for a collection of sites jointly; thus our method can also be seen as a graph regularization based multi-task learning approach. Learning the models jointly with the proposed method is very useful in two ways:(1) learning a local model for a website can be effectively influenced by labeled and unlabeled data from other websites; and (2) even for a website with only unlabeled examples it is possible to learn a decent local model. We demonstrate the efficacy of our method on several real-life data; experimental results show that significant performance improvement can be obtained by combining semi-supervised and multi-task learning in a single framework. Semi-supervised multi-task learning of structured prediction models for web information extraction"}, "2014.cikm_conference-2014.67": {"doc_id": "2014.cikm_conference-2014.67", "default_text": "DBLP:conf/cikm/2014 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014 659\u2013668 ACM 2014 https://doi.org/10.1145/2661829.2661983 10.1145/2661829.2661983 https://dblp.org/rec/conf/cikm/YuanCS14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/YuanCS14 inproceedings ['Quan Yuan', 'Gao Cong', 'Aixin Sun'] [] CIKM 2014.cikm_conference-2014.67 1585296194.0 ABSTRACTThe availability of user check-in data in large volume from the rapid growing location-based social networks (LBSNs) enables a number of important location-aware services. Point-of-interest (POI) recommendation is one of such services, which is to recommend POIs that users have not visited before. It has been observed that: (i) users tend to visit nearby places, and (ii) users tend to visit different places in different time slots, and in the same time slot, users tend to periodically visit the same places. For example, users usually visit a restaurant during lunch hours, and visit a pub at night. In this paper, we focus on the problem of time-aware POI recommendation, which aims at recommending a list of POIs for a user to visit at a given time. To exploit both geographical and temporal influences in time-aware POI recommendation, we propose the Geographical-Temporal influences Aware Graph (GTAG) to model check-in records, geographical influence and temporal influence. For effective and efficient recommendation based on GTAG, we develop a preference propagation algorithm named Breadth-first Preference Propagation (BPP). The algorithm follows a relaxed breathfirst search strategy, and returns recommendation results within at most 6 propagation steps. Our experimental results on two realworld datasets show that the proposed graph-based approach outperforms state-of-the-art POI recommendation methods substantially. Graph-based Point-of-interest Recommendation with Geographical and Temporal Influences", "text": "DBLP:conf/cikm/2014 Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM 2014, Shanghai, China, November 3-7, 2014 659\u2013668 ACM 2014 https://doi.org/10.1145/2661829.2661983 10.1145/2661829.2661983 https://dblp.org/rec/conf/cikm/YuanCS14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/YuanCS14 inproceedings ['Quan Yuan', 'Gao Cong', 'Aixin Sun'] [] CIKM 2014.cikm_conference-2014.67 1585296194.0 ABSTRACTThe availability of user check-in data in large volume from the rapid growing location-based social networks (LBSNs) enables a number of important location-aware services. Point-of-interest (POI) recommendation is one of such services, which is to recommend POIs that users have not visited before. It has been observed that: (i) users tend to visit nearby places, and (ii) users tend to visit different places in different time slots, and in the same time slot, users tend to periodically visit the same places. For example, users usually visit a restaurant during lunch hours, and visit a pub at night. In this paper, we focus on the problem of time-aware POI recommendation, which aims at recommending a list of POIs for a user to visit at a given time. To exploit both geographical and temporal influences in time-aware POI recommendation, we propose the Geographical-Temporal influences Aware Graph (GTAG) to model check-in records, geographical influence and temporal influence. For effective and efficient recommendation based on GTAG, we develop a preference propagation algorithm named Breadth-first Preference Propagation (BPP). The algorithm follows a relaxed breathfirst search strategy, and returns recommendation results within at most 6 propagation steps. Our experimental results on two realworld datasets show that the proposed graph-based approach outperforms state-of-the-art POI recommendation methods substantially. Graph-based Point-of-interest Recommendation with Geographical and Temporal Influences"}, "2009.cikm_conference-2009.190": {"doc_id": "2009.cikm_conference-2009.190", "default_text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1581\u20131584 ACM 2009 https://doi.org/10.1145/1645953.1646177 10.1145/1645953.1646177 https://dblp.org/rec/conf/cikm/KanungoGKW09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KanungoGKW09 inproceedings ['Tapas Kanungo', 'Nadia Ghamrawi', 'Ki Yuen Kim', 'Lawrence Wai'] [] CIKM 2009.cikm_conference-2009.190 1617981078.0 ABSTRACTEye tracking experiments have shown that titles of Web search results play a crucial role in guiding a user's search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pairwise modeling approach gives better results in terms of three offline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process -search success -that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success. Web search result summarization: title selection algorithms and user satisfaction", "text": "DBLP:conf/cikm/2009 Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China, November 2-6, 2009 1581\u20131584 ACM 2009 https://doi.org/10.1145/1645953.1646177 10.1145/1645953.1646177 https://dblp.org/rec/conf/cikm/KanungoGKW09.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KanungoGKW09 inproceedings ['Tapas Kanungo', 'Nadia Ghamrawi', 'Ki Yuen Kim', 'Lawrence Wai'] [] CIKM 2009.cikm_conference-2009.190 1617981078.0 ABSTRACTEye tracking experiments have shown that titles of Web search results play a crucial role in guiding a user's search process. We present a machine-learned algorithm that trains a boosted tree to pick the most relevant title for a Web search result. We compare two modeling approaches: i) using absolute editorial judgments and ii) using pairwise preference judgments. We find that the pairwise modeling approach gives better results in terms of three offline metrics. We present results of our models in four regions. We also describe a hybrid user satisfaction evaluation process -search success -that combines page relevance and user click behavior, and show that our machine-learned algorithm improves in search success. Web search result summarization: title selection algorithms and user satisfaction"}, "2018.cikm_conference-2018.98": {"doc_id": "2018.cikm_conference-2018.98", "default_text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 953\u2013962 ACM 2018 https://doi.org/10.1145/3269206.3271742 10.1145/3269206.3271742 https://dblp.org/rec/conf/cikm/ChenFEZC018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ChenFEZC018 inproceedings ['Jiawei Chen', 'Yan Feng', 'Martin Ester', 'Sheng Zhou', 'Chun Chen', 'Can Wang'] [] CIKM 2018.cikm_conference-2018.98 1605210009.0 ABSTRACTUsers' consumption behaviors are affected by both their personal preference and their exposure to items (i.e. whether a user knows the items). Most of the recent works in social recommendation assume that people share similar preference with their socially connected friends. However, this assumption may not hold due to the diversity of social relations, and modeling social influence on users' preference may not be suitable for implicit feedback data (i.e. whether a user has consumed certain items). Since users often share item information with their social relations, it will be less restrictive to model social influence on users' exposure to items. We notice that a user's exposure is affected by the exposure of the other users in his social communities and by the consumption of his connected friends. In this paper, we propose a novel social exposure-based recommendation model SoEXBMF by integrating two kinds of social influence on users' exposure, i.e. social knowledge influence and social consumption influence, into basic EXMF model for better recommendation performance. Furthermore, SoEXBMF uses Bernoulli distribution instead of Gaussian distribution in EXMF to better model the binary implicit feedback data. A variational inference method has been developed for the proposed SoEXBMF model to infer the posterior and make the recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over existing methods in various evaluation metrics. Modeling Users' Exposure with Social Knowledge Influence and Consumption Influence for Recommendation", "text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 953\u2013962 ACM 2018 https://doi.org/10.1145/3269206.3271742 10.1145/3269206.3271742 https://dblp.org/rec/conf/cikm/ChenFEZC018.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ChenFEZC018 inproceedings ['Jiawei Chen', 'Yan Feng', 'Martin Ester', 'Sheng Zhou', 'Chun Chen', 'Can Wang'] [] CIKM 2018.cikm_conference-2018.98 1605210009.0 ABSTRACTUsers' consumption behaviors are affected by both their personal preference and their exposure to items (i.e. whether a user knows the items). Most of the recent works in social recommendation assume that people share similar preference with their socially connected friends. However, this assumption may not hold due to the diversity of social relations, and modeling social influence on users' preference may not be suitable for implicit feedback data (i.e. whether a user has consumed certain items). Since users often share item information with their social relations, it will be less restrictive to model social influence on users' exposure to items. We notice that a user's exposure is affected by the exposure of the other users in his social communities and by the consumption of his connected friends. In this paper, we propose a novel social exposure-based recommendation model SoEXBMF by integrating two kinds of social influence on users' exposure, i.e. social knowledge influence and social consumption influence, into basic EXMF model for better recommendation performance. Furthermore, SoEXBMF uses Bernoulli distribution instead of Gaussian distribution in EXMF to better model the binary implicit feedback data. A variational inference method has been developed for the proposed SoEXBMF model to infer the posterior and make the recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over existing methods in various evaluation metrics. Modeling Users' Exposure with Social Knowledge Influence and Consumption Influence for Recommendation"}, "2018.cikm_conference-2018.112": {"doc_id": "2018.cikm_conference-2018.112", "default_text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 1093\u20131102 ACM 2018 https://doi.org/10.1145/3269206.3271685 10.1145/3269206.3271685 https://dblp.org/rec/conf/cikm/LeeOFKY18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/LeeOFKY18 inproceedings ['Dongha Lee', 'Jinoh Oh', 'Christos Faloutsos', 'Byungju Kim', 'Hwanjo Yu'] [] CIKM 2018.cikm_conference-2018.112 1542800655.0 ABSTRACTMore and more data need to be processed or analyzed within mobile devices for efficiency or privacy reasons, but performing machine learning tasks with large data within the devices is challenging because of their limited memory resources. For this reason, diskbased machine learning methods have been actively researched, which utilize storage resources without holding all the data in memory. This paper proposes D-MC 2 , a novel disk-based matrix completion method that (1) supports incremental data update (i.e., data insertion and deletion) and (2) spills both data and model to disk when necessary; these functionalities are not supported by existing methods. First, D-MC 2 builds a two-layered index to efficiently support incremental data update; there exists a tradeoff relationship between model learning and data update costs, and our two-layered index simultaneously optimizes the two costs. Second, we develop a window-based stochastic gradient descent (SGD) scheduler to efficiently support the dual spilling; a huge amount of disk I/O is incurred when the size of model is larger than that of memory, and our new scheduler substantially reduces it. Our evaluation results show that D-MC 2 is significantly more scalable and faster than other disk-based competitors under the limited memory environment. In terms of the co-optimization, D-MC 2 outperforms the baselines that only optimize one of the two costs up to 48x. Furthermore, the window-based scheduler improves the training speed 12.4x faster compared to a naive scheduler. Disk-based Matrix Completion for Memory Limited Devices", "text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 1093\u20131102 ACM 2018 https://doi.org/10.1145/3269206.3271685 10.1145/3269206.3271685 https://dblp.org/rec/conf/cikm/LeeOFKY18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/LeeOFKY18 inproceedings ['Dongha Lee', 'Jinoh Oh', 'Christos Faloutsos', 'Byungju Kim', 'Hwanjo Yu'] [] CIKM 2018.cikm_conference-2018.112 1542800655.0 ABSTRACTMore and more data need to be processed or analyzed within mobile devices for efficiency or privacy reasons, but performing machine learning tasks with large data within the devices is challenging because of their limited memory resources. For this reason, diskbased machine learning methods have been actively researched, which utilize storage resources without holding all the data in memory. This paper proposes D-MC 2 , a novel disk-based matrix completion method that (1) supports incremental data update (i.e., data insertion and deletion) and (2) spills both data and model to disk when necessary; these functionalities are not supported by existing methods. First, D-MC 2 builds a two-layered index to efficiently support incremental data update; there exists a tradeoff relationship between model learning and data update costs, and our two-layered index simultaneously optimizes the two costs. Second, we develop a window-based stochastic gradient descent (SGD) scheduler to efficiently support the dual spilling; a huge amount of disk I/O is incurred when the size of model is larger than that of memory, and our new scheduler substantially reduces it. Our evaluation results show that D-MC 2 is significantly more scalable and faster than other disk-based competitors under the limited memory environment. In terms of the co-optimization, D-MC 2 outperforms the baselines that only optimize one of the two costs up to 48x. Furthermore, the window-based scheduler improves the training speed 12.4x faster compared to a naive scheduler. Disk-based Matrix Completion for Memory Limited Devices"}, "2018.cikm_conference-2018.299": {"doc_id": "2018.cikm_conference-2018.299", "default_text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 2233\u20132241 ACM 2018 https://doi.org/10.1145/3269206.3272033 10.1145/3269206.3272033 https://dblp.org/rec/conf/cikm/BellKM18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/BellKM18 inproceedings ['Anthony Bell', 'Prathyusha Senthil Kumar', 'Daniel Miranda'] [] CIKM 2018.cikm_conference-2018.299 1542800656.0 ABSTRACTSearch relevance is a very critical component in e-commerce applications. One of the strongest signals that determine the relevance of an item listing to an e-commerce query is the title of the item. Traditional methods for capturing this signal compare words in listing titles and the user query using tf-idf scores, or use a machine learned model with words as features and target clicks or relevance labels. Contrary to these approaches, we build a parameterized model to determine the weights of popular title terms for a query and then use these title term weights to compute the relevance of a listing title to the query. For this, we use human judged binary relevance labels of query and item title pairs as labeled data and train a model leveraging a variety of features to learn these query specific title term weights. We propose two novel approaches to model these title term weights using the relevance target and explore several novel features specific to e-commerce for this term weighting model. We use the resulting title relevance score as a feature in eBay's machine learned ranker for e-commerce search serving millions of queries each day. We observe a significant improvement over a baseline click-based binary independence model for capturing item title relevance in several metrics including model accuracy and overall relevance and engagement observed through A/B testing. We also experimentally illustrate that this feature optimized for relevance works well in conjunction with textual features optimized for demand. The Title Says It All: A Title Term Weighting Strategy For eCommerce Ranking", "text": "DBLP:conf/cikm/2018 Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy, October 22-26, 2018 2233\u20132241 ACM 2018 https://doi.org/10.1145/3269206.3272033 10.1145/3269206.3272033 https://dblp.org/rec/conf/cikm/BellKM18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/BellKM18 inproceedings ['Anthony Bell', 'Prathyusha Senthil Kumar', 'Daniel Miranda'] [] CIKM 2018.cikm_conference-2018.299 1542800656.0 ABSTRACTSearch relevance is a very critical component in e-commerce applications. One of the strongest signals that determine the relevance of an item listing to an e-commerce query is the title of the item. Traditional methods for capturing this signal compare words in listing titles and the user query using tf-idf scores, or use a machine learned model with words as features and target clicks or relevance labels. Contrary to these approaches, we build a parameterized model to determine the weights of popular title terms for a query and then use these title term weights to compute the relevance of a listing title to the query. For this, we use human judged binary relevance labels of query and item title pairs as labeled data and train a model leveraging a variety of features to learn these query specific title term weights. We propose two novel approaches to model these title term weights using the relevance target and explore several novel features specific to e-commerce for this term weighting model. We use the resulting title relevance score as a feature in eBay's machine learned ranker for e-commerce search serving millions of queries each day. We observe a significant improvement over a baseline click-based binary independence model for capturing item title relevance in several metrics including model accuracy and overall relevance and engagement observed through A/B testing. We also experimentally illustrate that this feature optimized for relevance works well in conjunction with textual features optimized for demand. The Title Says It All: A Title Term Weighting Strategy For eCommerce Ranking"}, "2007.cikm_conference-2007.3": {"doc_id": "2007.cikm_conference-2007.3", "default_text": "DBLP:conf/cikm/2007 Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM 2007, Lisbon, Portugal, November 6-10, 2007 9\u201310 ACM 2007 https://doi.org/10.1145/1321440.1321443 10.1145/1321440.1321443 https://dblp.org/rec/conf/cikm/Pereira07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Pereira07 inproceedings ['Fernando Pereira'] [] CIKM 2007.cikm_conference-2007.3 1541519853.0 AbstractText, speech, images, video, DNA sequences provide information about entities that people can recognize when looking at a particular instance. But those entities and their attributes and relationships are not directly accessible to queries that join across types of sources. Information extraction methods based on supervised machine learning recognize mentions of entities and relationships of predefined types in different kinds of sources, which can then be used to answer some useful types of queries. However, supervised learning relies on hand-annotated training sets that are difficult to create and limit what types of entities and relationships can be joined for new applications. These limitations have prompted research into unsupervised extraction methods that rely on correlations among sources rather than hand-annotated training sets. While these methods are not yet as accurate as those based on supervised learning, they have the potential for a new query-by-example approach to information integration in which seed sets of query answers are expanded into ranked lists of potential answers by learning occurrence patterns from the seed answers. I will give examples of both types of methods from our research on biomedical information extraction, leading to some ideas on a possible convergence of search and databases through machine learning.\nCategories and Subject Descriptors: E.0 GENERAL\nGeneral Terms: Experimentation\nKeywords: Information and Knowledge Management\nBioFernando Pereira is the Andrew and Debra Rachleff Professor and chair of the department of Computer and Information Science, University of Pennsylvania. He received a Ph.D. in Artificial Intelligence from the University of Edinburgh in 1982. Before joining Penn, he held industrial research and management positions at SRI International, at AT&T Labs, where he led the machine learning and information retrieval research department from September 1995 to April 2000, and at WhizBang Labs, a Web information extraction company. His main research interests are in machine-learnable models of language and other natural sequential data such as biological sequences. He made major contributions to advances in finite-state models for speech and text processing now in everyday industrial use. He has over 100 research publications on computational linguistics, speech recognition, machine learning and logic programming, and several issued and pending patents on speech recognition, language processing, and humancomputer interfaces. He was elected Fellow of the American Association for Artificial Intelligence in 1991 for his contributions to computational linguistics and logic programming, and he is a past president of the Association for Computational Linguistics. Learning to join everything", "text": "DBLP:conf/cikm/2007 Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management, CIKM 2007, Lisbon, Portugal, November 6-10, 2007 9\u201310 ACM 2007 https://doi.org/10.1145/1321440.1321443 10.1145/1321440.1321443 https://dblp.org/rec/conf/cikm/Pereira07.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Pereira07 inproceedings ['Fernando Pereira'] [] CIKM 2007.cikm_conference-2007.3 1541519853.0 AbstractText, speech, images, video, DNA sequences provide information about entities that people can recognize when looking at a particular instance. But those entities and their attributes and relationships are not directly accessible to queries that join across types of sources. Information extraction methods based on supervised machine learning recognize mentions of entities and relationships of predefined types in different kinds of sources, which can then be used to answer some useful types of queries. However, supervised learning relies on hand-annotated training sets that are difficult to create and limit what types of entities and relationships can be joined for new applications. These limitations have prompted research into unsupervised extraction methods that rely on correlations among sources rather than hand-annotated training sets. While these methods are not yet as accurate as those based on supervised learning, they have the potential for a new query-by-example approach to information integration in which seed sets of query answers are expanded into ranked lists of potential answers by learning occurrence patterns from the seed answers. I will give examples of both types of methods from our research on biomedical information extraction, leading to some ideas on a possible convergence of search and databases through machine learning.\nCategories and Subject Descriptors: E.0 GENERAL\nGeneral Terms: Experimentation\nKeywords: Information and Knowledge Management\nBioFernando Pereira is the Andrew and Debra Rachleff Professor and chair of the department of Computer and Information Science, University of Pennsylvania. He received a Ph.D. in Artificial Intelligence from the University of Edinburgh in 1982. Before joining Penn, he held industrial research and management positions at SRI International, at AT&T Labs, where he led the machine learning and information retrieval research department from September 1995 to April 2000, and at WhizBang Labs, a Web information extraction company. His main research interests are in machine-learnable models of language and other natural sequential data such as biological sequences. He made major contributions to advances in finite-state models for speech and text processing now in everyday industrial use. He has over 100 research publications on computational linguistics, speech recognition, machine learning and logic programming, and several issued and pending patents on speech recognition, language processing, and humancomputer interfaces. He was elected Fellow of the American Association for Artificial Intelligence in 1991 for his contributions to computational linguistics and logic programming, and he is a past president of the Association for Computational Linguistics. Learning to join everything"}, "2010.cikm_conference-2010.163": {"doc_id": "2010.cikm_conference-2010.163", "default_text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1397\u20131400 ACM 2010 https://doi.org/10.1145/1871437.1871631 10.1145/1871437.1871631 https://dblp.org/rec/conf/cikm/KarimzadehganZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KarimzadehganZ10 inproceedings ['Maryam Karimzadehgan', 'ChengXiang Zhai'] [] CIKM 2010.cikm_conference-2010.163 1546877862.0 ABSTRACTWe study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively optimize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration. Exploration-exploitation tradeoff in interactive relevance feedback", "text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1397\u20131400 ACM 2010 https://doi.org/10.1145/1871437.1871631 10.1145/1871437.1871631 https://dblp.org/rec/conf/cikm/KarimzadehganZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/KarimzadehganZ10 inproceedings ['Maryam Karimzadehgan', 'ChengXiang Zhai'] [] CIKM 2010.cikm_conference-2010.163 1546877862.0 ABSTRACTWe study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively optimize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration. Exploration-exploitation tradeoff in interactive relevance feedback"}, "2010.cikm_conference-2010.192": {"doc_id": "2010.cikm_conference-2010.192", "default_text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1513\u20131516 ACM 2010 https://doi.org/10.1145/1871437.1871660 10.1145/1871437.1871660 https://dblp.org/rec/conf/cikm/FengZXY10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/FengZXY10 inproceedings ['Shicong Feng', 'Li Zhang', 'Yuhong Xiong', 'Conglei Yao'] [] CIKM 2010.cikm_conference-2010.192 1546877862.0 ABSTRACTThe goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessible web documents. The major challenge in focused crawling is how to effectively determine each hyperlink's capability of leading to target pages. To compute this capability, we 1 present a novel approach, called Navigational Rank (NR). In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link's capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experimental results show that our approach is domain-independent and significantly outperforms the state-of-arts. Focused crawling using navigational rank", "text": "DBLP:conf/cikm/2010 Proceedings of the 19th ACM Conference on Information and Knowledge Management, CIKM 2010, Toronto, Ontario, Canada, October 26-30, 2010 1513\u20131516 ACM 2010 https://doi.org/10.1145/1871437.1871660 10.1145/1871437.1871660 https://dblp.org/rec/conf/cikm/FengZXY10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/FengZXY10 inproceedings ['Shicong Feng', 'Li Zhang', 'Yuhong Xiong', 'Conglei Yao'] [] CIKM 2010.cikm_conference-2010.192 1546877862.0 ABSTRACTThe goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessible web documents. The major challenge in focused crawling is how to effectively determine each hyperlink's capability of leading to target pages. To compute this capability, we 1 present a novel approach, called Navigational Rank (NR). In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link's capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experimental results show that our approach is domain-independent and significantly outperforms the state-of-arts. Focused crawling using navigational rank"}, "2016.cikm_conference-2016.93": {"doc_id": "2016.cikm_conference-2016.93", "default_text": "DBLP:conf/cikm/2016 Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016 891 ACM 2016 https://doi.org/10.1145/2983323.2983371 10.1145/2983323.2983371 https://dblp.org/rec/conf/cikm/Najork16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Najork16 inproceedings ['Marc Najork'] [] CIKM 2016.cikm_conference-2016.93 1609263763.0 ABSTRACTEmail is an essential communication medium for billions of people, with most users relying on web-based email services. Two recent trends are changing the email experience: smartphones have become the primary tool for accessing online services including email, and machine learning has come of age. Smartphones have a number of compelling properties (they are location-aware, usually with us, and allow us to record and share photos and videos), but they also have a few limitations, notably limited screen size and small and tedious virtual keyboards. Over the past few years, Google researchers and engineers have leveraged machine learning to ameliorate these weaknesses, and in the process created novel experiences. In this talk, I will give three examples of machine learning improving the email experience.The first example describes how we are improving email search. Displaying the most relevant results as the query is being typed is particularly useful on smartphones due to the aforementioned limitations. Combining hand-crafted and machine-learned rankers is powerful, but training learned rankers requires a relevance-labeled training set. User privacy prohibits us from employing raters to produce relevance labels. Instead, we leverage implicit feedback (namely clicks) provided by the users themselves. Using click logs as training data in a learning-to-rank setting is intriguing, since there is a vast and continuous supply of fresh training data. However, the click stream is biased towards queries that receive more clicks -e.g. queries for which we already return the best result in the top-ranked position. I will summarize our work [2] on neutralizing that bias.The second example describes how we extract key information from appointment and reservation emails and surface it at the appropriate time as a reminder on the user's smartphone. Our basic approach  is to learn the templates that were used to generate these emails, use these templates to extract key information such as places, dates and times, store the extracted records in a personal information store, and surface them at the right time, taking contextual information such as estimated transit time into account.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).CIKM '16 October 24-28, 2016, Indianapolis, IN, USA     The third example describes Smart Reply [1], a system that offers a set of three short responses to those incoming emails for which a short response is appropriate, allowing users to respond quickly with just a few taps, without typing or involving voice-to-text transcription. The basic approach is to learn a model of likely short responses to original emails from the corpus, and then to apply the model whenever a new message arrives. Other considerations include offering a set of responses that are all appropriate and yet diverse, and triggering only when sufficiently confident that each responses is of high quality and appropriate. Using Machine Learning to Improve the Email Experience", "text": "DBLP:conf/cikm/2016 Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016 891 ACM 2016 https://doi.org/10.1145/2983323.2983371 10.1145/2983323.2983371 https://dblp.org/rec/conf/cikm/Najork16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/Najork16 inproceedings ['Marc Najork'] [] CIKM 2016.cikm_conference-2016.93 1609263763.0 ABSTRACTEmail is an essential communication medium for billions of people, with most users relying on web-based email services. Two recent trends are changing the email experience: smartphones have become the primary tool for accessing online services including email, and machine learning has come of age. Smartphones have a number of compelling properties (they are location-aware, usually with us, and allow us to record and share photos and videos), but they also have a few limitations, notably limited screen size and small and tedious virtual keyboards. Over the past few years, Google researchers and engineers have leveraged machine learning to ameliorate these weaknesses, and in the process created novel experiences. In this talk, I will give three examples of machine learning improving the email experience.The first example describes how we are improving email search. Displaying the most relevant results as the query is being typed is particularly useful on smartphones due to the aforementioned limitations. Combining hand-crafted and machine-learned rankers is powerful, but training learned rankers requires a relevance-labeled training set. User privacy prohibits us from employing raters to produce relevance labels. Instead, we leverage implicit feedback (namely clicks) provided by the users themselves. Using click logs as training data in a learning-to-rank setting is intriguing, since there is a vast and continuous supply of fresh training data. However, the click stream is biased towards queries that receive more clicks -e.g. queries for which we already return the best result in the top-ranked position. I will summarize our work [2] on neutralizing that bias.The second example describes how we extract key information from appointment and reservation emails and surface it at the appropriate time as a reminder on the user's smartphone. Our basic approach  is to learn the templates that were used to generate these emails, use these templates to extract key information such as places, dates and times, store the extracted records in a personal information store, and surface them at the right time, taking contextual information such as estimated transit time into account.Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).CIKM '16 October 24-28, 2016, Indianapolis, IN, USA     The third example describes Smart Reply [1], a system that offers a set of three short responses to those incoming emails for which a short response is appropriate, allowing users to respond quickly with just a few taps, without typing or involving voice-to-text transcription. The basic approach is to learn a model of likely short responses to original emails from the corpus, and then to apply the model whenever a new message arrives. Other considerations include offering a set of responses that are all appropriate and yet diverse, and triggering only when sufficiently confident that each responses is of high quality and appropriate. Using Machine Learning to Improve the Email Experience"}, "2016.cikm_conference-2016.202": {"doc_id": "2016.cikm_conference-2016.202", "default_text": "DBLP:conf/cikm/2016 Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016 1917\u20131920 ACM 2016 https://doi.org/10.1145/2983323.2983873 10.1145/2983323.2983873 https://dblp.org/rec/conf/cikm/ZhangWYLLZ16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ZhangWYLLZ16 inproceedings ['Qinzhe Zhang', 'Jia Wu', 'Hong Yang', 'Weixue Lu', 'Guodong Long', 'Chengqi Zhang'] [] CIKM 2016.cikm_conference-2016.202 1609263763.0 ABSTRACTSocial recommendation has been widely studied in recent years. Existing social recommendation models use various explicit pieces of social information as regularization terms, e.g., social links are considered as new constraints. However, social influence, an implicit source of information in social networks, is seldomly considered, even though it often drives recommendations in social networks. In this paper, we introduce a new global and local influence-based social recommendation model. Based on the observation that user purchase behaviour is influenced by both global influential nodes and the local influential nodes of the user, we formulate the global and local influence as an regularization terms, and incorporate them into a matrix factorization-based recommendation model. Experimental results on large data sets demonstrate the performance of the proposed method. Global and Local Influence-based Social Recommendation", "text": "DBLP:conf/cikm/2016 Proceedings of the 25th ACM International Conference on Information and Knowledge Management, CIKM 2016, Indianapolis, IN, USA, October 24-28, 2016 1917\u20131920 ACM 2016 https://doi.org/10.1145/2983323.2983873 10.1145/2983323.2983873 https://dblp.org/rec/conf/cikm/ZhangWYLLZ16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/cikm/ZhangWYLLZ16 inproceedings ['Qinzhe Zhang', 'Jia Wu', 'Hong Yang', 'Weixue Lu', 'Guodong Long', 'Chengqi Zhang'] [] CIKM 2016.cikm_conference-2016.202 1609263763.0 ABSTRACTSocial recommendation has been widely studied in recent years. Existing social recommendation models use various explicit pieces of social information as regularization terms, e.g., social links are considered as new constraints. However, social influence, an implicit source of information in social networks, is seldomly considered, even though it often drives recommendations in social networks. In this paper, we introduce a new global and local influence-based social recommendation model. Based on the observation that user purchase behaviour is influenced by both global influential nodes and the local influential nodes of the user, we formulate the global and local influence as an regularization terms, and incorporate them into a matrix factorization-based recommendation model. Experimental results on large data sets demonstrate the performance of the proposed method. Global and Local Influence-based Social Recommendation"}, "2012.wwwconf_conference-2012.18": {"doc_id": "2012.wwwconf_conference-2012.18", "default_text": "DBLP:conf/www/2012 Proceedings of the 21st World Wide Web Conference 2012, WWW 2012, Lyon, France, April 16-20, 2012 171\u2013180 ACM 2012 https://doi.org/10.1145/2187836.2187860 10.1145/2187836.2187860 https://dblp.org/rec/conf/www/KoehlW12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/KoehlW12 inproceedings ['Aaron Koehl', 'Haining Wang'] [] WWW 2012.wwwconf_conference-2012.18 1618560252.0 ABSTRACTSearch engines are an essential component of the web, but their web crawling agents can impose a significant burden on heavily loaded web servers. Unfortunately, blocking or deferring web crawler requests is not a viable solution due to economic consequences. We conduct a quantitative measurement study on the impact and cost of web crawling agents, seeking optimization points for this class of request. Based on our measurements, we present a practical caching approach for mitigating search engine overload, and implement the two-level cache scheme on a very busy web server. Our experimental results show that the proposed caching framework can effectively reduce the impact of search engine overload on service quality. Surviving a search engine overload", "text": "DBLP:conf/www/2012 Proceedings of the 21st World Wide Web Conference 2012, WWW 2012, Lyon, France, April 16-20, 2012 171\u2013180 ACM 2012 https://doi.org/10.1145/2187836.2187860 10.1145/2187836.2187860 https://dblp.org/rec/conf/www/KoehlW12.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/KoehlW12 inproceedings ['Aaron Koehl', 'Haining Wang'] [] WWW 2012.wwwconf_conference-2012.18 1618560252.0 ABSTRACTSearch engines are an essential component of the web, but their web crawling agents can impose a significant burden on heavily loaded web servers. Unfortunately, blocking or deferring web crawler requests is not a viable solution due to economic consequences. We conduct a quantitative measurement study on the impact and cost of web crawling agents, seeking optimization points for this class of request. Based on our measurements, we present a practical caching approach for mitigating search engine overload, and implement the two-level cache scheme on a very busy web server. Our experimental results show that the proposed caching framework can effectively reduce the impact of search engine overload on service quality. Surviving a search engine overload"}, "2011.wwwconf_conference-2011c.86": {"doc_id": "2011.wwwconf_conference-2011c.86", "default_text": "DBLP:conf/www/2011c Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011 (Companion Volume) 171\u2013172 ACM 2011 https://doi.org/10.1145/1963192.1963279 10.1145/1963192.1963279 https://dblp.org/rec/conf/www/YuZWC11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuZWC11 inproceedings ['Jianxing Yu', 'Zheng-Jun Zha', 'Meng Wang', 'Tat-Seng Chua'] [] WWW 2011.wwwconf_conference-2011c.86 1585295462.0 ABSTRACTIn this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach. Hierarchical organization of unstructured consumer reviews", "text": "DBLP:conf/www/2011c Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011 (Companion Volume) 171\u2013172 ACM 2011 https://doi.org/10.1145/1963192.1963279 10.1145/1963192.1963279 https://dblp.org/rec/conf/www/YuZWC11.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/YuZWC11 inproceedings ['Jianxing Yu', 'Zheng-Jun Zha', 'Meng Wang', 'Tat-Seng Chua'] [] WWW 2011.wwwconf_conference-2011c.86 1585295462.0 ABSTRACTIn this paper, we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews. Based on the derived hierarchy, we generate a hierarchical organization of the consumer reviews based on various aspects of the product, and aggregate consumer opinions on the aspects. With such hierarchical organization, people can easily grasp the overview of consumer reviews and opinions on various aspects, as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy. We conduct evaluation on two product review data sets: Liu et al.'s data set containing 314 reviews for five products [2], and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products. The experimental results demonstrate the effectiveness of our approach. Hierarchical organization of unstructured consumer reviews"}, "2014.wwwconf_conference-2014c.198": {"doc_id": "2014.wwwconf_conference-2014c.198", "default_text": "DBLP:conf/www/2014c 23rd International World Wide Web Conference, WWW '14, Seoul, Republic of Korea, April 7-11, 2014, Companion Volume 595\u2013598 ACM 2014 https://doi.org/10.1145/2567948.2578039 10.1145/2567948.2578039 https://dblp.org/rec/conf/www/BorgolteKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BorgolteKV14 inproceedings ['Kevin Borgolte', 'Christopher Kruegel', 'Giovanni Vigna'] [] WWW 2014.wwwconf_conference-2014c.198 1541519827.0 ABSTRACTTracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on \"diffing\" XML-encoded literary documents or XMLencoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software. Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines", "text": "DBLP:conf/www/2014c 23rd International World Wide Web Conference, WWW '14, Seoul, Republic of Korea, April 7-11, 2014, Companion Volume 595\u2013598 ACM 2014 https://doi.org/10.1145/2567948.2578039 10.1145/2567948.2578039 https://dblp.org/rec/conf/www/BorgolteKV14.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BorgolteKV14 inproceedings ['Kevin Borgolte', 'Christopher Kruegel', 'Giovanni Vigna'] [] WWW 2014.wwwconf_conference-2014c.198 1541519827.0 ABSTRACTTracking the evolution of websites has become fundamental to the understanding of today's Internet. The automatic reasoning of how and why websites change has become essential to developers and businesses alike, in particular because the manual reasoning has become impractical due to the sheer number of modifications that websites undergo during their operational lifetime, including but not limited to rotating advertisements, personalized content, insertion of new content, or removal of old content.Prior work in the area of change detection, such as XyDiff [3], X-Diff [8] or AT&T's internet difference engine [4], focused mainly on \"diffing\" XML-encoded literary documents or XMLencoded databases. Only some previous work investigated the differences that must be taken into account to accurately extract the difference between HTML documents for which the markup language does not necessarily describe the content but is used to describe how the content is displayed instead. Additionally, prior work identifies all changes to a website, even those that might not be relevant to the overall analysis goal, in turn, they unnecessarily burden the analysis engine with additional workload.In this paper, we introduce a novel analysis framework, the Delta framework, that works by (i) extracting the modifications between two versions of the same website using a fuzzy tree difference algorithm, and (ii) using a machine-learning algorithm to derive a model of relevant website changes that can be used to cluster similar modifications to reduce the overall workload imposed on an analysis engine. Based on this model for example, the tracked content changes can be used to identify ongoing or even inactive web-based malware campaigns, or to automatically learn semantic translations of sentences or paragraphs by analyzing websites that are available in multiple languages.In prior work, we showed the effectiveness of the Delta framework by applying it to the detection and automatic identification of web-based malware campaigns [2] on a data set of over 26 million pairs of websites that were crawled over a time span of four months. During this time, the system based on our framework successfully identified previously unknown web-based malware campaigns, such as a targeted campaign infecting installations of the Discuz!X Internet forum software. Relevant change detection: a framework for the precise extraction of modified and novel web-based content as a filtering technique for analysis engines"}, "2008.wwwconf_conference-2008.153": {"doc_id": "2008.wwwconf_conference-2008.153", "default_text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 1117\u20131118 ACM 2008 https://doi.org/10.1145/1367497.1367684 10.1145/1367497.1367684 https://dblp.org/rec/conf/www/HuLCS08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HuLCS08 inproceedings ['Nan Hu', 'Ling Liu', 'Bin Chen', 'Jialie Shen'] [] WWW 2008.wwwconf_conference-2008.153 1541519828.0 ABSTRACTThis paper investigates the strategic decisions of online vendors for offering different mechanism, such as sampling and online reviews of information products, to increase their online sales. Focusing on measuring the effectiveness of electronic market design (offering reviews, sampling, or both), our study shows that online markets behavior as communication markets, and consumers learn product quality information both passively (reading online reviews) and actively but subjectively (listening to music sampling). Using data from Amazon, first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect. In general, products with sampling option enjoy a higher conversion rate (which leads to better sales) than those without sampling because sampling decreases the uncertainty of consuming experience goods. Second, the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without. Third, when the uncertainty of the societal reviews is higher, sampling plays a more important role because it mitigates such uncertainty introduced by online reviews. How to influence my customers?: the impact of electronic market design", "text": "DBLP:conf/www/2008 Proceedings of the 17th International Conference on World Wide Web, WWW 2008, Beijing, China, April 21-25, 2008 1117\u20131118 ACM 2008 https://doi.org/10.1145/1367497.1367684 10.1145/1367497.1367684 https://dblp.org/rec/conf/www/HuLCS08.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HuLCS08 inproceedings ['Nan Hu', 'Ling Liu', 'Bin Chen', 'Jialie Shen'] [] WWW 2008.wwwconf_conference-2008.153 1541519828.0 ABSTRACTThis paper investigates the strategic decisions of online vendors for offering different mechanism, such as sampling and online reviews of information products, to increase their online sales. Focusing on measuring the effectiveness of electronic market design (offering reviews, sampling, or both), our study shows that online markets behavior as communication markets, and consumers learn product quality information both passively (reading online reviews) and actively but subjectively (listening to music sampling). Using data from Amazon, first we show that sampling along is a strong product quality signal that reduces the product uncertainty after controlling for halo effect. In general, products with sampling option enjoy a higher conversion rate (which leads to better sales) than those without sampling because sampling decreases the uncertainty of consuming experience goods. Second, the impact of online reviews on sales conversion rate is lower for experience goods with a sampling option than those without. Third, when the uncertainty of the societal reviews is higher, sampling plays a more important role because it mitigates such uncertainty introduced by online reviews. How to influence my customers?: the impact of electronic market design"}, "2020.wwwconf_conference-2020c.108": {"doc_id": "2020.wwwconf_conference-2020c.108", "default_text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 331\u2013336 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382184 10.1145/3366424.3382184 https://dblp.org/rec/conf/www/HernandezNI20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HernandezNI20 inproceedings ['Anthony Hernandez', 'Kin Wai Ng', 'Adriana Iamnitchi'] [] WWW 2020.wwwconf_conference-2020c.108 1602688588.0 The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub. Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations", "text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 331\u2013336 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3382184 10.1145/3366424.3382184 https://dblp.org/rec/conf/www/HernandezNI20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/HernandezNI20 inproceedings ['Anthony Hernandez', 'Kin Wai Ng', 'Adriana Iamnitchi'] [] WWW 2020.wwwconf_conference-2020c.108 1602688588.0 The recent advances in neural network-based machine learning algorithms promise a revolution in prediction-based tasks in a variety of domains. Of these, forecasting user activity in social media is particularly relevant for problems such as modeling and predicting information diffusion and designing intervention techniques to mitigate disinformation campaigns. Social media seems an ideal context for applying neural network techniques, as they provide large datasets and challenging prediction objectives. Yet, our experiments find a number of limitations in the power of deep neural networks and traditional machine learning approaches in predicting user activity on social media platforms. These limitations are related to dataset characteristics due to temporal aspects of user behavior. This work describes the challenges we encountered while attempting to forecast user activity on two popular social interaction sites: Twitter and GitHub. Using Deep Learning for Temporal Forecasting of User Activity on Social Media: Challenges and Limitations"}, "2020.wwwconf_conference-2020c.114": {"doc_id": "2020.wwwconf_conference-2020c.114", "default_text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 379\u2013384 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3383299 10.1145/3366424.3383299 https://dblp.org/rec/conf/www/BoM0L20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BoM0L20 inproceedings ['Hongbo Bo', 'Ryan McConville', 'Jun Hong', 'Weiru Liu'] [] WWW 2020.wwwconf_conference-2020c.114 1597337186.0 Within social networks user influence may be modelled based on user interactions. Further, it is typical to recommend users to others. What is the role of user influence in user recommendation? In this paper, we first propose to use a node embedding approach to integrate many types of interaction into embedded spaces where we then define a novel closeness measure to quantify the closeness of users based on interactions. We then propose a new influence ranking algorithm based on PageRank by incorporating the closeness measure into the ranking mechanism. We evaluate our algorithm, EIRank, using a dataset collected from Twitter. Our experimental results show that our algorithm measures user influence better by way of a user recommendation task, where our algorithm outperforms TwitterRank across a range of experimental network settings. Social Network Influence Ranking via Embedding Network Interactions for User Recommendation", "text": "DBLP:conf/www/2020c Companion of The 2020 Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 379\u2013384 ACM / IW3C2 2020 https://doi.org/10.1145/3366424.3383299 10.1145/3366424.3383299 https://dblp.org/rec/conf/www/BoM0L20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/BoM0L20 inproceedings ['Hongbo Bo', 'Ryan McConville', 'Jun Hong', 'Weiru Liu'] [] WWW 2020.wwwconf_conference-2020c.114 1597337186.0 Within social networks user influence may be modelled based on user interactions. Further, it is typical to recommend users to others. What is the role of user influence in user recommendation? In this paper, we first propose to use a node embedding approach to integrate many types of interaction into embedded spaces where we then define a novel closeness measure to quantify the closeness of users based on interactions. We then propose a new influence ranking algorithm based on PageRank by incorporating the closeness measure into the ranking mechanism. We evaluate our algorithm, EIRank, using a dataset collected from Twitter. Our experimental results show that our algorithm measures user influence better by way of a user recommendation task, where our algorithm outperforms TwitterRank across a range of experimental network settings. Social Network Influence Ranking via Embedding Network Interactions for User Recommendation"}, "2020.wwwconf_conference-2020.16": {"doc_id": "2020.wwwconf_conference-2020.16", "default_text": "DBLP:conf/www/2020 WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 167\u2013178 ACM / IW3C2 2020 https://doi.org/10.1145/3366423.3380104 10.1145/3366423.3380104 https://dblp.org/rec/conf/www/ZeberBORSWL20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/ZeberBORSWL20 inproceedings ['David Zeber', 'Sarah Bird', 'Camila Oliveira', 'Walter Rudametkin', 'Ilana Segall', 'Fredrik Wolls\u00e9n', 'Martin Lopatka'] [] WWW 2020.wwwconf_conference-2020.16 1608855298.0 Large-scale Web crawls have emerged as the state of the art for studying characteristics of the Web. In particular, they are a core tool for online tracking research. Web crawling is an attractive approach to data collection, as crawls can be run at relatively low infrastructure cost and don't require handling sensitive user data such as browsing histories. However, the biases introduced by using crawls as a proxy for human browsing data have not been well studied. Crawls may fail to capture the diversity of user environments, and the snapshot view of the Web presented by one-time crawls does not reflect its constantly evolving nature, which hinders reproducibility of crawl-based studies. In this paper, we quantify the repeatability and representativeness of Web crawls in terms of common tracking and fingerprinting metrics, considering both variation across crawls and divergence from human browser usage. We quantify baseline variation of simultaneous crawls, then isolate the effects of time, cloud IP address vs. residential, and operating system. This provides a foundation to assess the agreement between crawls visiting a standard list of high-traffic websites and actual browsing behaviour measured from an opt-in sample of over 50,000 users of the Firefox Web browser. Our analysis reveals differences between the treatment of stateless crawling infrastructure and generally stateful human browsing, showing, for example, that crawlers tend to experience higher rates of third-party activity than human browser users on loading pages from the same domains. The Representativeness of Automated Web Crawls as a Surrogate for Human Browsing", "text": "DBLP:conf/www/2020 WWW '20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 167\u2013178 ACM / IW3C2 2020 https://doi.org/10.1145/3366423.3380104 10.1145/3366423.3380104 https://dblp.org/rec/conf/www/ZeberBORSWL20.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/ZeberBORSWL20 inproceedings ['David Zeber', 'Sarah Bird', 'Camila Oliveira', 'Walter Rudametkin', 'Ilana Segall', 'Fredrik Wolls\u00e9n', 'Martin Lopatka'] [] WWW 2020.wwwconf_conference-2020.16 1608855298.0 Large-scale Web crawls have emerged as the state of the art for studying characteristics of the Web. In particular, they are a core tool for online tracking research. Web crawling is an attractive approach to data collection, as crawls can be run at relatively low infrastructure cost and don't require handling sensitive user data such as browsing histories. However, the biases introduced by using crawls as a proxy for human browsing data have not been well studied. Crawls may fail to capture the diversity of user environments, and the snapshot view of the Web presented by one-time crawls does not reflect its constantly evolving nature, which hinders reproducibility of crawl-based studies. In this paper, we quantify the repeatability and representativeness of Web crawls in terms of common tracking and fingerprinting metrics, considering both variation across crawls and divergence from human browser usage. We quantify baseline variation of simultaneous crawls, then isolate the effects of time, cloud IP address vs. residential, and operating system. This provides a foundation to assess the agreement between crawls visiting a standard list of high-traffic websites and actual browsing behaviour measured from an opt-in sample of over 50,000 users of the Firefox Web browser. Our analysis reveals differences between the treatment of stateless crawling infrastructure and generally stateful human browsing, showing, for example, that crawlers tend to experience higher rates of third-party activity than human browser users on loading pages from the same domains. The Representativeness of Automated Web Crawls as a Surrogate for Human Browsing"}, "2013.wwwconf_conference-2013.41": {"doc_id": "2013.wwwconf_conference-2013.41", "default_text": "DBLP:conf/www/2013 22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013 471\u2013482 International World Wide Web Conferences Steering Committee / ACM 2013 https://doi.org/10.1145/2488388.2488430 10.1145/2488388.2488430 https://dblp.org/rec/conf/www/GollapalliCMG13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GollapalliCMG13 inproceedings ['Sujatha Das Gollapalli', 'Cornelia Caragea', 'Prasenjit Mitra', 'C. Lee Giles'] [] WWW 2013.wwwconf_conference-2013.41 1603662470.0 ABSTRACTA classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying \"irrelevant\" pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end, we design novel URL-based features and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.In addition, we propose a novel technique for \"learning a conforming pair of classifiers\" using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make \"similar\" predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Miscellaneous\nGeneral TermsAlgorithms Keywords co-training, consensus maximization, gradient descent\nMOTIVATIONProfessional homepages of researchers, which typically summarize research interests, publications and other metadata related to researchers, are shown to be rich sources of information for digital libraries . Researchers' homepages    Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.WWW 2013, May 13-17, 2013, Rio de Janeiro, Brazil.   ACM 978-1-4503-2035 (also referred to as academic homepages or simply homepages in this paper) have been successfully employed in tasks such as expertise search , extraction of academic networks, author profile extraction and disambiguation  as they provide crucial evidence for improving these tasks in digital libraries.Furthermore, digital library systems such as CiteSeer 1 , ArnetMiner 2 , and Google Scholar 3 are primarily interested in obtaining and tracking researchers' homepages in order to retrieve appropriate scientific research publications. Given the infeasibility of collecting the entire content on the Web, a focused crawler aims to minimize the use of network bandwidth and hardware by selectively crawling only pages relevant to a (specified) set of topics . A key component for such a crawler is a classification module that identifies whether a webpage being accessed during the crawl process is potentially useful to the collection. For digital libraries, the \"yield\" of such crawlers highly depends on the accuracy of researcher homepage classification.Supervised methods for learning homepage classifiers rely on the availability of large amounts of labeled data. A widely used labeled dataset for webpage classification is the WebKB dataset 4 that is built in 1997. However, due to recent changes in the information content on academic websites, this dataset is becoming outdated. For example, there are now pages on academic websites that are related to various activities such as invited talks, news, events that do not occur in the WebKB dataset. We refer to university, department and research center websites as \"academic websites\" in this paper. Compared to few decades back, it is easier now to find faculty information, links to their homepages, information on research groups, course related notes and documents, and research papers from academic websites. Similarly, job postings, seminar announcements and notices are also being uploaded onto departmental websites in recent times .How can a homepage classifier keep up in the face of rapidly changing types of pages on the Web? Specifically, given a classifier that identifies homepages with reasonable accuracy (as measured on the training datasets), how does it perform in the potentially different deployment environment? Semi-supervised methods that can exploit large amounts of unlabeled data together with limited amounts of labeled data for learning accurate classifiers have received significant attention in recent research in machine learning due to Against this background, one question that can be raised is: Can we design techniques to effectively adjust the previouslytrained classifier to the changed content on the Web, while minimizing the human effort required for labeling new data, and under what conditions, can such an adjustment be possible? The research that we describe in this paper addresses specifically this question.Contributions and Organization. We present two approaches to researcher homepage classification using unlabeled data readily available from academic websites. More precisely, we first adapt the well-known co-training approach  to reflect the change in the data distribution over time. Second, we design an iterative algorithm based on the Minibatch Gradient Descent technique for learning a conforming pair of predictors using two different views of the data. We restrict ourselves to homepages of researchers in Computer Science since training datasets are available for this discipline. To the best of our knowledge, the problem of researcher homepage classification using unlabeled data available during focused crawling was not addressed in previous research. The contributions of this work are as follows:\u2022 We show that with the classifiers trained on existing datasets for researcher homepage classification we incorrectly identify pages of types not seen in the training datasets as homepages. This results in unacceptable yield from the perspective of a focused crawler.\u2022 We design novel features based on URL surface patterns and terms to complement term and HTML features extracted from the content of homepages and show that these two sets of features can be treated as two \"views\" for a researcher homepage instance.\u2022 We show that the URL and content-based views can be used successfully in a co-training setup to adapt classifiers to the changing academic environments using unlabeled data. This finding enables us to accurately crawl the new academic website content without having to label a new dataset for re-training the classifier.\u2022 Inspired by the success of co-training on this problem, we investigate loss functions that capture the disparity between the classifiers' predictions on the two views afforded by co-training. We design an iterative algorithm based on the Mini-batch Gradient Descent technique for minimizing this loss and learning a conforming pair of predictors with the two views.\u2022 Finally, we show that minimizing our proposed loss function on unlabeled data closely corresponds to the effect demonstrated by co-training techniques. We posit that this loss can, therefore, be used as a measure, for tracking the progress of co-training schemes even in the absence of a validation dataset.Although in this paper we focus on the design of accurate approaches for researcher homepage classification, our objective is to integrate this classification component in the context of focused crawling, to improve retrieval and indexing of scientific publications in digital libraries such as CiteSeer and ArnetMiner. In these usage environments, since maintaining up-to-date collections of research literature is of primary importance, having an accurate list of homepage URLs for frequent, periodic tracking is both feasible and scalable, compared to examining the entire content at academic websites each time. The rest of this paper is organized as follows: We briefly summarize closely related work in Section 2. Researcher homepage classification is discussed in Section 3. We elaborate on details of our co-training experiments and learning conforming predictor pairs in Section 4. Experimental setup, datasets and results are discussed in Section 5, followed by a summary and future extensions to our work.\nRELATED WORKResearcher homepage classification is a well-studied webpage classification problem in context of digital libraries such as CiteSeer  and ArnetMiner . Typically, contentbased term features and HTML structure-based features are used for classifying webpages . We propose the use of URL features as additional evidence for homepage identification. A smaller set (compared to ours) of URL-based features (presence of part of the name, presence of the character '\u223c', etc.), was used in isolating homepages among the search engine results for researcher name queries by Tang, et  al [40]. URL-based features are widely used in tasks related to the Web. For example, URL strings were used for extracting rules to solve the webpage de-duplication problem . Shih and Karger [38]  used URL features, the visual placements of links in the referring pages to a URL for improving applications such as ad-blocking and recommendation, whereas a preliminary study by Kan and Thi [20]  illustrates the use of URLs in performing fast webpage classification.The problem of collecting a high-quality researcher homepage collection was studied for Japanese websites by Wang, et al. using on-page and anchor text features . Tang, et al. studied homepage acquisition from search engine results using researcher names as queries . In contrast, we seek to apply focused crawling using a seed list of academic websites (where researcher homepages are typically hosted) to acquire such a collection. Focused crawling first proposed by Bra, et al. is a rich area of research on the Web . Chakrabarti, et al. [7]  present a discussion on the main components involved in building a focused crawler. Although focused crawling is our motivating application, this paper deals with the classifier component of the crawler and not with the crawler itself.We show that the focused crawling scenario presents novel challenges in using a pre-trained homepage classifier in identifying relevant pages. Specifically, the classifier needs to be attuned to the changing types of pages on the Web. Cotraining is proposed as a solution for addressing this challenge for homepage classification. Blum and Mitchell [4] first proposed co-training, an approach for semi-supervised learning when the number of labeled examples available for training is limited, and applied it to webpage classification. This approach requires having two views of features for the instances and has been shown to work well when the two views satisfy certain assumptions on \"sufficiency\" and \"independence\" . Recent research addresses techniques for decomposing the feature set into two views when such a split is not naturally available .Multiview learning (of which co-training is a special case, with two views) is typically addressed by maximizing \"consensus\" or agreement among the different views . Most solutions to multiview learning tend to frame the problem in terms of a global optimization problem and simulta-472 neously learn classifiers for all the underlying views. In some cases, the solutions depend on underlying classification algorithm used . Although our proposed algorithm based on mini-batch gradient descent seeks to maximize consensus as well, our approach is a generic technique assuming only that the underlying classifiers output initial \"parameter vectors\" that are altered using a simple, iterative algorithm.\nFEATURES FOR HOMEPAGE CLASSIFICATIONWebpage or text classification is typically handled using \"bag of words\" approaches. Specifically, the frequently occurring and discerning terms are collected from training data to form a feature dictionary that is used to represent instances as normalized term frequency or TFIDF vectors . Homepage classification was previously studied as a text classification problem using term features . Previous work on the same problem also used other content-based features related to the HTML structure of the page such as the number of images/tables on the page, and the terms commonly found in anchor text of homepages . In this study, we extracted content-based and URL-based features from our training sets. These features and the size of feature sets are summarized in . The term dictionaries contain terms that occur in at least three documents (i.e., webpages) and at least five times in the training set.In addition to term dictionaries, we hypothesize that the URL strings of homepages can provide additional evidence for identifying homepages. Hence, we design novel URLbased features based on surface patterns and presence in WordNet 5 . The URL-based features are explained in the next subsection.  \nURL strings as additional evidenceThe idea of using URL strings in academic homepage identification comes from an error analysis of a crawl obtained with the content-based classifier. Consider some example URLs we encountered in our crawl listed in .With some knowledge in academic browsing, one can confidently guess that the webpages at the URLs (1), (2), and (3) are unlikely to be researcher homepages. Similarly, among the URLs (4), and (5), while the former seems to be a homepage, the latter seems to lead to a course page. The above conjectures are based on the presumption that the URL strings are not \"arbitrary\", but, instead conventions are observed that are indicative of the target content at the URL. For instance, in the previous examples, words such as \"projects\", \"events\", alphanumeric patterns of the terms in the URL indicate that the URLs, (1), (2), (3) and (5) are most possibly not researcher homepages.Treating \"/\" as delimiters, we extract features from the URL string following the domain name of a webpage. The list of all unigrams and bigrams from URL strings that occur more than thrice in the training dataset, comprise the URL-term dictionary. For terms in the URL not present in this dictionary, we look for their presence in WordNet to check if they are common words or proper nouns. WordNet is a large, lexical database of nouns, verbs, adjectives and adverbs for English, organized as a concept graph .In addition, we capture the surface patterns of the URLs including the presence of hyphenated or underscored words, alphanumeric patterns, long words (i.e., words having greater than 30 characters), question marks and the presence of characters such as tilde. These features are designed to filter out the URLs that commonly represent course pages, announcements, calendars and other auto-generated content. For instance, a typical homepage URL string in Computer Science departments has the name of the researcher following the \u223c character after the domain name (e.g., http://people.cs.umass.edu/\u223cmccallum/). This pattern is usually captured by our \"TILDENONDICT\" feature, where mccallum is a non-dictionary term. Partial sets of extracted features are shown along with the URLs listed in .The above sets of features perform very well on the training datasets as shown Section 5. We, therefore, do not study other complicated, problem-specific feature design or feature selection. Instead our focus in this work is to study how these classifiers perform \"in the wild\". We also note here that, a classifier that can make accurate predictions using URL features can be quite beneficial from the perspective of efficiency for a focused crawler. A crawler can potentially bypass examining the content of a page if a confident decision can be made based on the URL string. However, we may not be able to always extract features from the URL strings. For instance, consider the following URLs from our crawls:http://john.blitzer.com/ http://clgiles.ist.psu.edu/ http://ben.adida.net/ In these cases, it is not clear from the URL string that the target content refers to academic homepages. Even if complicated name-extraction based features were designed for the above cases, it is rare to find academic homepages with '.com' and '.net' domain suffixes. Based on the URL alone, we cannot be confident if the target content is an academic homepage or a company/personal homepage. For the second case, 'clgiles' could refer to a machine name. In addition to the above cases, given that feature dictionaries typically comprise features that meet a frequency requirement, we may not be able to extract features for all URLs. In our training datasets (Section 5), we were unable to extract URL features for about 27% of the instances. Therefore, contentbased and URL features complement each other while identifying homepage instances and a focused crawler might be required to use either or both of these sets of features.\nHOMEPAGE CLASSIFICATION USING UNLABELED DATAWe show in our experiments (Section 5) that, although content-based features perform extremely well on the training datasets, they are not very successful on the validation and test sets that were collected from the current-day academic websites. On the other hand, URL features show good performance on both training and validation datasets.  However, as pointed out in the previous section, we may not be able to extract URL features for all instances and it is, therefore, imperative to have an accurate content-based classifier as well. We now address the questions: Can we adapt the contentbased classifier to perform well in the deployment environment with the help of the URL-based classifier? Can the two classifiers \"teach\" each other so as to perform better in the new environment, using the co-training approach? Since the URL and content features provide evidence for classifying a webpage instance independently, intuitively, it appears possible that there are instances that the URL classifier makes mistakes on, which the content-based classifier identifies correctly and vice versa.Blum and Mitchell proposed co-training in context of webpage classification . In their datasets, webpages are representable in terms of two distinct views: using terms on webpages and terms in the anchor text of hyperlinks pointing to these pages. When few labeled examples are available for training, they showed that co-training could be used to obtain predictions on the unlabeled data to enlarge the training set. Blum and Mitchell's experiments and the subsequent experiments by Nigam and Ghani  showed that when a natural split of features is available, co-training that explicitly leverages this split has the potential to outperform classifiers that do not.We study the applicability and extension of co-training for our problem. Although the essential motivation is to make use of the naturally available feature split and enable classifiers to learn from each other, we highlight the following aspects of our setup: Previous studies and benefits from co-training were illustrated on datasets where the unlabeled data is arguably from a similar distribution. That is, the positive and negative instances in the labeled datasets are representative of those in the unlabeled data. This is in contrast to our case, where our positive class is fairly welldefined (homepages), whereas the negative class is described in terms of \"not positive\". More precisely, although our training dataset has examples for the negative class, webpages encountered during the crawls can belong to types not encountered in the labeled data. We present an error analysis in Section 5, that illustrates the \"new\" types of webpages encountered in our crawl, potentially causing the pre-trained content-based classifiers to underperform during crawling.The number of negative instances encountered during our crawls is higher in comparison with the number of positive instances. While this aspect was noticed during our experiments, a previous estimation experiment using markrecapture methods had indicated that academic homepages comprise a minute fraction of the Web . We can expect this imbalance to become more prominent as more examples are sampled over the co-training rounds. In the algorithm studied by Blum and Mitchell, the ratio between the number of positive and negative instances added from the unlabeled data is maintained to be the same as that in the training dataset during each iteration of co-training . We argue that avoiding this constraint is better in our scenario since we want the datasets to be more representative of the changing distribution.Most classification algorithms are sensitive to the number of positive and negative instances available in the training data and are known to learn biased classifiers in case of severe imbalance . We employ the idea of altering the mis-classification costs for the underlying classifiers during each round of co-training to handle this problem. For example, if the training dataset has 10 positive and 100 negative instances, we can set the penalty incurred on making mistakes on a negative instance to be 1 10 th of the penalty incurred on making mistakes on a positive instance. For most implementations of classification algorithms, the misclassification costs can be specified as a parameter during the training process .Our co-training setup is detailed in Algorithm 1. L and U represent the labeled and unlabeled datasets, respectively, available at each iteration. They comprise instances with both the views (content-based and URL-based feature sets). For a round of co-training, we train classifiers, C1 and C2, on the two available views, using misclassification costs, \u03c11 and \u03c12, respectively. Next, \"s\" number of examples are sampled without replacement into S from the unlabeled data and C1 and C2 are used to obtain predictions for these instances. The GetConf identEgs method is a generic placeholder that stands for a function that determines what instances from S are chosen for addition in subsequent rounds of co-training. We use the notation L + 1 to represent the positive instances in the set L1 whereas L 1 1 indicates that the view 1 (or feature set 1) of the examples in L1 is being used.Based on previous studies in co-training , we studied the following strategies for this function:\u2022 AddBoth: In this scheme, we add all examples from S that are labeled by C1 or C2 confidently to the training set for the next round. This approach is similar to selftraining used in semi-supervised learning where confidently predicted unlabeled instances are added to the training set for retraining the classifier in subsequent rounds . However, in contrast with self-training that uses a single view, in AddBoth, confident predictions are obtained from two sources (view 1 and 2) for addition into subsequent rounds.\u2022 AddCross: In this scheme, examples from S, confidently labeled by C1 are added to view 2 for the next 474 round and vice versa. That is, we use the examples confidently labeled by one classifier while training the other classifier in the next round. Cross-addition also seems resilient to handling the possibility of cascaded errors over the iterations. If a classifier makes a confident but incorrect prediction, we would like to avoid feeding this example in the next round to the same classifier, a common problem in self-training [45].\u2022 AddCrossRC: This scheme is similar to AddCross with the additional constraint on the number of positive and negative instances added in each round. This constraint was originally studied by Blum and Mitchell and ensures that the ratio of the number of positive and negative instances added in each round is the same as that in the initial labeled dataset .Algorithm 1 Procedure for Co-trainingCompute \u03c1 1 usingThe co-training algorithm is general and can be applied with any choice of classifiers on the two views. Blum and Mitchell provided a PAC-style analysis of co-training with probabilistic classifiers and showed that co-training works when the assumptions on sufficiency and independence are met. That is, each view should be sufficient to predict the class label, and the two views are independent given the class label. Recent studies have proposed relaxed criteria under which co-training techniques still work . However, in practice, it is tricky to judge if co-training works for a problem and to verify if the assumptions are satisfied . These questions are more relevant in context of recent research in obtaining two views from a single view when two views are not naturally available for applying co-training . With this context, we now discuss our formulation of the effect obtained with co-training, in terms of a loss function. This formulation allows us to track whether the co-training process is beneficial for a given problem, even without the use of a validation dataset.\nLearning Conforming Predictors on Unlabeled DataWe assume that classifiers, C1 and C2 trained on the two views are parameterized in terms of their weight vectors, w1 and w2. Most classification algorithms e.g., Support Vector Machines (SVM) and Maximum Entropy (MaxEnt), output weight vectors capturing the importance of each feature as part of the training process .One can expect co-training to benefit a classification problem if one classifier (say, C1) can \"guide\" the other (C2) on examples that the latter makes mistakes on. This guidance is provided by adding examples confidently labeled by C1 to the subsequent round of training C2. This observation hints at the possibility of directly manipulating C2, based on C1's prediction for an example that C2 is not confident about. This effect can be achieved by optimizing a function that directly captures the mismatch in the predictions of the two classifiers.Elaborating further, given that the concept classes, \"positive\" and \"negative\" are still the same on unlabeled data, if C1 and C2 are accurate, they would make similar predictions on the unlabeled data. This intuition is the basis for \"consensus maximization\" widely adopted in multiview learning, of which co-training is a special case with two views . The mismatch in predictions by C1 and C2 on unlabeled data can be quantified using a loss function. The squared error loss function commonly used in machine learning captures this loss as:The above formulation captures the average squared-difference in predictions from the two views on unlabeled data. w1 and w2 correspond to the parameter vectors corresponding to C1 and C2, respectively, and u refers to an example from U , having two views, u1 and u2. For a given example, u = (u1, u2), the functions, f1 and f2 act on u1 and u2 respectively, and make the predictions from C1 and C2 comparable. These functions could be generic (e.g. a function that outputs the probability that the instance is positive) or classifier-dependent (for e.g. a function that outputs scaled distances from the separating hyperplane in case of Support Vector Machines). Minimizing L corresponds to adjusting the weight vectors, w1 and w2, so that they make similar predictions on U . In contrast with multiview learning methods, where learning the classifiers is folded into a global objective function in sophisticated ways , we adopt a simpler approach that works off the initial parameter vectors and iteratively modifies them in a \"co-training like\" manner. Note that this initialization plays a crucial role in avoiding trivial solutions (such as w1, w2 = 0) that are potentially possible since the loss is optimized only on unlabeled instances. Our proposed technique for obtaining the \"pair of conforming classifiers\" is described in Algorithm 2.In Algorithm 2, we start with the original parameter vectors w1 and w2 from classifiers C1 and C2, respectively, and iteratively adjust these vectors so that the values of f1(w1, u1) and f2(w2, u2) look similar for all u \u2208 U . The input parameter, #oIters, refers to the number of times the inner loop comprising of the two gradient descent steps is executed, where as, the #iIters, and \u03b1 are parameters for the gradient descent algorithm. Overall, the values of #oIters, #iIters, and \u03b1 control the rate of convergence of the algorithm and can be set experimentally. These parameters can be set based on the base classifiers used, noting when the decrease in the objective function value is below a threshold. Adaptive tuning of these parameters by tracking the change in the value of the objective function in every iteration is a subject for future study .In each iteration, we employ mini-batch gradient descent to minimize the loss function, once w.r.t. w1 and next w.r.t. w2. The mini-batch gradient descent algorithm is a hybrid approach often used for large-scale machine learning problems. This approach combines the best of stochastic (on- line) gradient descent and batch gradient descent to obtain fast convergence during optimization by running gradient descent on small batches of randomly selected examples .In our algorithm, in each iteration, a small batch of instances are randomly sampled from the unlabeled data, U and the loss function defined using instances for which w1 makes confident predictions from this sampled set. This loss is minimized using gradient descent to adjust w2. A similar process is then applied for adjusting w1 using confident predictions from w2. In effect, as the algorithm proceeds, we are adjusting the parameters of each classifier so that it makes predictions that are aligned with those of the other classifier's confident predictions. Upon convergence, both w1 and w2 are adjusted so that they make conforming predictions on the unlabeled data.In our experiments, we used the differentiable, logistic sigmoid function for f1 and f2. Typically, classifiers use the parameter vector, w, for computing decision values for each instance. That is, given an instance x, the dot product value, w, x , is used for determining the label assignment for the instance. This value can be 'squashed' to a number between 0 and 1 indicating that the probability that instance has a particular label with the logistic function [3]: P (t) = 1 1 + e \u2212t with dP (t) dt = P (t) \u00b7 (1 \u2212 P (t)) Given, the simple form for the derivative, we can directly use the values of f1 and f2 (that we compute anyway), for computing the gradients in Algorithm 2. Although the effect obtained by Algorithm 2 is similar to that of co-training, the conformity loss directly measures the effect of co-training as it is being applied. In contrast, Algorithm 1 is typically terminated either when no more examples are available or by tracking the performance on a validation dataset.We provide a preliminary, experimental demonstration of the connection between co-training and our proposed algorithm in Section 5. A more detailed analysis, study of other choices for the loss function L and the functions, f1 and f2, are a subject of future work. Nevertheless, quantifying the discrepancy in predictions from the two views and an algorithm to directly address this aspect is an exciting step in understanding when co-training works. We show in Section 5 that our method can be used in lieu of a validation dataset for tracking the performance of co-training.\nEXPERIMENTSWe discuss 3 types of experiments: First, we study the performance of content-based and URL-based features on the training and validation datasets. Second, we show that co-training can successfully address the problem of mismatch in the training and deployment environments for homepage classification. Finally, we show that our proposed algorithm (Algorithm 2), achieves the same effect as co-training. Researcher homepage classification using unlabeled data", "text": "DBLP:conf/www/2013 22nd International World Wide Web Conference, WWW '13, Rio de Janeiro, Brazil, May 13-17, 2013 471\u2013482 International World Wide Web Conferences Steering Committee / ACM 2013 https://doi.org/10.1145/2488388.2488430 10.1145/2488388.2488430 https://dblp.org/rec/conf/www/GollapalliCMG13.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GollapalliCMG13 inproceedings ['Sujatha Das Gollapalli', 'Cornelia Caragea', 'Prasenjit Mitra', 'C. Lee Giles'] [] WWW 2013.wwwconf_conference-2013.41 1603662470.0 ABSTRACTA classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changed content on the Web? We investigate this question in the context of researcher homepage crawling.We show experimentally that classifiers trained on existing datasets for homepage identification underperform while classifying \"irrelevant\" pages on current-day academic websites. As an alternative to obtaining datasets to retrain the classifier for the new content, we propose to use effectively unlimited amounts of unlabeled data readily available from these websites in a co-training scenario. To this end, we design novel URL-based features and use them in conjunction with content-based features as complementary views of the data to obtain remarkable improvements in accurately identifying homepages from the current-day university websites.In addition, we propose a novel technique for \"learning a conforming pair of classifiers\" using mini-batch gradient descent. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We demonstrate that tuning the classifiers so that they make \"similar\" predictions on unlabeled data strongly corresponds to the effect achieved by co-training algorithms. We argue that this loss formulation provides insight into understanding the co-training process and can be used even in absence of a validation set.\nCategories and Subject Descriptors\nH.3.3 [Information Search and Retrieval]: Miscellaneous\nGeneral TermsAlgorithms Keywords co-training, consensus maximization, gradient descent\nMOTIVATIONProfessional homepages of researchers, which typically summarize research interests, publications and other metadata related to researchers, are shown to be rich sources of information for digital libraries . Researchers' homepages    Copyright is held by the International World Wide Web Conference Committee (IW3C2). IW3C2 reserves the right to provide a hyperlink to the author's site if the Material is used in electronic media.WWW 2013, May 13-17, 2013, Rio de Janeiro, Brazil.   ACM 978-1-4503-2035 (also referred to as academic homepages or simply homepages in this paper) have been successfully employed in tasks such as expertise search , extraction of academic networks, author profile extraction and disambiguation  as they provide crucial evidence for improving these tasks in digital libraries.Furthermore, digital library systems such as CiteSeer 1 , ArnetMiner 2 , and Google Scholar 3 are primarily interested in obtaining and tracking researchers' homepages in order to retrieve appropriate scientific research publications. Given the infeasibility of collecting the entire content on the Web, a focused crawler aims to minimize the use of network bandwidth and hardware by selectively crawling only pages relevant to a (specified) set of topics . A key component for such a crawler is a classification module that identifies whether a webpage being accessed during the crawl process is potentially useful to the collection. For digital libraries, the \"yield\" of such crawlers highly depends on the accuracy of researcher homepage classification.Supervised methods for learning homepage classifiers rely on the availability of large amounts of labeled data. A widely used labeled dataset for webpage classification is the WebKB dataset 4 that is built in 1997. However, due to recent changes in the information content on academic websites, this dataset is becoming outdated. For example, there are now pages on academic websites that are related to various activities such as invited talks, news, events that do not occur in the WebKB dataset. We refer to university, department and research center websites as \"academic websites\" in this paper. Compared to few decades back, it is easier now to find faculty information, links to their homepages, information on research groups, course related notes and documents, and research papers from academic websites. Similarly, job postings, seminar announcements and notices are also being uploaded onto departmental websites in recent times .How can a homepage classifier keep up in the face of rapidly changing types of pages on the Web? Specifically, given a classifier that identifies homepages with reasonable accuracy (as measured on the training datasets), how does it perform in the potentially different deployment environment? Semi-supervised methods that can exploit large amounts of unlabeled data together with limited amounts of labeled data for learning accurate classifiers have received significant attention in recent research in machine learning due to Against this background, one question that can be raised is: Can we design techniques to effectively adjust the previouslytrained classifier to the changed content on the Web, while minimizing the human effort required for labeling new data, and under what conditions, can such an adjustment be possible? The research that we describe in this paper addresses specifically this question.Contributions and Organization. We present two approaches to researcher homepage classification using unlabeled data readily available from academic websites. More precisely, we first adapt the well-known co-training approach  to reflect the change in the data distribution over time. Second, we design an iterative algorithm based on the Minibatch Gradient Descent technique for learning a conforming pair of predictors using two different views of the data. We restrict ourselves to homepages of researchers in Computer Science since training datasets are available for this discipline. To the best of our knowledge, the problem of researcher homepage classification using unlabeled data available during focused crawling was not addressed in previous research. The contributions of this work are as follows:\u2022 We show that with the classifiers trained on existing datasets for researcher homepage classification we incorrectly identify pages of types not seen in the training datasets as homepages. This results in unacceptable yield from the perspective of a focused crawler.\u2022 We design novel features based on URL surface patterns and terms to complement term and HTML features extracted from the content of homepages and show that these two sets of features can be treated as two \"views\" for a researcher homepage instance.\u2022 We show that the URL and content-based views can be used successfully in a co-training setup to adapt classifiers to the changing academic environments using unlabeled data. This finding enables us to accurately crawl the new academic website content without having to label a new dataset for re-training the classifier.\u2022 Inspired by the success of co-training on this problem, we investigate loss functions that capture the disparity between the classifiers' predictions on the two views afforded by co-training. We design an iterative algorithm based on the Mini-batch Gradient Descent technique for minimizing this loss and learning a conforming pair of predictors with the two views.\u2022 Finally, we show that minimizing our proposed loss function on unlabeled data closely corresponds to the effect demonstrated by co-training techniques. We posit that this loss can, therefore, be used as a measure, for tracking the progress of co-training schemes even in the absence of a validation dataset.Although in this paper we focus on the design of accurate approaches for researcher homepage classification, our objective is to integrate this classification component in the context of focused crawling, to improve retrieval and indexing of scientific publications in digital libraries such as CiteSeer and ArnetMiner. In these usage environments, since maintaining up-to-date collections of research literature is of primary importance, having an accurate list of homepage URLs for frequent, periodic tracking is both feasible and scalable, compared to examining the entire content at academic websites each time. The rest of this paper is organized as follows: We briefly summarize closely related work in Section 2. Researcher homepage classification is discussed in Section 3. We elaborate on details of our co-training experiments and learning conforming predictor pairs in Section 4. Experimental setup, datasets and results are discussed in Section 5, followed by a summary and future extensions to our work.\nRELATED WORKResearcher homepage classification is a well-studied webpage classification problem in context of digital libraries such as CiteSeer  and ArnetMiner . Typically, contentbased term features and HTML structure-based features are used for classifying webpages . We propose the use of URL features as additional evidence for homepage identification. A smaller set (compared to ours) of URL-based features (presence of part of the name, presence of the character '\u223c', etc.), was used in isolating homepages among the search engine results for researcher name queries by Tang, et  al [40]. URL-based features are widely used in tasks related to the Web. For example, URL strings were used for extracting rules to solve the webpage de-duplication problem . Shih and Karger [38]  used URL features, the visual placements of links in the referring pages to a URL for improving applications such as ad-blocking and recommendation, whereas a preliminary study by Kan and Thi [20]  illustrates the use of URLs in performing fast webpage classification.The problem of collecting a high-quality researcher homepage collection was studied for Japanese websites by Wang, et al. using on-page and anchor text features . Tang, et al. studied homepage acquisition from search engine results using researcher names as queries . In contrast, we seek to apply focused crawling using a seed list of academic websites (where researcher homepages are typically hosted) to acquire such a collection. Focused crawling first proposed by Bra, et al. is a rich area of research on the Web . Chakrabarti, et al. [7]  present a discussion on the main components involved in building a focused crawler. Although focused crawling is our motivating application, this paper deals with the classifier component of the crawler and not with the crawler itself.We show that the focused crawling scenario presents novel challenges in using a pre-trained homepage classifier in identifying relevant pages. Specifically, the classifier needs to be attuned to the changing types of pages on the Web. Cotraining is proposed as a solution for addressing this challenge for homepage classification. Blum and Mitchell [4] first proposed co-training, an approach for semi-supervised learning when the number of labeled examples available for training is limited, and applied it to webpage classification. This approach requires having two views of features for the instances and has been shown to work well when the two views satisfy certain assumptions on \"sufficiency\" and \"independence\" . Recent research addresses techniques for decomposing the feature set into two views when such a split is not naturally available .Multiview learning (of which co-training is a special case, with two views) is typically addressed by maximizing \"consensus\" or agreement among the different views . Most solutions to multiview learning tend to frame the problem in terms of a global optimization problem and simulta-472 neously learn classifiers for all the underlying views. In some cases, the solutions depend on underlying classification algorithm used . Although our proposed algorithm based on mini-batch gradient descent seeks to maximize consensus as well, our approach is a generic technique assuming only that the underlying classifiers output initial \"parameter vectors\" that are altered using a simple, iterative algorithm.\nFEATURES FOR HOMEPAGE CLASSIFICATIONWebpage or text classification is typically handled using \"bag of words\" approaches. Specifically, the frequently occurring and discerning terms are collected from training data to form a feature dictionary that is used to represent instances as normalized term frequency or TFIDF vectors . Homepage classification was previously studied as a text classification problem using term features . Previous work on the same problem also used other content-based features related to the HTML structure of the page such as the number of images/tables on the page, and the terms commonly found in anchor text of homepages . In this study, we extracted content-based and URL-based features from our training sets. These features and the size of feature sets are summarized in . The term dictionaries contain terms that occur in at least three documents (i.e., webpages) and at least five times in the training set.In addition to term dictionaries, we hypothesize that the URL strings of homepages can provide additional evidence for identifying homepages. Hence, we design novel URLbased features based on surface patterns and presence in WordNet 5 . The URL-based features are explained in the next subsection.  \nURL strings as additional evidenceThe idea of using URL strings in academic homepage identification comes from an error analysis of a crawl obtained with the content-based classifier. Consider some example URLs we encountered in our crawl listed in .With some knowledge in academic browsing, one can confidently guess that the webpages at the URLs (1), (2), and (3) are unlikely to be researcher homepages. Similarly, among the URLs (4), and (5), while the former seems to be a homepage, the latter seems to lead to a course page. The above conjectures are based on the presumption that the URL strings are not \"arbitrary\", but, instead conventions are observed that are indicative of the target content at the URL. For instance, in the previous examples, words such as \"projects\", \"events\", alphanumeric patterns of the terms in the URL indicate that the URLs, (1), (2), (3) and (5) are most possibly not researcher homepages.Treating \"/\" as delimiters, we extract features from the URL string following the domain name of a webpage. The list of all unigrams and bigrams from URL strings that occur more than thrice in the training dataset, comprise the URL-term dictionary. For terms in the URL not present in this dictionary, we look for their presence in WordNet to check if they are common words or proper nouns. WordNet is a large, lexical database of nouns, verbs, adjectives and adverbs for English, organized as a concept graph .In addition, we capture the surface patterns of the URLs including the presence of hyphenated or underscored words, alphanumeric patterns, long words (i.e., words having greater than 30 characters), question marks and the presence of characters such as tilde. These features are designed to filter out the URLs that commonly represent course pages, announcements, calendars and other auto-generated content. For instance, a typical homepage URL string in Computer Science departments has the name of the researcher following the \u223c character after the domain name (e.g., http://people.cs.umass.edu/\u223cmccallum/). This pattern is usually captured by our \"TILDENONDICT\" feature, where mccallum is a non-dictionary term. Partial sets of extracted features are shown along with the URLs listed in .The above sets of features perform very well on the training datasets as shown Section 5. We, therefore, do not study other complicated, problem-specific feature design or feature selection. Instead our focus in this work is to study how these classifiers perform \"in the wild\". We also note here that, a classifier that can make accurate predictions using URL features can be quite beneficial from the perspective of efficiency for a focused crawler. A crawler can potentially bypass examining the content of a page if a confident decision can be made based on the URL string. However, we may not be able to always extract features from the URL strings. For instance, consider the following URLs from our crawls:http://john.blitzer.com/ http://clgiles.ist.psu.edu/ http://ben.adida.net/ In these cases, it is not clear from the URL string that the target content refers to academic homepages. Even if complicated name-extraction based features were designed for the above cases, it is rare to find academic homepages with '.com' and '.net' domain suffixes. Based on the URL alone, we cannot be confident if the target content is an academic homepage or a company/personal homepage. For the second case, 'clgiles' could refer to a machine name. In addition to the above cases, given that feature dictionaries typically comprise features that meet a frequency requirement, we may not be able to extract features for all URLs. In our training datasets (Section 5), we were unable to extract URL features for about 27% of the instances. Therefore, contentbased and URL features complement each other while identifying homepage instances and a focused crawler might be required to use either or both of these sets of features.\nHOMEPAGE CLASSIFICATION USING UNLABELED DATAWe show in our experiments (Section 5) that, although content-based features perform extremely well on the training datasets, they are not very successful on the validation and test sets that were collected from the current-day academic websites. On the other hand, URL features show good performance on both training and validation datasets.  However, as pointed out in the previous section, we may not be able to extract URL features for all instances and it is, therefore, imperative to have an accurate content-based classifier as well. We now address the questions: Can we adapt the contentbased classifier to perform well in the deployment environment with the help of the URL-based classifier? Can the two classifiers \"teach\" each other so as to perform better in the new environment, using the co-training approach? Since the URL and content features provide evidence for classifying a webpage instance independently, intuitively, it appears possible that there are instances that the URL classifier makes mistakes on, which the content-based classifier identifies correctly and vice versa.Blum and Mitchell proposed co-training in context of webpage classification . In their datasets, webpages are representable in terms of two distinct views: using terms on webpages and terms in the anchor text of hyperlinks pointing to these pages. When few labeled examples are available for training, they showed that co-training could be used to obtain predictions on the unlabeled data to enlarge the training set. Blum and Mitchell's experiments and the subsequent experiments by Nigam and Ghani  showed that when a natural split of features is available, co-training that explicitly leverages this split has the potential to outperform classifiers that do not.We study the applicability and extension of co-training for our problem. Although the essential motivation is to make use of the naturally available feature split and enable classifiers to learn from each other, we highlight the following aspects of our setup: Previous studies and benefits from co-training were illustrated on datasets where the unlabeled data is arguably from a similar distribution. That is, the positive and negative instances in the labeled datasets are representative of those in the unlabeled data. This is in contrast to our case, where our positive class is fairly welldefined (homepages), whereas the negative class is described in terms of \"not positive\". More precisely, although our training dataset has examples for the negative class, webpages encountered during the crawls can belong to types not encountered in the labeled data. We present an error analysis in Section 5, that illustrates the \"new\" types of webpages encountered in our crawl, potentially causing the pre-trained content-based classifiers to underperform during crawling.The number of negative instances encountered during our crawls is higher in comparison with the number of positive instances. While this aspect was noticed during our experiments, a previous estimation experiment using markrecapture methods had indicated that academic homepages comprise a minute fraction of the Web . We can expect this imbalance to become more prominent as more examples are sampled over the co-training rounds. In the algorithm studied by Blum and Mitchell, the ratio between the number of positive and negative instances added from the unlabeled data is maintained to be the same as that in the training dataset during each iteration of co-training . We argue that avoiding this constraint is better in our scenario since we want the datasets to be more representative of the changing distribution.Most classification algorithms are sensitive to the number of positive and negative instances available in the training data and are known to learn biased classifiers in case of severe imbalance . We employ the idea of altering the mis-classification costs for the underlying classifiers during each round of co-training to handle this problem. For example, if the training dataset has 10 positive and 100 negative instances, we can set the penalty incurred on making mistakes on a negative instance to be 1 10 th of the penalty incurred on making mistakes on a positive instance. For most implementations of classification algorithms, the misclassification costs can be specified as a parameter during the training process .Our co-training setup is detailed in Algorithm 1. L and U represent the labeled and unlabeled datasets, respectively, available at each iteration. They comprise instances with both the views (content-based and URL-based feature sets). For a round of co-training, we train classifiers, C1 and C2, on the two available views, using misclassification costs, \u03c11 and \u03c12, respectively. Next, \"s\" number of examples are sampled without replacement into S from the unlabeled data and C1 and C2 are used to obtain predictions for these instances. The GetConf identEgs method is a generic placeholder that stands for a function that determines what instances from S are chosen for addition in subsequent rounds of co-training. We use the notation L + 1 to represent the positive instances in the set L1 whereas L 1 1 indicates that the view 1 (or feature set 1) of the examples in L1 is being used.Based on previous studies in co-training , we studied the following strategies for this function:\u2022 AddBoth: In this scheme, we add all examples from S that are labeled by C1 or C2 confidently to the training set for the next round. This approach is similar to selftraining used in semi-supervised learning where confidently predicted unlabeled instances are added to the training set for retraining the classifier in subsequent rounds . However, in contrast with self-training that uses a single view, in AddBoth, confident predictions are obtained from two sources (view 1 and 2) for addition into subsequent rounds.\u2022 AddCross: In this scheme, examples from S, confidently labeled by C1 are added to view 2 for the next 474 round and vice versa. That is, we use the examples confidently labeled by one classifier while training the other classifier in the next round. Cross-addition also seems resilient to handling the possibility of cascaded errors over the iterations. If a classifier makes a confident but incorrect prediction, we would like to avoid feeding this example in the next round to the same classifier, a common problem in self-training [45].\u2022 AddCrossRC: This scheme is similar to AddCross with the additional constraint on the number of positive and negative instances added in each round. This constraint was originally studied by Blum and Mitchell and ensures that the ratio of the number of positive and negative instances added in each round is the same as that in the initial labeled dataset .Algorithm 1 Procedure for Co-trainingCompute \u03c1 1 usingThe co-training algorithm is general and can be applied with any choice of classifiers on the two views. Blum and Mitchell provided a PAC-style analysis of co-training with probabilistic classifiers and showed that co-training works when the assumptions on sufficiency and independence are met. That is, each view should be sufficient to predict the class label, and the two views are independent given the class label. Recent studies have proposed relaxed criteria under which co-training techniques still work . However, in practice, it is tricky to judge if co-training works for a problem and to verify if the assumptions are satisfied . These questions are more relevant in context of recent research in obtaining two views from a single view when two views are not naturally available for applying co-training . With this context, we now discuss our formulation of the effect obtained with co-training, in terms of a loss function. This formulation allows us to track whether the co-training process is beneficial for a given problem, even without the use of a validation dataset.\nLearning Conforming Predictors on Unlabeled DataWe assume that classifiers, C1 and C2 trained on the two views are parameterized in terms of their weight vectors, w1 and w2. Most classification algorithms e.g., Support Vector Machines (SVM) and Maximum Entropy (MaxEnt), output weight vectors capturing the importance of each feature as part of the training process .One can expect co-training to benefit a classification problem if one classifier (say, C1) can \"guide\" the other (C2) on examples that the latter makes mistakes on. This guidance is provided by adding examples confidently labeled by C1 to the subsequent round of training C2. This observation hints at the possibility of directly manipulating C2, based on C1's prediction for an example that C2 is not confident about. This effect can be achieved by optimizing a function that directly captures the mismatch in the predictions of the two classifiers.Elaborating further, given that the concept classes, \"positive\" and \"negative\" are still the same on unlabeled data, if C1 and C2 are accurate, they would make similar predictions on the unlabeled data. This intuition is the basis for \"consensus maximization\" widely adopted in multiview learning, of which co-training is a special case with two views . The mismatch in predictions by C1 and C2 on unlabeled data can be quantified using a loss function. The squared error loss function commonly used in machine learning captures this loss as:The above formulation captures the average squared-difference in predictions from the two views on unlabeled data. w1 and w2 correspond to the parameter vectors corresponding to C1 and C2, respectively, and u refers to an example from U , having two views, u1 and u2. For a given example, u = (u1, u2), the functions, f1 and f2 act on u1 and u2 respectively, and make the predictions from C1 and C2 comparable. These functions could be generic (e.g. a function that outputs the probability that the instance is positive) or classifier-dependent (for e.g. a function that outputs scaled distances from the separating hyperplane in case of Support Vector Machines). Minimizing L corresponds to adjusting the weight vectors, w1 and w2, so that they make similar predictions on U . In contrast with multiview learning methods, where learning the classifiers is folded into a global objective function in sophisticated ways , we adopt a simpler approach that works off the initial parameter vectors and iteratively modifies them in a \"co-training like\" manner. Note that this initialization plays a crucial role in avoiding trivial solutions (such as w1, w2 = 0) that are potentially possible since the loss is optimized only on unlabeled instances. Our proposed technique for obtaining the \"pair of conforming classifiers\" is described in Algorithm 2.In Algorithm 2, we start with the original parameter vectors w1 and w2 from classifiers C1 and C2, respectively, and iteratively adjust these vectors so that the values of f1(w1, u1) and f2(w2, u2) look similar for all u \u2208 U . The input parameter, #oIters, refers to the number of times the inner loop comprising of the two gradient descent steps is executed, where as, the #iIters, and \u03b1 are parameters for the gradient descent algorithm. Overall, the values of #oIters, #iIters, and \u03b1 control the rate of convergence of the algorithm and can be set experimentally. These parameters can be set based on the base classifiers used, noting when the decrease in the objective function value is below a threshold. Adaptive tuning of these parameters by tracking the change in the value of the objective function in every iteration is a subject for future study .In each iteration, we employ mini-batch gradient descent to minimize the loss function, once w.r.t. w1 and next w.r.t. w2. The mini-batch gradient descent algorithm is a hybrid approach often used for large-scale machine learning problems. This approach combines the best of stochastic (on- line) gradient descent and batch gradient descent to obtain fast convergence during optimization by running gradient descent on small batches of randomly selected examples .In our algorithm, in each iteration, a small batch of instances are randomly sampled from the unlabeled data, U and the loss function defined using instances for which w1 makes confident predictions from this sampled set. This loss is minimized using gradient descent to adjust w2. A similar process is then applied for adjusting w1 using confident predictions from w2. In effect, as the algorithm proceeds, we are adjusting the parameters of each classifier so that it makes predictions that are aligned with those of the other classifier's confident predictions. Upon convergence, both w1 and w2 are adjusted so that they make conforming predictions on the unlabeled data.In our experiments, we used the differentiable, logistic sigmoid function for f1 and f2. Typically, classifiers use the parameter vector, w, for computing decision values for each instance. That is, given an instance x, the dot product value, w, x , is used for determining the label assignment for the instance. This value can be 'squashed' to a number between 0 and 1 indicating that the probability that instance has a particular label with the logistic function [3]: P (t) = 1 1 + e \u2212t with dP (t) dt = P (t) \u00b7 (1 \u2212 P (t)) Given, the simple form for the derivative, we can directly use the values of f1 and f2 (that we compute anyway), for computing the gradients in Algorithm 2. Although the effect obtained by Algorithm 2 is similar to that of co-training, the conformity loss directly measures the effect of co-training as it is being applied. In contrast, Algorithm 1 is typically terminated either when no more examples are available or by tracking the performance on a validation dataset.We provide a preliminary, experimental demonstration of the connection between co-training and our proposed algorithm in Section 5. A more detailed analysis, study of other choices for the loss function L and the functions, f1 and f2, are a subject of future work. Nevertheless, quantifying the discrepancy in predictions from the two views and an algorithm to directly address this aspect is an exciting step in understanding when co-training works. We show in Section 5 that our method can be used in lieu of a validation dataset for tracking the performance of co-training.\nEXPERIMENTSWe discuss 3 types of experiments: First, we study the performance of content-based and URL-based features on the training and validation datasets. Second, we show that co-training can successfully address the problem of mismatch in the training and deployment environments for homepage classification. Finally, we show that our proposed algorithm (Algorithm 2), achieves the same effect as co-training. Researcher homepage classification using unlabeled data"}, "2018.wwwconf_conference-2018c.298": {"doc_id": "2018.wwwconf_conference-2018c.298", "default_text": "DBLP:conf/www/2018c Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 1507\u20131514 ACM 2018 https://doi.org/10.1145/3184558.3191600 10.1145/3184558.3191600 https://dblp.org/rec/conf/www/SodermanKPGG18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/SodermanKPGG18 inproceedings ['Sean Soderman', 'Anusha Kola', 'Maksim Podkorytov', 'Michael Geyer', 'Michael N. Gubanov'] [] WWW 2018.wwwconf_conference-2018c.298 1618560252.0 ABSTRACTVariety of Big data ] is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information.Here we describe Hybrid.AI, a learning search engine for largescale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs)  to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain. Hybrid.AI: A Learning Search Engine for Large-scale Structured Data", "text": "DBLP:conf/www/2018c Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018 1507\u20131514 ACM 2018 https://doi.org/10.1145/3184558.3191600 10.1145/3184558.3191600 https://dblp.org/rec/conf/www/SodermanKPGG18.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/SodermanKPGG18 inproceedings ['Sean Soderman', 'Anusha Kola', 'Maksim Podkorytov', 'Michael Geyer', 'Michael N. Gubanov'] [] WWW 2018.wwwconf_conference-2018c.298 1618560252.0 ABSTRACTVariety of Big data ] is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information.Here we describe Hybrid.AI, a learning search engine for largescale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs)  to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain. Hybrid.AI: A Learning Search Engine for Large-scale Structured Data"}, "2006.wwwconf_conference-2006.195": {"doc_id": "2006.wwwconf_conference-2006.195", "default_text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 1041\u20131042 ACM 2006 https://doi.org/10.1145/1135777.1136005 10.1145/1135777.1136005 https://dblp.org/rec/conf/www/GonzlezMMN06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GonzlezMMN06 inproceedings ['Iv\u00e1n Gonzlez', 'Adam Marcus', 'Daniel N. Meredith', 'Linda A. Nguyen'] [] WWW 2006.wwwconf_conference-2006.195 1541519827.0 ABSTRACTThe web crawler space is often delimited into two general areas: full-web crawling and focused crawling. We present netSifter, a crawler system which integrates features from these two areas to provide an effective mechanism for webscale crawling. netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website. These algorithms score individual web pages to determine the general utility of the overall website. In doing so, netSifter can formulate an indepth opinion of a website (and the entirety of its web pages) with a relative minimum of work. netSifter is then able to bias the future efforts of its crawl towards higher quality websites, and away from the myriad of low quality websites and crawler traps that litter the World Wide Web. Effective web-scale crawling through website analysis", "text": "DBLP:conf/www/2006 Proceedings of the 15th international conference on World Wide Web, WWW 2006, Edinburgh, Scotland, UK, May 23-26, 2006 1041\u20131042 ACM 2006 https://doi.org/10.1145/1135777.1136005 10.1145/1135777.1136005 https://dblp.org/rec/conf/www/GonzlezMMN06.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/GonzlezMMN06 inproceedings ['Iv\u00e1n Gonzlez', 'Adam Marcus', 'Daniel N. Meredith', 'Linda A. Nguyen'] [] WWW 2006.wwwconf_conference-2006.195 1541519827.0 ABSTRACTThe web crawler space is often delimited into two general areas: full-web crawling and focused crawling. We present netSifter, a crawler system which integrates features from these two areas to provide an effective mechanism for webscale crawling. netSifter utilizes a combination of page-level analytics and heuristics which are applied to a sample of web pages from a given website. These algorithms score individual web pages to determine the general utility of the overall website. In doing so, netSifter can formulate an indepth opinion of a website (and the entirety of its web pages) with a relative minimum of work. netSifter is then able to bias the future efforts of its crawl towards higher quality websites, and away from the myriad of low quality websites and crawler traps that litter the World Wide Web. Effective web-scale crawling through website analysis"}, "2010.wwwconf_conference-2010.62": {"doc_id": "2010.wwwconf_conference-2010.62", "default_text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 611\u2013620 ACM 2010 https://doi.org/10.1145/1772690.1772753 10.1145/1772690.1772753 https://dblp.org/rec/conf/www/LeiCYKFZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeiCYKFZ10 inproceedings ['Tao Lei', 'Rui Cai', 'Jiang-Ming Yang', 'Yan Ke', 'Xiaodong Fan', 'Lei Zhang'] [] WWW 2010.wwwconf_conference-2010.62 1541519829.0 ABSTRACTDuplicate URLs have brought serious troubles to the whole pipeline of a search engine, from crawling, indexing, to result serving. URL normalization is to transform duplicate URLs to a canonical form using a set of rewrite rules. Nowadays URL normalization has attracted significant attention as it is lightweight and can be flexibly integrated into both the online (e.g. crawling) and the offline (e.g. index compression) parts of a search engine. To deal with a large scale of websites, automatic approaches are highly desired to learn rewrite rules for various kinds of duplicate URLs. In this paper, we rethink the problem of URL normalization from a global perspective and propose a pattern treebased approach, which is remarkably different from existing approaches. Most current approaches learn rewrite rules by iteratively inducing local duplicate pairs to more general forms, and inevitably suffer from noisy training data and are practically inefficient. Given a training set of URLs partitioned into duplicate clusters for a targeted website, we develop a simple yet efficient algorithm to automatically construct a URL pattern tree. With the pattern tree, the statistical information from all the training samples is leveraged to make the learning process more robust and reliable. The learning process is also accelerated as rules are directly summarized based on pattern tree nodes. In addition, from an engineering perspective, the pattern tree helps select deployable rules by removing conflicts and redundancies. An evaluation on more than 70 million duplicate URLs from 200 websites showed that the proposed approach achieves very promising performance, in terms of both de-duping effectiveness and computational efficiency. A pattern tree-based approach to learning URL normalization rules", "text": "DBLP:conf/www/2010 Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010 611\u2013620 ACM 2010 https://doi.org/10.1145/1772690.1772753 10.1145/1772690.1772753 https://dblp.org/rec/conf/www/LeiCYKFZ10.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeiCYKFZ10 inproceedings ['Tao Lei', 'Rui Cai', 'Jiang-Ming Yang', 'Yan Ke', 'Xiaodong Fan', 'Lei Zhang'] [] WWW 2010.wwwconf_conference-2010.62 1541519829.0 ABSTRACTDuplicate URLs have brought serious troubles to the whole pipeline of a search engine, from crawling, indexing, to result serving. URL normalization is to transform duplicate URLs to a canonical form using a set of rewrite rules. Nowadays URL normalization has attracted significant attention as it is lightweight and can be flexibly integrated into both the online (e.g. crawling) and the offline (e.g. index compression) parts of a search engine. To deal with a large scale of websites, automatic approaches are highly desired to learn rewrite rules for various kinds of duplicate URLs. In this paper, we rethink the problem of URL normalization from a global perspective and propose a pattern treebased approach, which is remarkably different from existing approaches. Most current approaches learn rewrite rules by iteratively inducing local duplicate pairs to more general forms, and inevitably suffer from noisy training data and are practically inefficient. Given a training set of URLs partitioned into duplicate clusters for a targeted website, we develop a simple yet efficient algorithm to automatically construct a URL pattern tree. With the pattern tree, the statistical information from all the training samples is leveraged to make the learning process more robust and reliable. The learning process is also accelerated as rules are directly summarized based on pattern tree nodes. In addition, from an engineering perspective, the pattern tree helps select deployable rules by removing conflicts and redundancies. An evaluation on more than 70 million duplicate URLs from 200 websites showed that the proposed approach achieves very promising performance, in terms of both de-duping effectiveness and computational efficiency. A pattern tree-based approach to learning URL normalization rules"}, "2003.wwwconf_conference-2003.3": {"doc_id": "2003.wwwconf_conference-2003.3", "default_text": "DBLP:conf/www/2003 Proceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest, Hungary, May 20-24, 2003 19\u201328 ACM 2003 https://doi.org/10.1145/775152.775156 10.1145/775152.775156 https://dblp.org/rec/conf/www/LempelM03.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LempelM03 inproceedings ['Ronny Lempel', 'Shlomo Moran'] [] WWW 2003.wwwconf_conference-2003.3 1541519829.0 ABSTRACTWe study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53. Predictive caching and prefetching of query results in search engines", "text": "DBLP:conf/www/2003 Proceedings of the Twelfth International World Wide Web Conference, WWW 2003, Budapest, Hungary, May 20-24, 2003 19\u201328 ACM 2003 https://doi.org/10.1145/775152.775156 10.1145/775152.775156 https://dblp.org/rec/conf/www/LempelM03.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LempelM03 inproceedings ['Ronny Lempel', 'Shlomo Moran'] [] WWW 2003.wwwconf_conference-2003.3 1541519829.0 ABSTRACTWe study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53. Predictive caching and prefetching of query results in search engines"}, "2005.wwwconf_conference-2005.38": {"doc_id": "2005.wwwconf_conference-2005.38", "default_text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 342\u2013351 ACM 2005 https://doi.org/10.1145/1060745.1060797 10.1145/1060745.1060797 https://dblp.org/rec/conf/www/LiuHC05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LiuHC05 inproceedings ['Bing Liu', 'Minqing Hu', 'Junsheng Cheng'] [] WWW 2005.wwwconf_conference-2005.38 1543311638.0 ABSTRACTThe Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly. Opinion observer: analyzing and comparing opinions on the Web", "text": "DBLP:conf/www/2005 Proceedings of the 14th international conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 342\u2013351 ACM 2005 https://doi.org/10.1145/1060745.1060797 10.1145/1060745.1060797 https://dblp.org/rec/conf/www/LiuHC05.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LiuHC05 inproceedings ['Bing Liu', 'Minqing Hu', 'Junsheng Cheng'] [] WWW 2005.wwwconf_conference-2005.38 1543311638.0 ABSTRACTThe Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly. Opinion observer: analyzing and comparing opinions on the Web"}, "2016.wwwconf_conference-2016.11": {"doc_id": "2016.wwwconf_conference-2016.11", "default_text": "DBLP:conf/www/2016 Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016 85\u201397 ACM 2016 https://doi.org/10.1145/2872427.2882976 10.1145/2872427.2882976 https://dblp.org/rec/conf/www/LeeH16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeeH16 inproceedings ['Dokyun Lee', 'Kartik Hosanagar'] [] WWW 2016.wwwconf_conference-2016.11 1541519828.0 ABSTRACTWe investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase. When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance", "text": "DBLP:conf/www/2016 Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016 85\u201397 ACM 2016 https://doi.org/10.1145/2872427.2882976 10.1145/2872427.2882976 https://dblp.org/rec/conf/www/LeeH16.bib dblp computer science bibliography, https://dblp.org DBLP:conf/www/LeeH16 inproceedings ['Dokyun Lee', 'Kartik Hosanagar'] [] WWW 2016.wwwconf_conference-2016.11 1541519828.0 ABSTRACTWe investigate the moderating effect of product attributes and consumer reviews on the efficacy of a collaborative filtering recommender system on an e-commerce site. We run a randomized field experiment on a top North American retailer's website with 184,375 users split into a recommendertreated group and a control group with 37,215 unique products in the dataset. By augmenting the dataset with Amazon Mechanical Turk tagged product attributes and consumer review data from the website, we study their moderating influence on recommenders in generating conversion.We first confirm that the use of recommenders increases the baseline conversion rate by 5.9%. We find that the recommenders act as substitutes for high average review ratings with the effect of using recommenders increasing the conversion rate as much as about 1.4 additional average star ratings. Additionally, we find that the positive impacts on conversion from recommenders are greater for hedonic products compared to utilitarian products while searchexperience quality did not have any impact. We also find that the higher the price, the lower the positive impact of recommenders, while having lengthier product descriptions and higher review volumes increased the recommender's effectiveness. More findings are discussed in the Results.For managers, we 1) identify the products and product attributes for which the recommenders work well, 2) show how other product information sources on e-commerce sites interact with recommenders. Additionally, the insights from the results could inform novel recommender algorithm designs that are aware of strength and shortcomings. From an academic standpoint, we provide insight into the underlying mechanism behind how recommenders cause consumers to purchase. When do Recommender Systems Work the Best?: The Moderating Effects of Product Attributes and Consumer Reviews on Recommender Performance"}, "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1": {"doc_id": "2006.wwwjournals_journal-ir0anthology0volumeA9A4.1", "default_text": "World Wide Web 9 4 369\u2013395 2006 https://doi.org/10.1007/s11280-006-0221-0 10.1007/s11280-006-0221-0 https://dblp.org/rec/journals/www/LongS06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LongS06 article 2006 Volume 9 Issue 4 ['Xiaohui Long', 'Torsten Suel'] [] WWWJ 2006.wwwjournals_journal-ir0anthology0volumeA9A4.1 1495232734.0 Abstract Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395 Three-Level Caching for Efficient Query Processing in Large Web Search Engines", "text": "World Wide Web 9 4 369\u2013395 2006 https://doi.org/10.1007/s11280-006-0221-0 10.1007/s11280-006-0221-0 https://dblp.org/rec/journals/www/LongS06.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LongS06 article 2006 Volume 9 Issue 4 ['Xiaohui Long', 'Torsten Suel'] [] WWWJ 2006.wwwjournals_journal-ir0anthology0volumeA9A4.1 1495232734.0 Abstract Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.Keywords web search . search engine architecture . search engine query processing . inverted index . caching World Wide Web (2006) 9: 369-395 Three-Level Caching for Efficient Query Processing in Large Web Search Engines"}, "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2": {"doc_id": "2012.wwwjournals_journal-ir0anthology0volumeA15A3.2", "default_text": "World Wide Web 15 3 285\u2013323 2012 https://doi.org/10.1007/s11280-011-0134-4 10.1007/s11280-011-0134-4 https://dblp.org/rec/journals/www/SensoyY12.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SensoyY12 article 2012 Volume 15 Issue 3 ['Murat Sensoy', 'Pinar Yolum'] [] WWWJ 2012.wwwjournals_journal-ir0anthology0volumeA15A3.2 1496999016.0 AbstractThe Web is becoming a global market place, where the same services and products are offered by different providers. When obtaining a service, consumers have to select one provider among many alternatives to receive a service or buy a product. In real life, when obtaining a service, many consumers depend on the user reviews. User reviews-presumably written by other consumers-provide details on the consumers' experiences and thus are more informative than ratings. The down side is that such user reviews are written in natural language, making it extremely difficult to be interpreted by computers. Therefore, current technologies do not allow automation of user reviews and require too much human effort for tasks such as writing and reading reviews for the providers, aggregating existing information, and finally choosing among the possible candidates. In this paper, we represent consumers' reviews as machine processable structures using ontologies and develop a layered multiagent framework to enable consumers to find satisfactory service providers for their needs automatically. The framework can still function successfully when consumers evolve their language and when deceptive reviewers enter the system. We show the flexibility of the framework by employing different algorithms for various tasks and evaluate them for different circumstances. Automating user reviews using ontologies: an agent-based approach", "text": "World Wide Web 15 3 285\u2013323 2012 https://doi.org/10.1007/s11280-011-0134-4 10.1007/s11280-011-0134-4 https://dblp.org/rec/journals/www/SensoyY12.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SensoyY12 article 2012 Volume 15 Issue 3 ['Murat Sensoy', 'Pinar Yolum'] [] WWWJ 2012.wwwjournals_journal-ir0anthology0volumeA15A3.2 1496999016.0 AbstractThe Web is becoming a global market place, where the same services and products are offered by different providers. When obtaining a service, consumers have to select one provider among many alternatives to receive a service or buy a product. In real life, when obtaining a service, many consumers depend on the user reviews. User reviews-presumably written by other consumers-provide details on the consumers' experiences and thus are more informative than ratings. The down side is that such user reviews are written in natural language, making it extremely difficult to be interpreted by computers. Therefore, current technologies do not allow automation of user reviews and require too much human effort for tasks such as writing and reading reviews for the providers, aggregating existing information, and finally choosing among the possible candidates. In this paper, we represent consumers' reviews as machine processable structures using ontologies and develop a layered multiagent framework to enable consumers to find satisfactory service providers for their needs automatically. The framework can still function successfully when consumers evolve their language and when deceptive reviewers enter the system. We show the flexibility of the framework by employing different algorithms for various tasks and evaluate them for different circumstances. Automating user reviews using ontologies: an agent-based approach"}, "2016.wwwjournals_journal-ir0anthology0volumeA19A1.4": {"doc_id": "2016.wwwjournals_journal-ir0anthology0volumeA19A1.4", "default_text": "World Wide Web 19 1 69\u201388 2016 https://doi.org/10.1007/s11280-015-0328-2 10.1007/s11280-015-0328-2 https://dblp.org/rec/journals/www/GuptaMBMP16.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/GuptaMBMP16 article 2016 Volume 19 Issue 1 ['Kanik Gupta', 'Vishal Mittal', 'Bazir Bishnoi', 'Siddharth Maheshwari', 'Dhaval Patel'] [] WWWJ 2016.wwwjournals_journal-ir0anthology0volumeA19A1.4 1495232735.0 Abstract News aggregation websites collect news from various online sources using crawling techniques and provide a unified view to millions of users. Since, news sources update information frequently; aggregators have to recrawl them from time to time in order to have durable archiving of the news content. The majority of recrawling techniques assume the availability of unlimited resources and zero operating cost. However, in reality, the resources and budget are limited and it is impossible to crawl every news source at every point of time. To the best of our knowledge, none of the existing techniques discuss the crawling strategy that can retrieve the maximum amount of information in a resource/budget constrained environment. In this paper, we present a framework AcT that supports two different accuracy-aware personalized crawling techniques to attain the optimal accuracy level of retrieving the information. Given the crawling frequency as a resource constraint, the first scheme aims to find the optimal schedule that maximizes the accuracy. In the second scheme, we optimize the crawling frequency and the corresponding crawling schedule for a given accuracy level. We propose a supervised technique that monitors each news source for a particular time period and collect the news update patterns. The news update patterns are later analyzed using mixed integer programming to discover the optimal crawling schedule for the first scheme, whereas a greedy strategy is proposed to discover the optimal crawling frequency and crawling schedule for the second scheme. We develop a crawler for 87 news AcT: Accuracy-aware crawling techniques for cloud-crawler", "text": "World Wide Web 19 1 69\u201388 2016 https://doi.org/10.1007/s11280-015-0328-2 10.1007/s11280-015-0328-2 https://dblp.org/rec/journals/www/GuptaMBMP16.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/GuptaMBMP16 article 2016 Volume 19 Issue 1 ['Kanik Gupta', 'Vishal Mittal', 'Bazir Bishnoi', 'Siddharth Maheshwari', 'Dhaval Patel'] [] WWWJ 2016.wwwjournals_journal-ir0anthology0volumeA19A1.4 1495232735.0 Abstract News aggregation websites collect news from various online sources using crawling techniques and provide a unified view to millions of users. Since, news sources update information frequently; aggregators have to recrawl them from time to time in order to have durable archiving of the news content. The majority of recrawling techniques assume the availability of unlimited resources and zero operating cost. However, in reality, the resources and budget are limited and it is impossible to crawl every news source at every point of time. To the best of our knowledge, none of the existing techniques discuss the crawling strategy that can retrieve the maximum amount of information in a resource/budget constrained environment. In this paper, we present a framework AcT that supports two different accuracy-aware personalized crawling techniques to attain the optimal accuracy level of retrieving the information. Given the crawling frequency as a resource constraint, the first scheme aims to find the optimal schedule that maximizes the accuracy. In the second scheme, we optimize the crawling frequency and the corresponding crawling schedule for a given accuracy level. We propose a supervised technique that monitors each news source for a particular time period and collect the news update patterns. The news update patterns are later analyzed using mixed integer programming to discover the optimal crawling schedule for the first scheme, whereas a greedy strategy is proposed to discover the optimal crawling frequency and crawling schedule for the second scheme. We develop a crawler for 87 news AcT: Accuracy-aware crawling techniques for cloud-crawler"}, "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11": {"doc_id": "2019.wwwjournals_journal-ir0anthology0volumeA22A3.11", "default_text": "World Wide Web 22 3 1151\u20131173 2019 https://doi.org/10.1007/s11280-018-0599-5 10.1007/s11280-018-0599-5 https://dblp.org/rec/journals/www/LuSGCH19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LuSGCH19 article 2019 Volume 22 Issue 3 ['Yi-Shu Lu', 'Wen-Yueh Shih', 'Hung-Yi Gau', 'Kuan-Chieh Chung', 'Jiun-Long Huang'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.11 1559288933.0 Abstract With the increasing popularity of location-based social networks (LBSNs), users are able to share the Point-of-Interests (POIs) they visited by check-ins. By analyzing the users' historical check-in records, POI recommendation can help users get better visiting experience by recommending POIs which users may be interested in. Although recent successive POI recommendation methods consider geographical influence by measuring the distances among POIs, most of them ignore the influence of the regions where the POIs are located. Therefore, we propose in this paper two models to tackle the problem of successive POI recommendation. First, a feature-based successive POI recommendation method, named UGSE-LR, is proposed to take the influence of regions, named regional influence, into consideration when recommending POIs. UGSE-LR first splits an area into grids for estimating regional influence. Then, UGSE-LR applies Edge-weighted Personalized PageRank (EdgePPR) for modeling the successive transitions among POIs. Finally, UGSE-LR fuses user preference, regional influence and successive transition influence into a unified recommendation framework. In addition, with the aid of Recurrent Neural Network (RNN), we propose a latent-factor based successive POI recommendation method, named PEU-RNN, to integrate the sequential visits of POIs and user preference to recommend POIs. First, PEU-RNN adopts the word embedding technique to transform each POI into a latent vector. Then, RNN is used to recommend the POIs depend on the users' historical check-in records. Experimental results on two real LBSN datasets show that our methods are more accurate than the state-of-the-art successive POI recommendation methods in terms of precision and recall. In addition, experimental results also show that PEU-RNN is suitable for On successive point-of-interest recommendation", "text": "World Wide Web 22 3 1151\u20131173 2019 https://doi.org/10.1007/s11280-018-0599-5 10.1007/s11280-018-0599-5 https://dblp.org/rec/journals/www/LuSGCH19.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/LuSGCH19 article 2019 Volume 22 Issue 3 ['Yi-Shu Lu', 'Wen-Yueh Shih', 'Hung-Yi Gau', 'Kuan-Chieh Chung', 'Jiun-Long Huang'] [] WWWJ 2019.wwwjournals_journal-ir0anthology0volumeA22A3.11 1559288933.0 Abstract With the increasing popularity of location-based social networks (LBSNs), users are able to share the Point-of-Interests (POIs) they visited by check-ins. By analyzing the users' historical check-in records, POI recommendation can help users get better visiting experience by recommending POIs which users may be interested in. Although recent successive POI recommendation methods consider geographical influence by measuring the distances among POIs, most of them ignore the influence of the regions where the POIs are located. Therefore, we propose in this paper two models to tackle the problem of successive POI recommendation. First, a feature-based successive POI recommendation method, named UGSE-LR, is proposed to take the influence of regions, named regional influence, into consideration when recommending POIs. UGSE-LR first splits an area into grids for estimating regional influence. Then, UGSE-LR applies Edge-weighted Personalized PageRank (EdgePPR) for modeling the successive transitions among POIs. Finally, UGSE-LR fuses user preference, regional influence and successive transition influence into a unified recommendation framework. In addition, with the aid of Recurrent Neural Network (RNN), we propose a latent-factor based successive POI recommendation method, named PEU-RNN, to integrate the sequential visits of POIs and user preference to recommend POIs. First, PEU-RNN adopts the word embedding technique to transform each POI into a latent vector. Then, RNN is used to recommend the POIs depend on the users' historical check-in records. Experimental results on two real LBSN datasets show that our methods are more accurate than the state-of-the-art successive POI recommendation methods in terms of precision and recall. In addition, experimental results also show that PEU-RNN is suitable for On successive point-of-interest recommendation"}, "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3": {"doc_id": "2014.wwwjournals_journal-ir0anthology0volumeA17A6.3", "default_text": "World Wide Web 17 6 1321\u20131342 2014 https://doi.org/10.1007/s11280-013-0239-z 10.1007/s11280-013-0239-z https://dblp.org/rec/journals/www/SiLQD14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SiLQD14 article 2014 Volume 17 Issue 6 ['Jianfeng Si', 'Qing Li', 'Tieyun Qian', 'Xiaotie Deng'] [] WWWJ 2014.wwwjournals_journal-ir0anthology0volumeA17A6.3 1585294593.0 Abstract Large volume of online review data can reveal consumers' major interests on domain product, which attracts great research interests from the academic community. Most of the existing works focus on the problems of review summarization, aspect identification or opinion mining from an item's point of view such as the quality or popularity of products. Considering the fact that users who generate those review texts draw different attentions to product aspects with respect to their own interests, in this article, we aim to learn K users' interest groups indicated by their review writings. Such K interest groups' identification can facilitate better understanding of major and potential consumers' concerns which are crucial for applications like product improvement on customer-oriented design or diverse marketing strategies. Instead of using a traditional text clustering approach, we treat the groupId/clusterId as a hidden variable and use a permutation-based structural topic model called KMM. Through this model, we infer K interest groups' distribution by discovering not only the frequency of product aspects (Topic Frequency), but also the occurrence Users' interest grouping from online reviews based on topic frequency and order", "text": "World Wide Web 17 6 1321\u20131342 2014 https://doi.org/10.1007/s11280-013-0239-z 10.1007/s11280-013-0239-z https://dblp.org/rec/journals/www/SiLQD14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/www/SiLQD14 article 2014 Volume 17 Issue 6 ['Jianfeng Si', 'Qing Li', 'Tieyun Qian', 'Xiaotie Deng'] [] WWWJ 2014.wwwjournals_journal-ir0anthology0volumeA17A6.3 1585294593.0 Abstract Large volume of online review data can reveal consumers' major interests on domain product, which attracts great research interests from the academic community. Most of the existing works focus on the problems of review summarization, aspect identification or opinion mining from an item's point of view such as the quality or popularity of products. Considering the fact that users who generate those review texts draw different attentions to product aspects with respect to their own interests, in this article, we aim to learn K users' interest groups indicated by their review writings. Such K interest groups' identification can facilitate better understanding of major and potential consumers' concerns which are crucial for applications like product improvement on customer-oriented design or diverse marketing strategies. Instead of using a traditional text clustering approach, we treat the groupId/clusterId as a hidden variable and use a permutation-based structural topic model called KMM. Through this model, we infer K interest groups' distribution by discovering not only the frequency of product aspects (Topic Frequency), but also the occurrence Users' interest grouping from online reviews based on topic frequency and order"}, "2011.tweb_journal-ir0anthology0volumeA5A2.3": {"doc_id": "2011.tweb_journal-ir0anthology0volumeA5A2.3", "default_text": "ACM Trans. Web 5 2 9:1\u20139:25 2011 https://doi.org/10.1145/1961659.1961663 10.1145/1961659.1961663 https://dblp.org/rec/journals/tweb/OzcanAU11.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanAU11 article 2011 Volume 5 Issue 2 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2011.tweb_journal-ir0anthology0volumeA5A2.3 1609262376.0 Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time. Cost-Aware Strategies for Query Result Caching in Web Search Engines", "text": "ACM Trans. Web 5 2 9:1\u20139:25 2011 https://doi.org/10.1145/1961659.1961663 10.1145/1961659.1961663 https://dblp.org/rec/journals/tweb/OzcanAU11.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/OzcanAU11 article 2011 Volume 5 Issue 2 ['Rifat Ozcan', 'Ismail Seng\u00f6r Alting\u00f6vde', '\u00d6zg\u00fcr Ulusoy'] [] TWEB 2011.tweb_journal-ir0anthology0volumeA5A2.3 1609262376.0 Search engines and large-scale IR systems need to cache query results for efficiency and scalability purposes. Static and dynamic caching techniques (as well as their combinations) are employed to effectively cache query results. In this study, we propose cost-aware strategies for static and dynamic caching setups. Our research is motivated by two key observations: (i) query processing costs may significantly vary among different queries, and (ii) the processing cost of a query is not proportional to its popularity (i.e., frequency in the previous logs). The first observation implies that cache misses have different, that is, nonuniform, costs in this context. The latter observation implies that typical caching policies, solely based on query popularity, can not always minimize the total cost. Therefore, we propose to explicitly incorporate the query costs into the caching policies. Simulation results using two large Web crawl datasets and a real query log reveal that the proposed approach improves overall system performance in terms of the average query execution time. Cost-Aware Strategies for Query Result Caching in Web Search Engines"}, "2008.tweb_journal-ir0anthology0volumeA2A4.2": {"doc_id": "2008.tweb_journal-ir0anthology0volumeA2A4.2", "default_text": "ACM Trans. Web 2 4 20:1\u201320:28 2008 https://doi.org/10.1145/1409220.1409223 10.1145/1409220.1409223 https://dblp.org/rec/journals/tweb/Baeza-YatesGJMPS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/Baeza-YatesGJMPS08 article 2008 Volume 2 Issue 4 ['Ricardo Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] TWEB 2008.tweb_journal-ir0anthology0volumeA2A4.2 1619422013.0 In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Design trade-offs for search engine caching", "text": "ACM Trans. Web 2 4 20:1\u201320:28 2008 https://doi.org/10.1145/1409220.1409223 10.1145/1409220.1409223 https://dblp.org/rec/journals/tweb/Baeza-YatesGJMPS08.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tweb/Baeza-YatesGJMPS08 article 2008 Volume 2 Issue 4 ['Ricardo Baeza-Yates', 'Aristides Gionis', 'Flavio Junqueira', 'Vanessa Murdock', 'Vassilis Plachouras', 'Fabrizio Silvestri'] [] TWEB 2008.tweb_journal-ir0anthology0volumeA2A4.2 1619422013.0 In this article we study the trade-offs in designing efficient caching systems for Web search engines. We explore the impact of different approaches, such as static vs. dynamic caching, and caching query results vs. caching posting lists. Using a query log spanning a whole year, we explore the limitations of caching and we demonstrate that caching posting lists can achieve higher hit rates than caching query answers. We propose a new algorithm for static caching of posting lists, which outperforms previous methods. We also study the problem of finding the optimal way to split the static cache between answers and posting lists. Finally, we measure how the changes in the query log influence the effectiveness of static caching, given our observation that the distribution of the queries changes slowly over time. Our results and observations are applicable to different levels of the data-access hierarchy, for instance, for a memory/disk layer or a broker/remote server layer. Design trade-offs for search engine caching"}, "2009.ipm_journal-ir0anthology0volumeA45A3.2": {"doc_id": "2009.ipm_journal-ir0anthology0volumeA45A3.2", "default_text": "Inf. Process. Manag. 45 3 341\u2013355 2009 https://doi.org/10.1016/j.ipm.2008.11.002 10.1016/j.ipm.2008.11.002 https://dblp.org/rec/journals/ipm/LiLZ09.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiLZ09 article 2009 Volume 45 Issue 3 ['Ming Li', 'Hang Li', 'Zhi-Hua Zhou'] [] IPM 2009.ipm_journal-ir0anthology0volumeA45A3.2 1582287073.0 a b s t r a c tThis paper proposes a new machine learning method for constructing ranking models in document retrieval. The method, which is referred to as SSRANK, aims to use the advantages of both the traditional Information Retrieval (IR) methods and the supervised learning methods for IR proposed recently. The advantages include the use of limited amount of labeled data and rich model representation. To do so, the method adopts a semi-supervised learning framework in ranking model construction. Specifically, given a small number of labeled documents with respect to some queries, the method effectively labels the unlabeled documents for the queries. It then uses all the labeled data to train a machine learning model (in our case, Neural Network). In the data labeling, the method also makes use of a traditional IR model (in our case, BM25). A stopping criterion based on machine learning theory is given for the data labeling process. Experimental results on three benchmark datasets and one web search dataset indicate that SSRANK consistently and almost always significantly outperforms the baseline methods (unsupervised and supervised learning methods), given the same amount of labeled data. This is because SSRANK can effectively leverage the use of unlabeled data in learning. Semi-supervised document retrieval", "text": "Inf. Process. Manag. 45 3 341\u2013355 2009 https://doi.org/10.1016/j.ipm.2008.11.002 10.1016/j.ipm.2008.11.002 https://dblp.org/rec/journals/ipm/LiLZ09.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiLZ09 article 2009 Volume 45 Issue 3 ['Ming Li', 'Hang Li', 'Zhi-Hua Zhou'] [] IPM 2009.ipm_journal-ir0anthology0volumeA45A3.2 1582287073.0 a b s t r a c tThis paper proposes a new machine learning method for constructing ranking models in document retrieval. The method, which is referred to as SSRANK, aims to use the advantages of both the traditional Information Retrieval (IR) methods and the supervised learning methods for IR proposed recently. The advantages include the use of limited amount of labeled data and rich model representation. To do so, the method adopts a semi-supervised learning framework in ranking model construction. Specifically, given a small number of labeled documents with respect to some queries, the method effectively labels the unlabeled documents for the queries. It then uses all the labeled data to train a machine learning model (in our case, Neural Network). In the data labeling, the method also makes use of a traditional IR model (in our case, BM25). A stopping criterion based on machine learning theory is given for the data labeling process. Experimental results on three benchmark datasets and one web search dataset indicate that SSRANK consistently and almost always significantly outperforms the baseline methods (unsupervised and supervised learning methods), given the same amount of labeled data. This is because SSRANK can effectively leverage the use of unlabeled data in learning. Semi-supervised document retrieval"}, "2018.ipm_journal-ir0anthology0volumeA54A6.5": {"doc_id": "2018.ipm_journal-ir0anthology0volumeA54A6.5", "default_text": "Inf. Process. Manag. 54 6 938\u2013957 2018 https://doi.org/10.1016/j.ipm.2018.06.003 10.1016/j.ipm.2018.06.003 https://dblp.org/rec/journals/ipm/KimK18.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KimK18 article 2018 Volume 54 Issue 6 ['Sung Guen Kim', 'Juyoung Kang'] [] IPM 2018.ipm_journal-ir0anthology0volumeA54A6.5 1582287077.0 A B S T R A C TConsumers evaluate products through online reviews, in addition to sharing their product experiences. Online reviews affect product marketing, and companies use online reviews to investigate consumer attitudes and perceptions of their products. However, when analyzing a review, it is often the case that specific contexts are not taken into consideration and meaningful information is not obtained from the analysis results. This study suggests a methodology for analyzing reviews in the context of comparing two competing products. In addition, by analyzing the discriminative attributes of competing products, we were able to derive more specific information than an overall product analysis. Analyzing the discriminative attributes in the context of comparing competing products provides clarity on analyzing the strengths and weaknesses of competitive products and provides realistic information that can help the company's management activities. Considering this purpose, this study collected a review of the BB Cream product line in the cosmetics field. The analysis was sequentially carried out in three stages. First, we extracted words that represent discriminative attributes by analyzing the percentage difference of words. Second, different attribute words were classified according to the meaning used in the review by using latent semantic analysis. Finally, the polarity of discriminative attribute words was analyzed using Labeled-LDA. This analysis method can be used as a market research method as it can extract more information than a traditional survey or interview method, and can save cost and time through the automation of the program. Analyzing the discriminative attributes of products using text mining focused on cosmetic reviews", "text": "Inf. Process. Manag. 54 6 938\u2013957 2018 https://doi.org/10.1016/j.ipm.2018.06.003 10.1016/j.ipm.2018.06.003 https://dblp.org/rec/journals/ipm/KimK18.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/KimK18 article 2018 Volume 54 Issue 6 ['Sung Guen Kim', 'Juyoung Kang'] [] IPM 2018.ipm_journal-ir0anthology0volumeA54A6.5 1582287077.0 A B S T R A C TConsumers evaluate products through online reviews, in addition to sharing their product experiences. Online reviews affect product marketing, and companies use online reviews to investigate consumer attitudes and perceptions of their products. However, when analyzing a review, it is often the case that specific contexts are not taken into consideration and meaningful information is not obtained from the analysis results. This study suggests a methodology for analyzing reviews in the context of comparing two competing products. In addition, by analyzing the discriminative attributes of competing products, we were able to derive more specific information than an overall product analysis. Analyzing the discriminative attributes in the context of comparing competing products provides clarity on analyzing the strengths and weaknesses of competitive products and provides realistic information that can help the company's management activities. Considering this purpose, this study collected a review of the BB Cream product line in the cosmetics field. The analysis was sequentially carried out in three stages. First, we extracted words that represent discriminative attributes by analyzing the percentage difference of words. Second, different attribute words were classified according to the meaning used in the review by using latent semantic analysis. Finally, the polarity of discriminative attribute words was analyzed using Labeled-LDA. This analysis method can be used as a market research method as it can extract more information than a traditional survey or interview method, and can save cost and time through the automation of the program. Analyzing the discriminative attributes of products using text mining focused on cosmetic reviews"}, "2015.ipm_journal-ir0anthology0volumeA51A1.3": {"doc_id": "2015.ipm_journal-ir0anthology0volumeA51A1.3", "default_text": "Inf. Process. Manag. 51 1 58\u201367 2015 https://doi.org/10.1016/j.ipm.2014.08.005 10.1016/j.ipm.2014.08.005 https://dblp.org/rec/journals/ipm/LiZL15.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiZL15 article 2015 Volume 51 Issue 1 ['Shi Li', 'Lina Zhou', 'Yijun Li'] [] IPM 2015.ipm_journal-ir0anthology0volumeA51A1.3 1582287091.0 a b s t r a c tOnline review mining has been used to help manufacturers and service providers improve their products and services, and to provide valuable support for consumer decision making. Product aspect extraction is fundamental to online review mining. This research is aimed to improve the performance of aspect extraction from online consumer reviews. To this end, we augment a frequency-based extraction method with PMI-IR, which utilizes web search in measuring the semantic similarity between aspect candidates and target entities. In addition, we extend RCut, an algorithm originally developed for text classification, to learn the threshold for selecting candidate aspects. Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency-based method for aspect extraction but also generalizes across different product domains and various data sizes. Improving aspect extraction by augmenting a frequency-based method with web-based similarity measures", "text": "Inf. Process. Manag. 51 1 58\u201367 2015 https://doi.org/10.1016/j.ipm.2014.08.005 10.1016/j.ipm.2014.08.005 https://dblp.org/rec/journals/ipm/LiZL15.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LiZL15 article 2015 Volume 51 Issue 1 ['Shi Li', 'Lina Zhou', 'Yijun Li'] [] IPM 2015.ipm_journal-ir0anthology0volumeA51A1.3 1582287091.0 a b s t r a c tOnline review mining has been used to help manufacturers and service providers improve their products and services, and to provide valuable support for consumer decision making. Product aspect extraction is fundamental to online review mining. This research is aimed to improve the performance of aspect extraction from online consumer reviews. To this end, we augment a frequency-based extraction method with PMI-IR, which utilizes web search in measuring the semantic similarity between aspect candidates and target entities. In addition, we extend RCut, an algorithm originally developed for text classification, to learn the threshold for selecting candidate aspects. Experiment results with Chinese online reviews show that our proposed method not only outperforms the state of the art frequency-based method for aspect extraction but also generalizes across different product domains and various data sizes. Improving aspect extraction by augmenting a frequency-based method with web-based similarity measures"}, "2007.ipm_journal-ir0anthology0volumeA43A4.3": {"doc_id": "2007.ipm_journal-ir0anthology0volumeA43A4.3", "default_text": "Inf. Process. Manag. 43 4 902\u2013913 2007 https://doi.org/10.1016/j.ipm.2006.08.010 10.1016/j.ipm.2006.08.010 https://dblp.org/rec/journals/ipm/LeeK07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LeeK07 article 2007 Volume 43 Issue 4 ['Kyung-Soon Lee', 'Kyo Kageura'] [] IPM 2007.ipm_journal-ir0anthology0volumeA43A4.3 1582287026.0 AbstractThis paper explores the incorporation of prior knowledge into support vector machines as a means of compensating for a shortage of training data in text categorization. The prior knowledge about transformation invariance is generated by a virtual document method. The method applies a simple transformation to documents, i.e., making virtual documents by combining relevant document pairs for a topic in the training set. The virtual document thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting relevant terms that are not given high importance in individual real documents. Artificially generated documents result in the change in the distribution of training data without the randomization. Experiments with support vector machines based on linear, polynomial and radial-basis function kernels showed the effectiveness on Reuters-21578 set for the topics with a small number of relevant documents. The proposed method achieved 131%, 34%, 12% improvements in micro-averaged F 1 for 25, 46, and 58 topics with less than 10, 30, and 50 relevant documents in learning, respectively. The result analysis indicates that incorporating virtual documents contributes to a steady improvement on the performance. Virtual relevant documents in text categorization with support vector machines", "text": "Inf. Process. Manag. 43 4 902\u2013913 2007 https://doi.org/10.1016/j.ipm.2006.08.010 10.1016/j.ipm.2006.08.010 https://dblp.org/rec/journals/ipm/LeeK07.bib dblp computer science bibliography, https://dblp.org DBLP:journals/ipm/LeeK07 article 2007 Volume 43 Issue 4 ['Kyung-Soon Lee', 'Kyo Kageura'] [] IPM 2007.ipm_journal-ir0anthology0volumeA43A4.3 1582287026.0 AbstractThis paper explores the incorporation of prior knowledge into support vector machines as a means of compensating for a shortage of training data in text categorization. The prior knowledge about transformation invariance is generated by a virtual document method. The method applies a simple transformation to documents, i.e., making virtual documents by combining relevant document pairs for a topic in the training set. The virtual document thus created not only is expected to preserve the topic, but even improve the topical representation by exploiting relevant terms that are not given high importance in individual real documents. Artificially generated documents result in the change in the distribution of training data without the randomization. Experiments with support vector machines based on linear, polynomial and radial-basis function kernels showed the effectiveness on Reuters-21578 set for the topics with a small number of relevant documents. The proposed method achieved 131%, 34%, 12% improvements in micro-averaged F 1 for 25, 46, and 58 topics with less than 10, 30, and 50 relevant documents in learning, respectively. The result analysis indicates that incorporating virtual documents contributes to a steady improvement on the performance. Virtual relevant documents in text categorization with support vector machines"}, "2001.tois_journal-ir0anthology0volumeA19A3.3": {"doc_id": "2001.tois_journal-ir0anthology0volumeA19A3.3", "default_text": "ACM Trans. Inf. Syst. 19 3 286\u2013309 2001 https://doi.org/10.1145/502115.502119 10.1145/502115.502119 https://dblp.org/rec/journals/tois/AggarwalAY01.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/AggarwalAY01 article 2001 Volume 19 Issue 3 ['Charu C. Aggarwal', 'Fatima Al-Garawi', 'Philip S. Yu'] [] TOIS 2001.tois_journal-ir0anthology0volumeA19A3.3 1541505116.0 In recent years, the World Wide Web has shown enormous growth in size. Vast repositories of information are available on practically every possible topic. In such cases, it is valuable to perform topical resource discovery effectively. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the World Wide Web quickly, without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the World Wide Web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries, or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. We discuss how to intelligently select features which are most useful for a given crawl. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates. This is the extended version of the preliminary paper ] which has appeared in the ACM published On the design of a learning crawler for topical resource discovery", "text": "ACM Trans. Inf. Syst. 19 3 286\u2013309 2001 https://doi.org/10.1145/502115.502119 10.1145/502115.502119 https://dblp.org/rec/journals/tois/AggarwalAY01.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/AggarwalAY01 article 2001 Volume 19 Issue 3 ['Charu C. Aggarwal', 'Fatima Al-Garawi', 'Philip S. Yu'] [] TOIS 2001.tois_journal-ir0anthology0volumeA19A3.3 1541505116.0 In recent years, the World Wide Web has shown enormous growth in size. Vast repositories of information are available on practically every possible topic. In such cases, it is valuable to perform topical resource discovery effectively. Consequently, several new ideas have been proposed in recent years; among them a key technique is focused crawling which is able to crawl particular topical portions of the World Wide Web quickly, without having to explore all web pages. In this paper, we propose the novel concept of intelligent crawling which actually learns characteristics of the linkage structure of the World Wide Web while performing the crawling. Specifically, the intelligent crawler uses the inlinking web page content, candidate URL structure, or other behaviors of the inlinking web pages or siblings in order to estimate the probability that a candidate is useful for a given crawl. This is a much more general framework than the focused crawling technique which is based on a pre-defined understanding of the topical structure of the web. The techniques discussed in this paper are applicable for crawling web pages which satisfy arbitrary user-defined predicates such as topical queries, keyword queries, or any combinations of the above. Unlike focused crawling, it is not necessary to provide representative topical examples, since the crawler can learn its way into the appropriate topic. We refer to this technique as intelligent crawling because of its adaptive nature in adjusting to the web page linkage structure. We discuss how to intelligently select features which are most useful for a given crawl. The learning crawler is capable of reusing the knowledge gained in a given crawl in order to provide more efficient crawling for closely related predicates. This is the extended version of the preliminary paper ] which has appeared in the ACM published On the design of a learning crawler for topical resource discovery"}, "2014.tois_journal-ir0anthology0volumeA32A4.5": {"doc_id": "2014.tois_journal-ir0anthology0volumeA32A4.5", "default_text": "ACM Trans. Inf. Syst. 32 4 21:1\u201321:26 2014 https://doi.org/10.1145/2661629 10.1145/2661629 https://dblp.org/rec/journals/tois/WangLYTWL14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/WangLYTWL14 article 2014 Volume 32 Issue 4 ['Jianguo Wang', 'Eric Lo', 'Man Lung Yiu', 'Jiancong Tong', 'Gang Wang', 'Xiaoguang Liu'] [] TOIS 2014.tois_journal-ir0anthology0volumeA32A4.5 1614258226.0 Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines. Cache Design of SSD-Based Search Engine Architectures: An Experimental Study", "text": "ACM Trans. Inf. Syst. 32 4 21:1\u201321:26 2014 https://doi.org/10.1145/2661629 10.1145/2661629 https://dblp.org/rec/journals/tois/WangLYTWL14.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/WangLYTWL14 article 2014 Volume 32 Issue 4 ['Jianguo Wang', 'Eric Lo', 'Man Lung Yiu', 'Jiancong Tong', 'Gang Wang', 'Xiaoguang Liu'] [] TOIS 2014.tois_journal-ir0anthology0volumeA32A4.5 1614258226.0 Caching is an important optimization in search engine architectures. Existing caching techniques for search engine optimization are mostly biased towards the reduction of random accesses to disks, because random accesses are known to be much more expensive than sequential accesses in traditional magnetic hard disk drive (HDD). Recently, solid-state drive (SSD) has emerged as a new kind of secondary storage medium, and some search engines like Baidu have already used SSD to completely replace HDD in their infrastructure. One notable property of SSD is that its random access latency is comparable to its sequential access latency. Therefore, the use of SSDs to replace HDDs in a search engine infrastructure may void the cache management of existing search engines. In this article, we carry out a series of empirical experiments to study the impact of SSD on search engine cache management. Based on the results, we give insights to practitioners and researchers on how to adapt the infrastructure and caching policies for SSD-based search engines. Cache Design of SSD-Based Search Engine Architectures: An Experimental Study"}, "2005.tois_journal-ir0anthology0volumeA23A4.2": {"doc_id": "2005.tois_journal-ir0anthology0volumeA23A4.2", "default_text": "ACM Trans. Inf. Syst. 23 4 430\u2013462 2005 https://doi.org/10.1145/1095872.1095875 10.1145/1095872.1095875 https://dblp.org/rec/journals/tois/PantS05.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/PantS05 article 2005 Volume 23 Issue 4 ['Gautam Pant', 'Padmini Srinivasan'] [] TOIS 2005.tois_journal-ir0anthology0volumeA23A4.2 1603696856.0 Topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques. The use of classification algorithms to guide topical crawlers has been sporadically suggested in the literature. No systematic study, however, has been done on their relative merits. Using the lessons learned from our previous crawler evaluation studies, we experiment with multiple versions of different classification schemes. The crawling process is modeled as a parallel best-first search over a graph defined by the Web. The classifiers provide heuristics to the crawler thus biasing it towards certain portions of the Web graph. Our results show that Naive Bayes is a weak choice for guiding a topical crawler when compared with Support Vector Machine or Neural Network. Further, the weak performance of Naive Bayes can be partly explained by extreme skewness of posterior probabilities generated by it. We also observe that despite similar performances, different topical crawlers cover subspaces on the Web with low overlap. Learning to crawl: Comparing classification schemes", "text": "ACM Trans. Inf. Syst. 23 4 430\u2013462 2005 https://doi.org/10.1145/1095872.1095875 10.1145/1095872.1095875 https://dblp.org/rec/journals/tois/PantS05.bib dblp computer science bibliography, https://dblp.org DBLP:journals/tois/PantS05 article 2005 Volume 23 Issue 4 ['Gautam Pant', 'Padmini Srinivasan'] [] TOIS 2005.tois_journal-ir0anthology0volumeA23A4.2 1603696856.0 Topical crawling is a young and creative area of research that holds the promise of benefiting from several sophisticated data mining techniques. The use of classification algorithms to guide topical crawlers has been sporadically suggested in the literature. No systematic study, however, has been done on their relative merits. Using the lessons learned from our previous crawler evaluation studies, we experiment with multiple versions of different classification schemes. The crawling process is modeled as a parallel best-first search over a graph defined by the Web. The classifiers provide heuristics to the crawler thus biasing it towards certain portions of the Web graph. Our results show that Naive Bayes is a weak choice for guiding a topical crawler when compared with Support Vector Machine or Neural Network. Further, the weak performance of Naive Bayes can be partly explained by extreme skewness of posterior probabilities generated by it. We also observe that despite similar performances, different topical crawlers cover subspaces on the Web with low overlap. Learning to crawl: Comparing classification schemes"}}};
  </script>
  <script type="text/javascript">
    var allWeightsA = {};
    var allWeightsB = {};
    var mergedWeights = {};
    var COLOR_A = '236, 154, 8';
    var COLOR_B = '121, 196, 121';
    var singleRunView = (data.meta.run2_name === null);

    function markup(text, weights) {
      weights = weights.filter(function (e) {
        return (e[2] > 0 || typeof e[2] === 'string');
      })
      var $result = $('<div></div>');
      if (weights.length === 0) {
        $result.text(text);
      } else {
        $result.append($('<span></span>').text(text.substring(0, weights[0][0])));
        $.each(weights, function (i, weight) {
          if (typeof weight[2] === 'string') {
            var weightColor = weight[2];
          } else {
            var weightColor = 'rgba(255, 237, 140, ' + weight[2].toString() + ')';
          }
          $result.append($('<mark></mark>').text(text.substring(weight[0], weight[1])).css('background', weightColor).attr("run1", weight[3]).attr("run2", weight[4]));
          if (i + 1 < weights.length) {
            $result.append($('<span></span>').text(text.substring(weight[1], weights[i + 1][0])));
          }
        });
        $result.append($('<span></span>').text(text.substring(weights[weights.length - 1][1], text.length)));
      }
      return $result;
    }

    function colorizeWeights(mergedWeights) {
      // deep copu & handle if doesn't exist
      mergedWeights = mergedWeights ? JSON.parse(JSON.stringify(mergedWeights)) : [];
      var results = mergedWeights.map((segment) => {
        if (!("run2" in segment[2]) || segment[2].run2 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')', segment[2].run1, segment[2].run2];
        } else if (!("run" in segment[2]) || segment[2].run1 === null) {
          return [segment[0], segment[1], 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')', segment[2].run1, segment[2].run2];
        } else {
          var nil = 'rgba(0, 0, 0, 0)'
          var colorA = 'rgba(' + COLOR_A + ', ' + segment[2].run1.toString() + ')';
          var colorB = 'rgba(' + COLOR_B + ', ' + segment[2].run2.toString() + ')';
          var overlapColors = 'linear-gradient(' + colorA + ', ' + nil + '), linear-gradient(' + nil + ', ' + colorB + ')'
          return [segment[0], segment[1], overlapColors, segment[2].run1, segment[2].run2];
        }
      })
      return results;
    }
    function generateDocListSingleView(run, container, oneRunWeights) {
      $(container).empty();
      $(container).css("padding-left", "15%").css("padding-right", "15%");
      $.each(run, function (i, doc) {
        oneRunWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text('⋮ ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css("right", '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        var newEl = $('<div></div>')
          .append($('<div class="card"></div>')
            .attr('data-docid', doc.doc_id)
            .attr('run1-rank', doc.rank)
            .append($('<div class="card-header"></div>')
              // .css('padding-' + docIdFloat, '30px')
              .append(doc.rank)
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }
    function generateDocList(run, otherRun, container, docIdFloat, allWeights) {
      $(container).empty();
      $.each(run, function (i, doc) {
        allWeights[doc.doc_id] = doc.weights;
        if (i >= 1 && run[i - 1].rank + 1 != doc.rank) {
          $('<div class="elip"></div>').text('⋮ ' + (doc.rank - run[i - 1].rank - 1).toString() + ' doc(s) skipped').appendTo(container);
        }
        var $did = $('<div class="docid"></div>').append($('<div class="docid-value"></div>').text(doc.doc_id));
        $did.css('background-color', data.meta.relevanceColors[doc.relevance !== null ? doc.relevance.toString() : 'null']).css(docIdFloat, '0');
        if (doc.relevance === null) {
          var $rel = $('<h6 class="badge badge-info">Unjudged</h6>').css('background-color', data.meta.relevanceColors['null']);
        } else {
          var $rel = $('<h6 class="badge badge-info"></h6>').text('Rel: ' + doc.relevance).css('background-color', data.meta.relevanceColors[doc.relevance.toString()]).attr('title', data.meta.qrelDefs[doc.relevance.toString()]).css('cursor', 'help');
        }
        var $score = $('<h6 class="badge"></h6>').text('Score: ' + doc.score.toFixed(4));
        var doc_fields = data.docs[doc.doc_id];
        var $text = markup(doc_fields[doc.snippet.field].substring(doc.snippet.start, doc.snippet.stop), doc.snippet.weights)
        if (doc.snippet.stop < doc_fields[doc.snippet.field].length) {
          $text.append('...');
        }
        if (doc.snippet.start > 0) {
          $text.prepend('...');
        }
        $text.prepend($('<span style="color: #999;"></span>').text(doc.snippet.field + ': '));
        // $text.append(' ').append('<a href="#" class="doc-info" role="button">See more</a>');
        var otherRank = null;
        $.each(otherRun, function (i, otherDoc) {
          if (otherDoc.doc_id === doc.doc_id) {
            otherRank = otherDoc.rank;
            return false; // break
          }
        });
        if (otherRank === null) {
          var symbol = '×';
          var tip = 'not ranked in other run';
        }
        else if (doc.rank === otherRank) {
          var symbol = docIdFloat === 'right' ? '→' : '←';
          var tip = 'ranked equally in other run'
        } else if (doc.rank < otherRank) {
          var symbol = docIdFloat === 'right' ? '⬊' : '⬋';
          var tip = 'ranked lower in other run (' + otherRank + ')'
        } else if (doc.rank > otherRank) {
          var symbol = docIdFloat === 'right' ? '⬈' : '⬉';
          var tip = 'ranked higher in other run (' + otherRank + ')'
        }
        var newEl = $('<div></div>')
          // .append($('<span class="other-rank"></span>').text(symbol).css('float', docIdFloat).css('text-align', docIdFloat === 'right' ? 'left' : 'right').attr('title', tip))
          .append($('<div class="card"></div>')
            .attr("run1-rank", docIdFloat === 'right' ? doc.rank : (otherRank === null ? "No" : otherRank))
            .attr("run2-rank", docIdFloat === 'right' ? (otherRank === null ? "No" : otherRank): doc.rank)
            .attr('data-docid', doc.doc_id)
            .append($('<div class="card-header"></div>')
              .css('padding-' + docIdFloat, '30px')
              .append($("<span class='border badge' style='min-width: 50px; font-weight: normal;color: grey;'></span>").html('<span style="font-size: 1.2em;font-weight:bold; color: black;">'+doc.rank +'</span> '+symbol + (otherRank === null ? '': otherRank)).attr('title', tip))
              .append(' ')
              .append($did)
              .append(' ')
              .append($rel)
              .append(' ')
              .append($score)
              .append($('<div class="snippet"></div>').append($text))
            )
          )
          .appendTo(container);
      });
    }

    function selectQuery() {
      var $select = $('#Queries');
      var query_id = $select.val();
      var query = data.queries.filter(query => query.fields.query_id === query_id);
      mergedWeights = query[0].mergedWeights
      var $query = $('#Query');
      $query.empty();
      var $table = $('<table class="fields"></table>').appendTo($query);
      if (query.length > 0) {
        query = query[0];
        $.each(query.fields, function (fname, fvalue) {
          if (fname == "contrast"){
            fvalue = fvalue.name +" (" + fvalue.value.toFixed(3)+")";
            $("#contrast-measure").text("Contrast measure: "+fvalue);
          } else {
            $('<tr></tr>')
            .append($('<th></th>').text(fname))
            .append($('<td></td>').text(fvalue))
            .appendTo($table);
          }
        });
        colors = ["badge-secondary", "badge-info", "badge-warning", "badge-primary"]
        $summary = $("<ul></ul>")
        $.each(query.summary, function(index, data){
          $l=$("<li></li>");
          $.each(data, function(idx, statement){
            $("<span class='badge "+ colors[index] +"'></span>").text(statement).appendTo($l);
          });
          $summary.append($l);
        });
        $("#ranking-summary").empty().append($summary)
        if (singleRunView) {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Value</td>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0]==null? "No" : metric_value[0].toFixed(3)))
            .appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);          
          allWeightsA = {}
          generateDocListSingleView(query.run_1, "#docList", allWeightsA);
        } else {
          var $metricsTable = $('<table class="styled-table" align="center"></table>')
          $("<thead> <tr> <th>Metric</th> <td>Run1</td> <td>Run2</td></tr></thead>").appendTo($metricsTable);
          $tbody = $("<tbody></tbody>");
          $.each(query.metrics, function(metric_name, metric_value){
            $('<tr></tr>').append($('<th></th>').text(metric_name))
            .append($('<td></td>').text(metric_value[0] == null? "No" : metric_value[0].toFixed(3)))
            .append($('<td></td>').text(metric_value[1] == null? "No" : metric_value[1].toFixed(3))).appendTo($tbody)
          });
          $metricsTable.append($tbody);
          $("#metrics").empty().append($metricsTable);
          allWeightsA = {};
          allWeightsB = {};          
          generateDocList(query.run_1, query.run_2, '#Run1Docs', 'right', allWeightsA);
          generateDocList(query.run_2, query.run_1, '#Run2Docs', 'left', allWeightsB);          
        }
      }
      var extraFields = $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse");
      // Don't show expand/collapse button if there are not fields to expand/collapse
      $('#query-collapse-btn').toggle(extraFields.length > 0);
    }

    function checkThreshold(value, threshold) {
      if (typeof value !== 'undefined') {
        return parseFloat(value) < threshold;
      } else return true;
    }

    function onChangeWeightThreshold() {
      $("#DocumentDetails mark").removeClass("nobackground")
      var run1Threshold = parseFloat($("#run1Threshold").text());
      var run2Threshold = parseFloat($("#run2Threshold").text());
      $("#DocumentDetails mark").each(function () {
        var run1w = $(this).attr("run1");
        var run2w = $(this).attr("run2");
        if (checkThreshold(run1w, run1Threshold) && checkThreshold(run2w, run2Threshold)) {
          $(this).addClass("nobackground");
        } else if (checkThreshold(run1w, run1Threshold)) {
          $(this).css("background", "rgba(" + COLOR_B + "," + run2w + ")");
        } else if (checkThreshold(run2w, run2Threshold)) {
          $(this).css("background", "rgba(" + COLOR_A + "," + run1w + ")");
        } else {
          $(this).css("background", 'linear-gradient(rgba('+ COLOR_A + "," + run1w + '),  rgba( '+ COLOR_B + "," + run2w + '))');
        }
      })
    }

    function onCardEnter() {
      var did = $(this).attr('data-docid');
      $('.card[data-docid="' + did + '"').addClass('highlight');
    }

    function onCardLeave() {
      var did = $(this).attr('data-docid');
      $('.card.highlight').removeClass('highlight');
    }

    function onDocInfoClick() {
      var docid = $(this).closest('[data-docid]').attr('data-docid');
      var doc = data.docs[docid];
      $('<div id="DocumentOverlay"></div>').appendTo(document.body)
      var page = $('<div id="DocumentDetails" class="sticky-top"></div>')
        .append($('<div class="close-overlay">X</div>').click(closeDoc))
        .appendTo(document.body);
      var legendTable = $('<table class="fields"></table>')
        .appendTo(page);
      var run1Rank = $(this).closest('[run1-rank]').attr('run1-rank');
      legendTable.append($('<tr></tr>')
        .append($('<th></th>').text(data.meta.run1_name))
        .append($('<td></td>')
          .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_A + ')'))
        )
        .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run1Rank)))
        .append($('<td></td>').append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdA"></div></form>').attr("title", "slide to change weight threshold")))
        .append($('<td><span id="run1Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
      );
      if (!singleRunView) {
        var run2Rank = $(this).closest('[run2-rank]').attr('run2-rank');
        legendTable.append($('<tr></tr>')
          .append($('<th></th>').text(data.meta.run2_name))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background-color', 'rgb(' + COLOR_B + ')'))
          )
          .append($('<td></td>').append($(" <span class='border badge' style='min-width: 70px;'></span>").text("Rank: " +run2Rank)))
          .append($('<td></td>')
            .append($('<form><div class="form-group"><input type="range" class="form-control-range" min="0", max="1.1", step="0.1" value="0.1" id="weightThresholdB"></div></form>').attr("title", "slide to change weight threshold"))
          ).append($('<td><span id="run2Threshold" class="badge border rounded threshold-value">0.1</span></td>'))
        );
        legendTable.append($('<tr></tr>')
          .append($('<th>both</th>'))
          .append($('<td></td>')
            .append($('<span class="swatch"></span>').css('background', 'linear-gradient(rgb(' + COLOR_A + '), rgb(' + COLOR_B + '))'))
          ));
      }
      var fieldTable = $('<table class="fields"></table>')
        .appendTo(page);
      var weightsA = allWeightsA[docid] || {};
      var weightsB = allWeightsB[docid] || {};
      var mweights = mergedWeights[docid] || {};

      $.each(doc, function (fname, fvalue) {
        if (singleRunView) {
          if (!(fname in weightsA)) {
            mweights[fname] = [];
          } else {
            mweights[fname] = weightsA[fname].map(segment => {
              return [segment[0], segment[1], { "run1": segment[2] }];
            });
          }
        }
        var weights = colorizeWeights(mweights[fname]);
        $('<tr></tr>')
          .append($('<th></th>').text(fname))
          .append($('<td></td>').append(markup(fvalue, weights)))
          .appendTo(fieldTable);
      });

      $("input").change(function () {
        var threshold = $(this).closest("form :input").val();
        if ($(this).attr("id") === "weightThresholdA") {
          $("#run1Threshold").text(threshold);
        } else {
          $("#run2Threshold").text(threshold);
        }
        onChangeWeightThreshold();
      });
      onChangeWeightThreshold();
      return false; // prevent nav
    }

    function closeDoc() {
      $('#DocumentOverlay,#DocumentDetails').remove();
    }
    function ding() {
      console.log("Reaching limits! Alert");
    }

    $(function () {
      if (singleRunView) {
        $("#runName").empty();
        $("#runName").append($('<h6 style="text-align: center;"></h6>').text(data.meta.run1_name));
      } else {
        $('#Run1Name').text(data.meta.run1_name);
        $('#Run2Name').text(data.meta.run2_name);
      }
      var $select = $('#Queries');
      var queryDisplayField = null;
      $.each(data.meta.queryFields, function (i, e) {
        if (e !== 'query_id') {
          queryDisplayField = e;
          return false; // break
        }
      });
      $.each(data.queries, function (_, query) {
        if (!singleRunView){
          $('<option>').attr('value', query['fields']['query_id']).attr("data-tokens", query.fields.query_id + " " + query.fields[queryDisplayField]).attr('data-subtext', query.fields.contrast.name+': '+query.fields.contrast.value.toFixed(3)).text(query.fields[queryDisplayField]).appendTo($select);
        } else {
          $('<option>').attr('value', query['fields']['query_id']).text(query.fields[queryDisplayField]).appendTo($select);
        }
      });
      $select.change(selectQuery).change();
      $(document).on('mouseenter', '.card', onCardEnter);
      $(document).on('mouseleave', '.card', onCardLeave);
      $(document).on('click', '.card', onDocInfoClick);
      $(document).on('click', '#DocumentOverlay', closeDoc);
      $(document).keyup(function (e) {
        if (e.key === "Escape") {
          closeDoc();
        }
        if (e.key === "ArrowLeft" && !$("#Queries").is(":focus")) {
          var prev_val = $("#Queries option:selected").prev().val();
          if (typeof prev_val != "undefined") {
            $select.val(prev_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
        if (e.key === "ArrowRight" && !$("#Queries").is(":focus")) {
          var next_val = $("#Queries option:selected").next().val();
          if (typeof next_val != "undefined") {
            $select.val(next_val);
            $select.trigger("change");
          } else {
            ding();
          }
        }
      });
    });
    $(document).ready(function () {
      var $select = $('#Queries');
      if (data.queries.length > 20)
        $select.attr("data-live-search","true")
      $select.selectpicker();
      $("#Query").find("tr").slice(2).attr("class", "query_collapse collapse")
      $(".query_collapse").on("shown.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-contract" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M.172 15.828a.5.5 0 0 0 .707 0l4.096-4.096V14.5a.5.5 0 1 0 1 0v-3.975a.5.5 0 0 0-.5-.5H1.5a.5.5 0 0 0 0 1h2.768L.172 15.121a.5.5 0 0 0 0 .707zM15.828.172a.5.5 0 0 0-.707 0l-4.096 4.096V1.5a.5.5 0 1 0-1 0v3.975a.5.5 0 0 0 .5.5H14.5a.5.5 0 0 0 0-1h-2.768L15.828.879a.5.5 0 0 0 0-.707z"/></svg>';
        $("#query-collapse-btn").html(text);
      })
      $(".query_collapse").on("hidden.bs.collapse", function () {
        text = '<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-arrows-angle-expand" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M5.828 10.172a.5.5 0 0 0-.707 0l-4.096 4.096V11.5a.5.5 0 0 0-1 0v3.975a.5.5 0 0 0 .5.5H4.5a.5.5 0 0 0 0-1H1.732l4.096-4.096a.5.5 0 0 0 0-.707zm4.344-4.344a.5.5 0 0 0 .707 0l4.096-4.096V4.5a.5.5 0 1 0 1 0V.525a.5.5 0 0 0-.5-.5H11.5a.5.5 0 0 0 0 1h2.768l-4.096 4.096a.5.5 0 0 0 0 .707z"></svg>';
        $("#query-collapse-btn").html(text);
      })
    })
  </script>
</body>

</html>

